{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"XLS : Accelerated HW Synthesis What is XLS? XLS implements a High Level Synthesis (HLS) toolchain which produces synthesizable designs (Verilog and SystemVerilog) from flexible, high-level descriptions of functionality. It is fully Open Source: Apache 2 licensed and developed via GitHub. XLS (Accelerated HW Synthesis) aims to be the Software Development Kit (SDK) for the End of Moore's Law (EoML) era. In this \"age of specialization\", software and hardware engineers must do more co-design across their domain boundaries -- collaborate on shared artifacts, understand each other's cost models, and share tooling/methodology. XLS attempts to leverage automation, software engineers, and machine cycles to accelerate this overall process. In effect, XLS enables the rapid development of hardware IP that also runs as efficient host software via \"software style\" methodology. A single XLS source program runs at native speeds for use in host software or a simulator, but that source can also be used to generate hardware block output -- the XLS tools' correctness ensures (and provides tools to help formally verify) that they are functionally identical. XLS is used inside of Google for generating feed-forward pipelines from \"building block\" routines / libraries that can be easily retargeted, reused, and composed in a latency-insensitive manner. XLS also supports concurrent processes , in Communicating Sequential Processes (CSP) style, that allow pipelines to communicate with each other and induct over time. This feature is still under active development but today supports base use cases. XLS is still experimental, undergoing rapid development, and not an officially supported Google product. Expect bugs and sharp edges. Please help by trying it out, running through some tutorials , reporting bugs , and letting us know what you think! Building From Source Currently, XLS must be built from source using the Bazel build system. Note: Binary distributions of the XLS library are not currently available, but we hope to enable them via continuous integration, see this issue . The following instructions are for the Ubuntu 20.04 (Focal Fossa) and Ubuntu 22.04 (Jammy Jellyfish) Linux distributions. We start by assuming Bazel has been installed . On an average 8-core VM, a full initial build (including the C++ frontend) may take up to 6 hours. A build without the C++ frontend may take about 2 hours. Please see the two corresponding command lines below: ~$ git clone https://github.com/google/xls.git ~$ cd xls ~/xls$ # Follow the bazel install instructions: ~/xls$ # https://bazel.build/install/ubuntu ~/xls$ # Afterwards we observe: ~/xls$ bazel --version bazel 5.2.0 ~/xls$ # Note we're going to tell Ubuntu that `/usr/bin/env python` is actually python3 ~/xls$ # here, since that is not the case by default on Ubuntu 20.04. ~/xls$ # This is important. Without this step, you may experience cryptic error messages: ~/xls$ sudo apt install python3-distutils python3-dev libtinfo5 python-is-python3 ~/xls$ # Now build/test in optimized build mode. ~/xls$ # If you don't plan on using the C++ frontend, which is not needed to get started, ~/xls$ # use this command line: ~/xls$ bazel test -c opt -- //xls/... -//xls/contrib/xlscc/... ~/xls$ # To build everything, including the C++ frontend: ~/xls$ bazel test -c opt -- //xls/... Reference build/test environment setups are also provided via Dockerfile s: ~$ git clone https://github.com/google/xls.git ~$ cd xls ~/xls$ # Several Dockerfiles are available to choose from: ~/xls$ docker build . -f Dockerfile-ubuntu-20.04 # Performs optimized build and test. ~/xls$ docker build . -f Dockerfile-ubuntu-20.10 ~/xls$ docker build . -f Dockerfile-ubuntu-22.04 Stack Diagram and Project Layout Navigating a new code base can be daunting; the following description provides a high-level view of the important directories and their intended organization / purpose, and correspond to the components in this XLS stack diagram: dependency_support : Configuration files that load, build, and expose Bazel targets for external dependencies of XLS. docs : Generated documentation served via GitHub pages: https://google.github.io/xls/ docs_src : Markdown file sources, rendered to docs via mkdocs . xls : Project-named subdirectory within the repository, in common Bazel-project style. build : Build macros that create XLS artifacts; e.g. convert DSL to IR, create test targets for DSL code, etc. codegen : Verilog AST (VAST) support to generate Verilog/SystemVerilog operations and FSMs. VAST is built up by components we call generators (e.g. PipelineGenerator, SequentialGenerator for FSMs) in the translation from XLS IR. common : \"base\" functionality that layers on top of standard library usage. Generally we use Abseil versions of base constructs wherever possible. contrib/xlscc : Experimental C++ syntax support that targets XLS IR (alternative path to DSLX) developed by a sister team at Google, sharing the same open source / testing flow as the rest of the XLS project. May be of particular interest for teams with existing C++ HLS code bases. data_structures : Generic data structures used in XLS that augment standard libraries; e.g. BDDs, union find, min cut, etc. delay_model : Functionality to characterize, describe, and interpolate data delay for XLS IR operations on a target backend process. Already-characterized descriptions are placed in xls/delay_model/models and can be referred to via command line flags. dslx : A DSL (called \"DSLX\") that mimics Rust, while being an immutable expression-language dataflow DSL with hardware-oriented features; e.g. arbitrary bitwidths, entirely fixed size objects, fully analyzeable call graph. XLS team has found dataflow DSLs are a good fit to describe hardware as compared to languages designed assume von Neumann style computation. fuzzer : A whole-stack multiprocess fuzzer that generates programs at the DSL level and cross-compares different execution engines (DSL interpreter, IR interpreter, IR JIT, code-generated-Verilog simulator). Designed so that it can easily be run on different nodes in a cluster simultaneously and accumulate shared findings. examples : Example computations that are tested and executable through the XLS stack. experimental : Artifacts captured from experimental explorations. interpreter : Interpreter for XLS IR - useful for debugging and exploration. For cases needing throughput, consider using the JIT (below). ir : XLS IR definition, text parser/formatter, and facilities for abstract evaluation. jit : LLVM-based JIT for XLS IR. Enables native-speed execution of DSLX and XLS IR programs. modules : Hardware building block DSLX \"libraries\" (outside the DSLX standard library) that may be easily reused or instantiated in a broader design. netlist : Libraries that parse/analyze/interpret netlist-level descriptions, as are generally given in simple structural Verilog with an associated cell library. passes : Passes that run on the XLS IR as part of optimization, before scheduling / code generation. scheduling : Scheduling algorithms, determine when operations execute (e.g. which pipeline stage) in a clocked design. simulation : Code that wraps Verilog simulators and generates Verilog testbenches for XLS computations. iverilog is currently used to simulate as it supports non-synthesizable testbench constructs. solvers : Converters from XLS IR into SMT solver input, such that formal proofs can be run on XLS computations; e.g. Logical Equalence Checks between XLS IR and a netlist description. Z3 is used as the solver engine. synthesis : Interface that wraps backend synthesis flows, such that tools can be retargeted e.g. between ASIC and FPGA flows. tests : Integration tests that span various top-level components of the XLS project. tools : Many tools that work with the XLS system and its libraries in a decomposed way via command line interfaces. uncore_rtl : Helper RTL that interfaces XLS-generated blocks with device top-level for e.g. FPGA experiments. visualization : Visualization tools to inspect the XLS compiler/system interactively. See IR visualization . Community Discussions about XLS - development, debugging, usage, and anything else - should go to the xls-dev mailing list . Contributors The following are contributors to the XLS project, see our contributing documentation and good first issues ! Albert Magyar Amin Kalantar Balint Christian Blaok Brandon Jiang Brian Searls Chen-hao Chang Chris Drake Chris Leary Dan Killebrew Derek Lockhart Eric Astor Ethan Mahintorabi Felix Zhu Georges Rotival Hans Montero Iliyan Malchev Johan Euphrosine Jonathan Bailey Josh Varga Julian Viera Kevin Harlley Leonardo Romor Manav Kohli Mark Heffernan Paul Rigge Per Gr\u00f6n Ravi Nanavati Rebecca Chen (Pytype) Remy Goldschmidt Robert Hundt Rob Springer Sameer Agarwal Sean Purser-Haskell Ted Hong Ted Xie Vincent Mirian","title":"Overview"},{"location":"#xls-accelerated-hw-synthesis","text":"","title":"XLS: Accelerated HW Synthesis"},{"location":"#what-is-xls","text":"XLS implements a High Level Synthesis (HLS) toolchain which produces synthesizable designs (Verilog and SystemVerilog) from flexible, high-level descriptions of functionality. It is fully Open Source: Apache 2 licensed and developed via GitHub. XLS (Accelerated HW Synthesis) aims to be the Software Development Kit (SDK) for the End of Moore's Law (EoML) era. In this \"age of specialization\", software and hardware engineers must do more co-design across their domain boundaries -- collaborate on shared artifacts, understand each other's cost models, and share tooling/methodology. XLS attempts to leverage automation, software engineers, and machine cycles to accelerate this overall process. In effect, XLS enables the rapid development of hardware IP that also runs as efficient host software via \"software style\" methodology. A single XLS source program runs at native speeds for use in host software or a simulator, but that source can also be used to generate hardware block output -- the XLS tools' correctness ensures (and provides tools to help formally verify) that they are functionally identical. XLS is used inside of Google for generating feed-forward pipelines from \"building block\" routines / libraries that can be easily retargeted, reused, and composed in a latency-insensitive manner. XLS also supports concurrent processes , in Communicating Sequential Processes (CSP) style, that allow pipelines to communicate with each other and induct over time. This feature is still under active development but today supports base use cases. XLS is still experimental, undergoing rapid development, and not an officially supported Google product. Expect bugs and sharp edges. Please help by trying it out, running through some tutorials , reporting bugs , and letting us know what you think!","title":"What is XLS?"},{"location":"#building-from-source","text":"Currently, XLS must be built from source using the Bazel build system. Note: Binary distributions of the XLS library are not currently available, but we hope to enable them via continuous integration, see this issue . The following instructions are for the Ubuntu 20.04 (Focal Fossa) and Ubuntu 22.04 (Jammy Jellyfish) Linux distributions. We start by assuming Bazel has been installed . On an average 8-core VM, a full initial build (including the C++ frontend) may take up to 6 hours. A build without the C++ frontend may take about 2 hours. Please see the two corresponding command lines below: ~$ git clone https://github.com/google/xls.git ~$ cd xls ~/xls$ # Follow the bazel install instructions: ~/xls$ # https://bazel.build/install/ubuntu ~/xls$ # Afterwards we observe: ~/xls$ bazel --version bazel 5.2.0 ~/xls$ # Note we're going to tell Ubuntu that `/usr/bin/env python` is actually python3 ~/xls$ # here, since that is not the case by default on Ubuntu 20.04. ~/xls$ # This is important. Without this step, you may experience cryptic error messages: ~/xls$ sudo apt install python3-distutils python3-dev libtinfo5 python-is-python3 ~/xls$ # Now build/test in optimized build mode. ~/xls$ # If you don't plan on using the C++ frontend, which is not needed to get started, ~/xls$ # use this command line: ~/xls$ bazel test -c opt -- //xls/... -//xls/contrib/xlscc/... ~/xls$ # To build everything, including the C++ frontend: ~/xls$ bazel test -c opt -- //xls/... Reference build/test environment setups are also provided via Dockerfile s: ~$ git clone https://github.com/google/xls.git ~$ cd xls ~/xls$ # Several Dockerfiles are available to choose from: ~/xls$ docker build . -f Dockerfile-ubuntu-20.04 # Performs optimized build and test. ~/xls$ docker build . -f Dockerfile-ubuntu-20.10 ~/xls$ docker build . -f Dockerfile-ubuntu-22.04","title":"Building From Source"},{"location":"#stack-diagram-and-project-layout","text":"Navigating a new code base can be daunting; the following description provides a high-level view of the important directories and their intended organization / purpose, and correspond to the components in this XLS stack diagram: dependency_support : Configuration files that load, build, and expose Bazel targets for external dependencies of XLS. docs : Generated documentation served via GitHub pages: https://google.github.io/xls/ docs_src : Markdown file sources, rendered to docs via mkdocs . xls : Project-named subdirectory within the repository, in common Bazel-project style. build : Build macros that create XLS artifacts; e.g. convert DSL to IR, create test targets for DSL code, etc. codegen : Verilog AST (VAST) support to generate Verilog/SystemVerilog operations and FSMs. VAST is built up by components we call generators (e.g. PipelineGenerator, SequentialGenerator for FSMs) in the translation from XLS IR. common : \"base\" functionality that layers on top of standard library usage. Generally we use Abseil versions of base constructs wherever possible. contrib/xlscc : Experimental C++ syntax support that targets XLS IR (alternative path to DSLX) developed by a sister team at Google, sharing the same open source / testing flow as the rest of the XLS project. May be of particular interest for teams with existing C++ HLS code bases. data_structures : Generic data structures used in XLS that augment standard libraries; e.g. BDDs, union find, min cut, etc. delay_model : Functionality to characterize, describe, and interpolate data delay for XLS IR operations on a target backend process. Already-characterized descriptions are placed in xls/delay_model/models and can be referred to via command line flags. dslx : A DSL (called \"DSLX\") that mimics Rust, while being an immutable expression-language dataflow DSL with hardware-oriented features; e.g. arbitrary bitwidths, entirely fixed size objects, fully analyzeable call graph. XLS team has found dataflow DSLs are a good fit to describe hardware as compared to languages designed assume von Neumann style computation. fuzzer : A whole-stack multiprocess fuzzer that generates programs at the DSL level and cross-compares different execution engines (DSL interpreter, IR interpreter, IR JIT, code-generated-Verilog simulator). Designed so that it can easily be run on different nodes in a cluster simultaneously and accumulate shared findings. examples : Example computations that are tested and executable through the XLS stack. experimental : Artifacts captured from experimental explorations. interpreter : Interpreter for XLS IR - useful for debugging and exploration. For cases needing throughput, consider using the JIT (below). ir : XLS IR definition, text parser/formatter, and facilities for abstract evaluation. jit : LLVM-based JIT for XLS IR. Enables native-speed execution of DSLX and XLS IR programs. modules : Hardware building block DSLX \"libraries\" (outside the DSLX standard library) that may be easily reused or instantiated in a broader design. netlist : Libraries that parse/analyze/interpret netlist-level descriptions, as are generally given in simple structural Verilog with an associated cell library. passes : Passes that run on the XLS IR as part of optimization, before scheduling / code generation. scheduling : Scheduling algorithms, determine when operations execute (e.g. which pipeline stage) in a clocked design. simulation : Code that wraps Verilog simulators and generates Verilog testbenches for XLS computations. iverilog is currently used to simulate as it supports non-synthesizable testbench constructs. solvers : Converters from XLS IR into SMT solver input, such that formal proofs can be run on XLS computations; e.g. Logical Equalence Checks between XLS IR and a netlist description. Z3 is used as the solver engine. synthesis : Interface that wraps backend synthesis flows, such that tools can be retargeted e.g. between ASIC and FPGA flows. tests : Integration tests that span various top-level components of the XLS project. tools : Many tools that work with the XLS system and its libraries in a decomposed way via command line interfaces. uncore_rtl : Helper RTL that interfaces XLS-generated blocks with device top-level for e.g. FPGA experiments. visualization : Visualization tools to inspect the XLS compiler/system interactively. See IR visualization .","title":"Stack Diagram and Project Layout"},{"location":"#community","text":"Discussions about XLS - development, debugging, usage, and anything else - should go to the xls-dev mailing list .","title":"Community"},{"location":"#contributors","text":"The following are contributors to the XLS project, see our contributing documentation and good first issues ! Albert Magyar Amin Kalantar Balint Christian Blaok Brandon Jiang Brian Searls Chen-hao Chang Chris Drake Chris Leary Dan Killebrew Derek Lockhart Eric Astor Ethan Mahintorabi Felix Zhu Georges Rotival Hans Montero Iliyan Malchev Johan Euphrosine Jonathan Bailey Josh Varga Julian Viera Kevin Harlley Leonardo Romor Manav Kohli Mark Heffernan Paul Rigge Per Gr\u00f6n Ravi Nanavati Rebecca Chen (Pytype) Remy Goldschmidt Robert Hundt Rob Springer Sameer Agarwal Sean Purser-Haskell Ted Hong Ted Xie Vincent Mirian","title":"Contributors"},{"location":"adding_ir_operation/","text":"Adding a new IR operation XLS has about 60 different opcodes and periodically new ones are added to extend functionality or improve the expressiveness of the IR. XLS has many different components and adding a new opcode involves changes to numerous places in the code. These changes, some of which are optional, are described below: Add operation to op_specification.py Opcodes and IR node classes are defined in the file op_specification.py . This Python code generates the C++ header and source files which define opcodes ( op.h and op.cc ) and the IR node type hierarchy ( nodes.h and nodes.cc ). Every opcode has an associated node subclass derived from the xls::Node base class. Some opcodes such as Op::kArray have their own class ( Array ) because of the unique structure of the operation. Other opcodes such as the logical operations ( Op::kAnd , Op::kOr , etc) share a common base class ( BinOp ). The first step to adding a new operations is to add an opcode, and potentially a new Node class, in op_specification.py . After adding the opcode numerous files will fail to build because switch statements over the set of opcodes will no longer be exhaustive. Add the necessary cases to each switch statement. The exact code in each case will, of course, be operation-specific. Initially the implementation might return an absl::UnimplementedError status until later changes add proper support for the new operation. As part of this change the new operations needs to be added to the DFS visitor class DfsVisitor by adding a handler method. This class is used throughout XLS to traverse the IR. This will also adding an implementation of this new method to many of the subclasses derived from DfsVisitor . (Code example) IR Verifier The IR verifier checks numerous invariants about the IR including operation-specific properties such as the number and type of operands. Add an additional handler method for the new operand and add appropriate operation-specific checks. (Code example) IR Semantics document Describe the semantics and syntax of the new operation in the IR semantics document. (Code example) Function builder The function builder is the primary API for constructing IR. If appropriate, add a method to the BuilderBase class which adds an IR node of the new type to a function. (Code example) IR Parser Add support for parsing of the new operation. The parser tests typically send a snippet of IR with the operation through the parser and text serialization and verifies that the output matches the original. Supporting the new operation may require modifying the xls::Node::ToString method to emit any special fields required by the operation. (Code example) IR Interpreter The IR interpreter has C++ implementations of all of the operations. Implement the new operation and add tests. (Code example) IR Matcher The IR matcher is used in tests to enable easy matching of IR expressions. For example, the following tests that the return value of a function is the parameter x plus the parameter y : EXPECT_THAT(f->return_value(), m::Add(m::Param(\"x\", m::Param(\"y\"))); If the new operation has no named attributed, IR matcher support is typically a single line using the macro NODE_MATCHER . Otherwise, a custom matcher should be added to enable matching the attribute as well. (Code example) LLVM JIT The LLVM JIT enables fast simulation of the XLS IR. The JIT constructs LLVM IR for each XLS operation which is then optimized by LLVM and runs natively on the host. Implement the new operation in the FunctionBuilderVisitor class. (Code example) Code generation In XLS \"code generation\" refers to the generation of (System)Verilog from XLS IR. If the operation can be emitted as a single Verilog expression, then likely support for the new operation can be added to node_expressions.h , otherwise if the implementation requires multiple statements then support is added to module_builder.h . (Code example) Abstract evaluator The abstract evaluator enables evaluation of the XLS IR using different evaluation systems than Boolean algebra. Users define the semantics of simple logical operations such as and, or, and not. Then, the abstract evaluator interprets an IR function using these rules. One example use case is ternary logic which uses three logic values (true, false, and unknown) rather than two (true and false) Ternary evaluation is used by the optimizer to discover statically known bits in the IR graph. The abstract evaluator can also be used for translation of the IR to other representations. For example, IR is translated to the Z3 solver representation for formal verification using the abstract evaluator. If appropriate, the operation should be implemented in AbstractNodeEvaluator by providing an implementation which decomposes the operation into fundamental logical operations. (Code example) Z3 solver The Z3 solver is used for theorem proving and logical equivalence checking between the IR in different stages of compilation and the netlist. To enable this functionality for the new operation, add a lowering of the operation to Z3's internal representation. (Code example) Delay model In order to generate efficient circuits which meet timing requirement, XLS models the delay (in picoseconds) of each operation for different process technology nodes. This model is constructed by characterizing the process node using an EDA tool to synthesize the circuit and estimate delay. Typically, a new operation will need to be characterized by running numerous permutations of the operation (e.g., with different bit widths) through a synthesis flow, extracting delay, and building a delay model. (Code example) DSLX frontend Most ops are used by the DSLX frontend in the lowering of DSLX to IR. The operation may be exposed directly as a builtin (or other operation) or used in the lowering of other AST nodes. In any case, some changes to the DSLX frontend will likely be necessary. (Code example) Fuzzer The fuzzer generates random DSLX functions and random inputs to check and compare different parts of XLS, for example checking that un-optimized and optimized IR give the same outputs when interpreted. If there is an operation in DSLX that maps nicely onto the newly added operation, the fuzzer can be modified to generate functions with DSLX that exercise the new operation. This is done by adding a handler to AstGenerator . See here for more details on how the fuzzer works and how to run it. (Code example) Operation-specific optimizations Typically, a new operation provides optimization opportunities unique to the node. The details, of course, will be vary for different operations. However, typically these are at least several easy optimizations which can be implemented.","title":"Adding a new IR operation"},{"location":"adding_ir_operation/#adding-a-new-ir-operation","text":"XLS has about 60 different opcodes and periodically new ones are added to extend functionality or improve the expressiveness of the IR. XLS has many different components and adding a new opcode involves changes to numerous places in the code. These changes, some of which are optional, are described below: Add operation to op_specification.py Opcodes and IR node classes are defined in the file op_specification.py . This Python code generates the C++ header and source files which define opcodes ( op.h and op.cc ) and the IR node type hierarchy ( nodes.h and nodes.cc ). Every opcode has an associated node subclass derived from the xls::Node base class. Some opcodes such as Op::kArray have their own class ( Array ) because of the unique structure of the operation. Other opcodes such as the logical operations ( Op::kAnd , Op::kOr , etc) share a common base class ( BinOp ). The first step to adding a new operations is to add an opcode, and potentially a new Node class, in op_specification.py . After adding the opcode numerous files will fail to build because switch statements over the set of opcodes will no longer be exhaustive. Add the necessary cases to each switch statement. The exact code in each case will, of course, be operation-specific. Initially the implementation might return an absl::UnimplementedError status until later changes add proper support for the new operation. As part of this change the new operations needs to be added to the DFS visitor class DfsVisitor by adding a handler method. This class is used throughout XLS to traverse the IR. This will also adding an implementation of this new method to many of the subclasses derived from DfsVisitor . (Code example) IR Verifier The IR verifier checks numerous invariants about the IR including operation-specific properties such as the number and type of operands. Add an additional handler method for the new operand and add appropriate operation-specific checks. (Code example) IR Semantics document Describe the semantics and syntax of the new operation in the IR semantics document. (Code example) Function builder The function builder is the primary API for constructing IR. If appropriate, add a method to the BuilderBase class which adds an IR node of the new type to a function. (Code example) IR Parser Add support for parsing of the new operation. The parser tests typically send a snippet of IR with the operation through the parser and text serialization and verifies that the output matches the original. Supporting the new operation may require modifying the xls::Node::ToString method to emit any special fields required by the operation. (Code example) IR Interpreter The IR interpreter has C++ implementations of all of the operations. Implement the new operation and add tests. (Code example) IR Matcher The IR matcher is used in tests to enable easy matching of IR expressions. For example, the following tests that the return value of a function is the parameter x plus the parameter y : EXPECT_THAT(f->return_value(), m::Add(m::Param(\"x\", m::Param(\"y\"))); If the new operation has no named attributed, IR matcher support is typically a single line using the macro NODE_MATCHER . Otherwise, a custom matcher should be added to enable matching the attribute as well. (Code example) LLVM JIT The LLVM JIT enables fast simulation of the XLS IR. The JIT constructs LLVM IR for each XLS operation which is then optimized by LLVM and runs natively on the host. Implement the new operation in the FunctionBuilderVisitor class. (Code example) Code generation In XLS \"code generation\" refers to the generation of (System)Verilog from XLS IR. If the operation can be emitted as a single Verilog expression, then likely support for the new operation can be added to node_expressions.h , otherwise if the implementation requires multiple statements then support is added to module_builder.h . (Code example) Abstract evaluator The abstract evaluator enables evaluation of the XLS IR using different evaluation systems than Boolean algebra. Users define the semantics of simple logical operations such as and, or, and not. Then, the abstract evaluator interprets an IR function using these rules. One example use case is ternary logic which uses three logic values (true, false, and unknown) rather than two (true and false) Ternary evaluation is used by the optimizer to discover statically known bits in the IR graph. The abstract evaluator can also be used for translation of the IR to other representations. For example, IR is translated to the Z3 solver representation for formal verification using the abstract evaluator. If appropriate, the operation should be implemented in AbstractNodeEvaluator by providing an implementation which decomposes the operation into fundamental logical operations. (Code example) Z3 solver The Z3 solver is used for theorem proving and logical equivalence checking between the IR in different stages of compilation and the netlist. To enable this functionality for the new operation, add a lowering of the operation to Z3's internal representation. (Code example) Delay model In order to generate efficient circuits which meet timing requirement, XLS models the delay (in picoseconds) of each operation for different process technology nodes. This model is constructed by characterizing the process node using an EDA tool to synthesize the circuit and estimate delay. Typically, a new operation will need to be characterized by running numerous permutations of the operation (e.g., with different bit widths) through a synthesis flow, extracting delay, and building a delay model. (Code example) DSLX frontend Most ops are used by the DSLX frontend in the lowering of DSLX to IR. The operation may be exposed directly as a builtin (or other operation) or used in the lowering of other AST nodes. In any case, some changes to the DSLX frontend will likely be necessary. (Code example) Fuzzer The fuzzer generates random DSLX functions and random inputs to check and compare different parts of XLS, for example checking that un-optimized and optimized IR give the same outputs when interpreted. If there is an operation in DSLX that maps nicely onto the newly added operation, the fuzzer can be modified to generate functions with DSLX that exercise the new operation. This is done by adding a handler to AstGenerator . See here for more details on how the fuzzer works and how to run it. (Code example) Operation-specific optimizations Typically, a new operation provides optimization opportunities unique to the node. The details, of course, will be vary for different operations. However, typically these are at least several easy optimizations which can be implemented.","title":"Adding a new IR operation"},{"location":"bazel_rules_macros/","text":"Bazel Rules And Macros check_sha256sum_frozen check_sha256sum_frozen( name , frozen_file , sha256sum , src ) Produces a frozen file if the sha256sum checksum of a source file matches a user-defined checksum. As projects cut releases or freeze, it's important to know that generated (e.g. Verilog) code is never changing without having to actually check in the generated artifact. This rule performs a checksum of a generated file as an integrity check. Users might use this rule to help enable confidence that there is neither: non-determinism in the toolchain, nor an accidental dependence on a non-released toolchain (e.g. an accidental dependence on top-of-tree, where the toolchain is constantly changing) Say there was a codegen rule producing my_output.v , a user might instantiate something like: check_sha256sum_frozen( name = \"my_output_checksum\", src = \":my_output.v\", sha256sum = \"d1bc8d3ba4afc7e109612cb73acbdddac052c93025aa1f82942edabb7deb82a1\", frozen_file = \"my_output.frozen.x\", ) ... and then take a dependency on my_output.frozen.v in the surrounding project, knowing that it had been checksum-verified. Taking a dependence on my_output.v directly may also be ok if the :my_output_checksum target is also built (e.g. via the same wildcard build request), but taking a dependence on the output .frozen.v file ensures that the checking is an integral part of the downstream build-artifact-creation process. At its core, this rule ensure that the contents of a file does not change by verifying that it matches a given checksum. Typically, this rule is used to control the build process. The rule serves as a trigger on rules depending on its output (the frozen file). When the validation of the sha256sum succeed, rules depending on the frozen file are built/executed. When the validation of the sha256sum fails, rules depending on the frozen file are not built/executed. In the example below, when the validation of the sha256sum for target 'generated_file_sha256sum_frozen' succeeds, target 'generated_file_dslx' is built. However, when the validation of the sha256sum for target 'generated_file_sha256sum_frozen' fails, target 'generated_file_dslx' is not built. Examples: A simple example. check_sha256sum_frozen( name = \"generated_file_sha256sum_frozen\", src = \":generated_file.x\", sha256sum = \"6522799f7b64dbbb2a31eb2862052b8988e78821d8b61fff7f508237a9d9f01d\", frozen_file = \"generated_file.frozen.x\", ) dslx_library( name = \"generated_file_dslx\", src = \":generated_file.frozen.x\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required frozen_file The frozen output file. Label required sha256sum The sha256sum of the source file. String required src The source file. Label required check_sha256sum_test check_sha256sum_test( name , sha256sum , src ) Validates the sha256sum checksum of a source file with a user-defined checksum. This rule is typically used to ensure that the contents of a file is unchanged. Examples: A simple example. check_sha256sum_test( name = \"generated_file_sha256sum_test\", src = \":generated_file.x\", sha256sum = \"6522799f7b64dbbb2a31eb2862052b8988e78821d8b61fff7f508237a9d9f01d\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required sha256sum The sha256sum of the source file. String required src The source file. Label required proto_data proto_data( name , protobin_file , src ) Converts a proto text with a xlscc.HLSBlock message to a proto binary. This rules is used in conjunction with the (e.g. xls_cc_ir and xls_cc_verilog) rules and xls_cc_* (e.g. xls_cc_ir_macro and xls_cc_verilog_macro) macros. Examples: A simple example. proto_data( name = \"packet_selector_block_pb\", src = \"packet_selector.textproto\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required protobin_file The name of the output file to write binary proto to. If not specified, the target name of the bazel rule followed by an .protobin extension is used. Label optional src The source file. Label required xls_benchmark_ir xls_benchmark_ir( name , benchmark_ir_args , src , top ) Executes the benchmark tool on an IR file. Examples: A file as the source. xls_benchmark_ir( name = \"a_benchmark\", src = \"a.ir\", ) An xls_ir_opt_ir target as the source. xls_ir_opt_ir( name = \"a_opt_ir\", src = \"a.ir\", ) xls_benchmark_ir( name = \"a_benchmark\", src = \":a_opt_ir\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required benchmark_ir_args Arguments of the benchmark IR tool. For details on the arguments, refer to the benchmark_main application at //xls/tools/benchmark_main.cc. Dictionary: String -> String optional {} src The IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required top The ( mangled ) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional \"\" xls_benchmark_verilog xls_benchmark_verilog( name , verilog_target ) Computes and prints various metrics about a Verilog target. Example: xls_benchmark_verilog( name = \"a_benchmark\", verilog_target = \"a_verilog_target\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required verilog_target The verilog target to benchmark. Label optional None xls_dslx_library xls_dslx_library( name , deps , srcs , warnings_as_errors ) A build rule that parses and type checks DSLX source files. Examples: A collection of DSLX source files. xls_dslx_library( name = \"files_123_dslx\", srcs = [ \"file_1.x\", \"file_2.x\", \"file_3.x\", ], ) Dependency on other xls_dslx_library targets. xls_dslx_library( name = \"a_dslx\", srcs = [\"a.x\"], ) # Depends on target a_dslx. xls_dslx_library( name = \"b_dslx\", srcs = [\"b.x\"], deps = [\":a_dslx\"], ) # Depends on target a_dslx. xls_dslx_library( name = \"c_dslx\", srcs = [\"c.x\"], deps = [\":a_dslx\"], ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required deps Dependency targets for the rule. List of labels optional [] srcs Source files for the rule. Files must have a '.x' extension. List of labels optional [] warnings_as_errors Whether warnings are errors within this library definition. Boolean optional False xls_dslx_opt_ir_test xls_dslx_opt_ir_test( name , benchmark_ir_args , dep , dslx_test_args , input_validator , input_validator_expr , ir_equivalence_args , ir_eval_args , top ) A build rule that tests a xls_dslx_opt_ir target. Executes the test commands for the following rules in the order presented: xls_dslx_test xls_ir_equivalence_test xls_eval_ir_test xls_benchmark_ir Examples: A simple example. xls_dslx_opt_ir( name = \"a_opt_ir\", srcs = [\"a.x\"], dslx_top = \"a\", ) xls_dslx_opt_ir_test( name = \"a_opt_ir_test\", dep = \":a_opt_ir\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required benchmark_ir_args Arguments of the benchmark IR tool. For details on the arguments, refer to the benchmark_main application at //xls/tools/benchmark_main.cc. Dictionary: String -> String optional {} dep The xls_dslx_opt_ir target to test. Label optional None dslx_test_args Arguments of the DSLX interpreter executable. For details on the arguments, refer to the interpreter_main application at //xls/dslx/interpreter_main.cc. Dictionary: String -> String optional {} input_validator The DSLX library defining the input validator for this test. Mutually exclusive with \"input_validator_expr\". Label optional None input_validator_expr The expression to validate an input for the test function. Mutually exclusive with \"input_validator\". String optional \"\" ir_equivalence_args Arguments of the IR equivalence tool. For details on the arguments, refer to the check_ir_equivalence_main application at //xls/tools/check_ir_equivalence_main.cc. The 'function' argument is not assigned using this attribute. Dictionary: String -> String optional {} ir_eval_args Arguments of the IR interpreter. For details on the arguments, refer to the eval_ir_main application at //xls/tools/eval_ir_main.cc.The 'top' argument is not assigned using this attribute. Dictionary: String -> String optional {\"random_inputs\": \"100\", \"optimize_ir\": \"true\"} top The ( mangled ) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional \"\" xls_dslx_test xls_dslx_test( name , deps , dslx_test_args , library , srcs ) A dslx test executes the tests and quick checks of a DSLX source file. Examples: xls_dslx_test on DSLX source files. # Assume a xls_dslx_library target bc_dslx is present. xls_dslx_test( name = \"e_dslx_test\", srcs = [ \"d.x\", \"e.x\", ], deps = [\":bc_dslx\"], ) xls_dslx_test on a xls_dslx_library. xls_dslx_library( name = \"b_dslx\", srcs = [\"b.x\"], deps = [\":a_dslx\"], ) xls_dslx_test( name = \"b_dslx_test\", library = \"b_dslx\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required deps Dependency targets for the files in the 'srcs' attribute. This attribute is mutually exclusive with the 'library' attribute. List of labels optional [] dslx_test_args Arguments of the DSLX interpreter executable. For details on the arguments, refer to the interpreter_main application at //xls/dslx/interpreter_main.cc. Dictionary: String -> String optional {} library A DSLX library target where the direct (non-transitive) files of the target are tested. This attribute is mutually exclusive with the 'srcs' and 'deps' attribute. Label optional None srcs Source files for the rule. The files must have a '.x' extension. This attribute is mutually exclusive with the 'library' attribute. List of labels optional [] xls_eval_ir_test xls_eval_ir_test( name , input_validator , input_validator_expr , ir_eval_args , src , top ) Executes the IR interpreter on an IR file. Examples: A file as the source. xls_eval_ir_test( name = \"a_eval_ir_test\", src = \"a.ir\", ) An xls_ir_opt_ir target as the source. xls_ir_opt_ir( name = \"a_opt_ir\", src = \"a.ir\", ) xls_eval_ir_test( name = \"a_eval_ir_test\", src = \":a_opt_ir\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required input_validator The DSLX library defining the input validator for this test. Mutually exclusive with \"input_validator_expr\". Label optional None input_validator_expr The expression to validate an input for the test function. Mutually exclusive with \"input_validator\". String optional \"\" ir_eval_args Arguments of the IR interpreter. For details on the arguments, refer to the eval_ir_main application at //xls/tools/eval_ir_main.cc.The 'top' argument is not assigned using this attribute. Dictionary: String -> String optional {\"random_inputs\": \"100\", \"optimize_ir\": \"true\"} src The IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required top The ( mangled ) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional \"\" xls_ir_equivalence_test xls_ir_equivalence_test( name , ir_equivalence_args , src_0 , src_1 , top ) Executes the equivalence tool on two IR files. Examples: A file as the source. xls_ir_equivalence_test( name = \"ab_ir_equivalence_test\", src_0 = \"a.ir\", src_1 = \"b.ir\", ) A target as the source. xls_dslx_ir( name = \"b_ir\", srcs = [\"b.x\"], ) xls_ir_equivalence_test( name = \"ab_ir_equivalence_test\", src_0 = \"a.ir\", src_1 = \":b_ir\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required ir_equivalence_args Arguments of the IR equivalence tool. For details on the arguments, refer to the check_ir_equivalence_main application at //xls/tools/check_ir_equivalence_main.cc. The 'function' argument is not assigned using this attribute. Dictionary: String -> String optional {} src_0 An IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required src_1 An IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required top The ( mangled ) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional \"\" cc_xls_ir_jit_wrapper cc_xls_ir_jit_wrapper( name , src , jit_wrapper_args , kwargs ) Invokes the JIT wrapper generator and compiles the result as a cc_library. The macro invokes the JIT wrapper generator on an IR source file. The generated source files are the inputs to a cc_library with its target name identical to this macro. PARAMETERS Name Description Default Value name The name of the cc_library target. none src The path to the IR file. none jit_wrapper_args Arguments of the JIT wrapper tool. Note: argument 'output_name' cannot be defined. {} kwargs Keyword arguments. Named arguments. none get_mangled_ir_symbol get_mangled_ir_symbol( module_name , function_name , parametric_values , is_implicit_token , is_proc_next ) Returns the mangled IR symbol for the module/function combination. \"Mangling\" is the process of turning nicely namedspaced symbols into \"grosser\" (mangled) flat (non hierarchical) symbol, e.g. that lives on a package after IR conversion. To retrieve/execute functions that have been IR converted, we use their mangled names to refer to them in the IR namespace. PARAMETERS Name Description Default Value module_name The DSLX module name that the function is within. none function_name The DSLX function name within the module. none parametric_values Any parametric values used for instantiation (e.g. for a parametric entry point that is known to be instantiated in the IR converted module). This is generally for more advanced use cases like internals testing. The argument is mutually exclusive with argument 'is_proc_next'. None is_implicit_token A boolean flag denoting whether the symbol contains an implicit token. The argument is mutually exclusive with argument 'is_proc_next'. False is_proc_next A boolean flag denoting whether the symbol is a next proc function. The argument is mutually exclusive with arguments: 'parametric_values' and 'is_implicit_token'. False RETURNS The \"mangled\" symbol string. xls_dslx_cpp_type_library xls_dslx_cpp_type_library( name , src ) Creates a cc_library target for transpiled DSLX types. This macros invokes the DSLX-to-C++ transpiler and compiles the result as a cc_library with its target name identical to this macro. PARAMETERS Name Description Default Value name The name of the eventual cc_library. none src The DSLX file whose types to compile as C++. none xls_dslx_ir xls_dslx_ir( name , dslx_top , srcs , deps , library , ir_conv_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule converting a DSLX source file to an IR file. The macro instantiates a rule that converts a DSLX source file to an IR file. The macro also instantiates the 'enable_generated_file_wrapper' function. The generated files are listed in the outs attribute of the rule. Example: An IR conversion with a top entity defined. ``` # Assume a xls_dslx_library target bc_dslx is present. xls_dslx_ir( name = \"d_ir\", srcs = [\"d.x\"], deps = [\":bc_dslx\"], dslx_top = \"d\", ) ``` PARAMETERS Name Description Default Value name The name of the rule. none dslx_top The top entity to perform the IR conversion. none srcs Top level source files for the conversion. Files must have a '.x' extension. There must be single source file. None deps Dependency targets for the files in the 'srcs' argument. None library A DSLX library target where the direct (non-transitive) files of the target are tested. This argument is mutually exclusive with the 'srcs' and 'deps' arguments. None ir_conv_args Arguments of the IR conversion tool. For details on the arguments, refer to the ir_converter_main application at //xls/dslx/ir_converter_main.cc. Note: the 'top' argument is not assigned using this attribute. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none xls_dslx_opt_ir xls_dslx_opt_ir( name , dslx_top , srcs , deps , library , ir_conv_args , opt_ir_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule generating an optimized IR file from a DSLX source file. The macro instantiates a build rule that generates an optimized IR file from a DSLX source file. The build rule executes the core functionality of following macros: xls_dslx_ir (converts a DSLX file to an IR), and, xls_ir_opt_ir (optimizes the IR). The macro also instantiates the 'enable_generated_file_wrapper' function. The generated files are listed in the outs attribute of the rule. Examples: A simple example. # Assume a xls_dslx_library target bc_dslx is present. xls_dslx_opt_ir( name = \"d_opt_ir\", srcs = [\"d.x\"], deps = [\":bc_dslx\"], dslx_top = \"d\", ) PARAMETERS Name Description Default Value name The name of the rule. none dslx_top The top entity to perform the IR conversion. none srcs Top level source files for the conversion. Files must have a '.x' extension. There must be single source file. None deps Dependency targets for the files in the 'srcs' argument. None library A DSLX library target where the direct (non-transitive) files of the target are tested. This argument is mutually exclusive with the 'srcs' and 'deps' arguments. None ir_conv_args Arguments of the IR conversion tool. For details on the arguments, refer to the ir_converter_main application at //xls/dslx/ir_converter_main.cc. Note: the 'top' argument is not assigned using this attribute. {} opt_ir_args Arguments of the IR optimizer tool. For details on the arguments, refer to the opt_main application at //xls/tools/opt_main.cc. Note: the 'top' argument is not assigned using this attribute. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none xls_dslx_verilog xls_dslx_verilog( name , dslx_top , verilog_file , srcs , deps , library , ir_conv_args , opt_ir_args , codegen_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule generating a Verilog file from a DSLX source file and tests the build. The macro instantiates a build rule that generates a Verilog file from a DSLX source file. The build rule executes the core functionality of following macros: xls_dslx_ir (converts a DSLX file to an IR), xls_ir_opt_ir (optimizes the IR), and, xls_ir_verilog (generated a Verilog file). The macro also instantiates a 'build_test' testing that the build rule generating a Verilog file. If the build is not successful, an error is produced when executing a test command on the target. Examples: A simple example. # Assume a xls_dslx_library target bc_dslx is present. xls_dslx_verilog( name = \"d_verilog\", srcs = [\"d.x\"], deps = [\":bc_dslx\"], codegen_args = { \"pipeline_stages\": \"1\", }, dslx_top = \"d\", ) PARAMETERS Name Description Default Value name The name of the rule. none dslx_top The top entity to perform the IR conversion. none verilog_file The filename of Verilog file generated. The filename must have a '.v' extension. none srcs Top level source files for the conversion. Files must have a '.x' extension. There must be single source file. None deps Dependency targets for the files in the 'srcs' argument. None library A DSLX library target where the direct (non-transitive) files of the target are tested. This argument is mutually exclusive with the 'srcs' and 'deps' arguments. None ir_conv_args Arguments of the IR conversion tool. For details on the arguments, refer to the ir_converter_main application at //xls/dslx/ir_converter_main.cc. Note: the 'top' argument is not assigned using this attribute. {} opt_ir_args Arguments of the IR optimizer tool. For details on the arguments, refer to the opt_main application at //xls/tools/opt_main.cc. Note: the 'top' argument is not assigned using this attribute. {} codegen_args Arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none xls_ir_cc_library xls_ir_cc_library( name , src , top , namespaces ) Invokes the AOT compiles the input IR into a cc_library. Example: xls_ir_opt_ir( name \"foo\", ... ) xls_ir_cc_library_macro( name = \"foo_cc\", src = \":foo.opt.ir\", top = \"bar\", namespaces = \"a,b,c\", ) This will produce a cc_library that will execute the fn bar from the foo IR file. The call itself will be inside the namespace a::b::c . PARAMETERS Name Description Default Value name The name of the resulting library. none src The path to the IR file to compile. none top The entry point in the IR file of interest. None namespaces A comma-separated list of namespaces into which the generated code should go. \"\" xls_ir_opt_ir xls_ir_opt_ir( name , src , opt_ir_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule optimizing an IR file. The macro instantiates a build rule that optimizes an IR file. The macro also instantiates the 'enable_generated_file_wrapper' function. The generated files are listed in the outs attribute of the rule. Examples: A simple example. xls_ir_opt_ir( name = \"a_opt_ir\", src = \"a.ir\", ) Optimizing an IR file with an top entity defined. xls_ir_opt_ir( name = \"a_opt_ir\", src = \"a.ir\", opt_ir_args = { \"inline_procs\" : \"true\", }, ) PARAMETERS Name Description Default Value name The name of the rule. none src The IR source file. A single source file must be provided. The file must have a '.ir' extension. none opt_ir_args Arguments of the IR optimizer tool. For details on the arguments, refer to the opt_main application at //xls/tools/opt_main.cc. Note: the 'top' argument is not assigned using this attribute. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none xls_ir_verilog xls_ir_verilog( name , src , verilog_file , codegen_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule generating a Verilog file from an IR file and tests the build. The macro instantiates a build rule that generate a Verilog file from an IR file, and a 'build_test' testing that the build rule generating a Verilog file. If the build is not successful, an error is produced when executing a test command on the target. Example: ``` xls_ir_verilog( name = \"a_verilog\", src = \"a.ir\", codegen_args = { \"pipeline_stages\": \"1\", ... }, ) ``` PARAMETERS Name Description Default Value name The name of the rule. none src The IR source file. A single source file must be provided. The file must have a '.ir' extension. none verilog_file The filename of Verilog file generated. The filename must have a '.v' or '.sv', extension. none codegen_args Arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none","title":"Bazel Rules And Macros"},{"location":"bazel_rules_macros/#bazel-rules-and-macros","text":"","title":"Bazel Rules And Macros"},{"location":"bazel_rules_macros/#check_sha256sum_frozen","text":"check_sha256sum_frozen( name , frozen_file , sha256sum , src ) Produces a frozen file if the sha256sum checksum of a source file matches a user-defined checksum. As projects cut releases or freeze, it's important to know that generated (e.g. Verilog) code is never changing without having to actually check in the generated artifact. This rule performs a checksum of a generated file as an integrity check. Users might use this rule to help enable confidence that there is neither: non-determinism in the toolchain, nor an accidental dependence on a non-released toolchain (e.g. an accidental dependence on top-of-tree, where the toolchain is constantly changing) Say there was a codegen rule producing my_output.v , a user might instantiate something like: check_sha256sum_frozen( name = \"my_output_checksum\", src = \":my_output.v\", sha256sum = \"d1bc8d3ba4afc7e109612cb73acbdddac052c93025aa1f82942edabb7deb82a1\", frozen_file = \"my_output.frozen.x\", ) ... and then take a dependency on my_output.frozen.v in the surrounding project, knowing that it had been checksum-verified. Taking a dependence on my_output.v directly may also be ok if the :my_output_checksum target is also built (e.g. via the same wildcard build request), but taking a dependence on the output .frozen.v file ensures that the checking is an integral part of the downstream build-artifact-creation process. At its core, this rule ensure that the contents of a file does not change by verifying that it matches a given checksum. Typically, this rule is used to control the build process. The rule serves as a trigger on rules depending on its output (the frozen file). When the validation of the sha256sum succeed, rules depending on the frozen file are built/executed. When the validation of the sha256sum fails, rules depending on the frozen file are not built/executed. In the example below, when the validation of the sha256sum for target 'generated_file_sha256sum_frozen' succeeds, target 'generated_file_dslx' is built. However, when the validation of the sha256sum for target 'generated_file_sha256sum_frozen' fails, target 'generated_file_dslx' is not built. Examples: A simple example. check_sha256sum_frozen( name = \"generated_file_sha256sum_frozen\", src = \":generated_file.x\", sha256sum = \"6522799f7b64dbbb2a31eb2862052b8988e78821d8b61fff7f508237a9d9f01d\", frozen_file = \"generated_file.frozen.x\", ) dslx_library( name = \"generated_file_dslx\", src = \":generated_file.frozen.x\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required frozen_file The frozen output file. Label required sha256sum The sha256sum of the source file. String required src The source file. Label required","title":"check_sha256sum_frozen"},{"location":"bazel_rules_macros/#check_sha256sum_test","text":"check_sha256sum_test( name , sha256sum , src ) Validates the sha256sum checksum of a source file with a user-defined checksum. This rule is typically used to ensure that the contents of a file is unchanged. Examples: A simple example. check_sha256sum_test( name = \"generated_file_sha256sum_test\", src = \":generated_file.x\", sha256sum = \"6522799f7b64dbbb2a31eb2862052b8988e78821d8b61fff7f508237a9d9f01d\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required sha256sum The sha256sum of the source file. String required src The source file. Label required","title":"check_sha256sum_test"},{"location":"bazel_rules_macros/#proto_data","text":"proto_data( name , protobin_file , src ) Converts a proto text with a xlscc.HLSBlock message to a proto binary. This rules is used in conjunction with the (e.g. xls_cc_ir and xls_cc_verilog) rules and xls_cc_* (e.g. xls_cc_ir_macro and xls_cc_verilog_macro) macros. Examples: A simple example. proto_data( name = \"packet_selector_block_pb\", src = \"packet_selector.textproto\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required protobin_file The name of the output file to write binary proto to. If not specified, the target name of the bazel rule followed by an .protobin extension is used. Label optional src The source file. Label required","title":"proto_data"},{"location":"bazel_rules_macros/#xls_benchmark_ir","text":"xls_benchmark_ir( name , benchmark_ir_args , src , top ) Executes the benchmark tool on an IR file. Examples: A file as the source. xls_benchmark_ir( name = \"a_benchmark\", src = \"a.ir\", ) An xls_ir_opt_ir target as the source. xls_ir_opt_ir( name = \"a_opt_ir\", src = \"a.ir\", ) xls_benchmark_ir( name = \"a_benchmark\", src = \":a_opt_ir\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required benchmark_ir_args Arguments of the benchmark IR tool. For details on the arguments, refer to the benchmark_main application at //xls/tools/benchmark_main.cc. Dictionary: String -> String optional {} src The IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required top The ( mangled ) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional \"\"","title":"xls_benchmark_ir"},{"location":"bazel_rules_macros/#xls_benchmark_verilog","text":"xls_benchmark_verilog( name , verilog_target ) Computes and prints various metrics about a Verilog target. Example: xls_benchmark_verilog( name = \"a_benchmark\", verilog_target = \"a_verilog_target\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required verilog_target The verilog target to benchmark. Label optional None","title":"xls_benchmark_verilog"},{"location":"bazel_rules_macros/#xls_dslx_library","text":"xls_dslx_library( name , deps , srcs , warnings_as_errors ) A build rule that parses and type checks DSLX source files. Examples: A collection of DSLX source files. xls_dslx_library( name = \"files_123_dslx\", srcs = [ \"file_1.x\", \"file_2.x\", \"file_3.x\", ], ) Dependency on other xls_dslx_library targets. xls_dslx_library( name = \"a_dslx\", srcs = [\"a.x\"], ) # Depends on target a_dslx. xls_dslx_library( name = \"b_dslx\", srcs = [\"b.x\"], deps = [\":a_dslx\"], ) # Depends on target a_dslx. xls_dslx_library( name = \"c_dslx\", srcs = [\"c.x\"], deps = [\":a_dslx\"], ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required deps Dependency targets for the rule. List of labels optional [] srcs Source files for the rule. Files must have a '.x' extension. List of labels optional [] warnings_as_errors Whether warnings are errors within this library definition. Boolean optional False","title":"xls_dslx_library"},{"location":"bazel_rules_macros/#xls_dslx_opt_ir_test","text":"xls_dslx_opt_ir_test( name , benchmark_ir_args , dep , dslx_test_args , input_validator , input_validator_expr , ir_equivalence_args , ir_eval_args , top ) A build rule that tests a xls_dslx_opt_ir target. Executes the test commands for the following rules in the order presented: xls_dslx_test xls_ir_equivalence_test xls_eval_ir_test xls_benchmark_ir Examples: A simple example. xls_dslx_opt_ir( name = \"a_opt_ir\", srcs = [\"a.x\"], dslx_top = \"a\", ) xls_dslx_opt_ir_test( name = \"a_opt_ir_test\", dep = \":a_opt_ir\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required benchmark_ir_args Arguments of the benchmark IR tool. For details on the arguments, refer to the benchmark_main application at //xls/tools/benchmark_main.cc. Dictionary: String -> String optional {} dep The xls_dslx_opt_ir target to test. Label optional None dslx_test_args Arguments of the DSLX interpreter executable. For details on the arguments, refer to the interpreter_main application at //xls/dslx/interpreter_main.cc. Dictionary: String -> String optional {} input_validator The DSLX library defining the input validator for this test. Mutually exclusive with \"input_validator_expr\". Label optional None input_validator_expr The expression to validate an input for the test function. Mutually exclusive with \"input_validator\". String optional \"\" ir_equivalence_args Arguments of the IR equivalence tool. For details on the arguments, refer to the check_ir_equivalence_main application at //xls/tools/check_ir_equivalence_main.cc. The 'function' argument is not assigned using this attribute. Dictionary: String -> String optional {} ir_eval_args Arguments of the IR interpreter. For details on the arguments, refer to the eval_ir_main application at //xls/tools/eval_ir_main.cc.The 'top' argument is not assigned using this attribute. Dictionary: String -> String optional {\"random_inputs\": \"100\", \"optimize_ir\": \"true\"} top The ( mangled ) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional \"\"","title":"xls_dslx_opt_ir_test"},{"location":"bazel_rules_macros/#xls_dslx_test","text":"xls_dslx_test( name , deps , dslx_test_args , library , srcs ) A dslx test executes the tests and quick checks of a DSLX source file. Examples: xls_dslx_test on DSLX source files. # Assume a xls_dslx_library target bc_dslx is present. xls_dslx_test( name = \"e_dslx_test\", srcs = [ \"d.x\", \"e.x\", ], deps = [\":bc_dslx\"], ) xls_dslx_test on a xls_dslx_library. xls_dslx_library( name = \"b_dslx\", srcs = [\"b.x\"], deps = [\":a_dslx\"], ) xls_dslx_test( name = \"b_dslx_test\", library = \"b_dslx\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required deps Dependency targets for the files in the 'srcs' attribute. This attribute is mutually exclusive with the 'library' attribute. List of labels optional [] dslx_test_args Arguments of the DSLX interpreter executable. For details on the arguments, refer to the interpreter_main application at //xls/dslx/interpreter_main.cc. Dictionary: String -> String optional {} library A DSLX library target where the direct (non-transitive) files of the target are tested. This attribute is mutually exclusive with the 'srcs' and 'deps' attribute. Label optional None srcs Source files for the rule. The files must have a '.x' extension. This attribute is mutually exclusive with the 'library' attribute. List of labels optional []","title":"xls_dslx_test"},{"location":"bazel_rules_macros/#xls_eval_ir_test","text":"xls_eval_ir_test( name , input_validator , input_validator_expr , ir_eval_args , src , top ) Executes the IR interpreter on an IR file. Examples: A file as the source. xls_eval_ir_test( name = \"a_eval_ir_test\", src = \"a.ir\", ) An xls_ir_opt_ir target as the source. xls_ir_opt_ir( name = \"a_opt_ir\", src = \"a.ir\", ) xls_eval_ir_test( name = \"a_eval_ir_test\", src = \":a_opt_ir\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required input_validator The DSLX library defining the input validator for this test. Mutually exclusive with \"input_validator_expr\". Label optional None input_validator_expr The expression to validate an input for the test function. Mutually exclusive with \"input_validator\". String optional \"\" ir_eval_args Arguments of the IR interpreter. For details on the arguments, refer to the eval_ir_main application at //xls/tools/eval_ir_main.cc.The 'top' argument is not assigned using this attribute. Dictionary: String -> String optional {\"random_inputs\": \"100\", \"optimize_ir\": \"true\"} src The IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required top The ( mangled ) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional \"\"","title":"xls_eval_ir_test"},{"location":"bazel_rules_macros/#xls_ir_equivalence_test","text":"xls_ir_equivalence_test( name , ir_equivalence_args , src_0 , src_1 , top ) Executes the equivalence tool on two IR files. Examples: A file as the source. xls_ir_equivalence_test( name = \"ab_ir_equivalence_test\", src_0 = \"a.ir\", src_1 = \"b.ir\", ) A target as the source. xls_dslx_ir( name = \"b_ir\", srcs = [\"b.x\"], ) xls_ir_equivalence_test( name = \"ab_ir_equivalence_test\", src_0 = \"a.ir\", src_1 = \":b_ir\", ) ATTRIBUTES Name Description Type Mandatory Default name A unique name for this target. Name required ir_equivalence_args Arguments of the IR equivalence tool. For details on the arguments, refer to the check_ir_equivalence_main application at //xls/tools/check_ir_equivalence_main.cc. The 'function' argument is not assigned using this attribute. Dictionary: String -> String optional {} src_0 An IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required src_1 An IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required top The ( mangled ) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional \"\"","title":"xls_ir_equivalence_test"},{"location":"bazel_rules_macros/#cc_xls_ir_jit_wrapper","text":"cc_xls_ir_jit_wrapper( name , src , jit_wrapper_args , kwargs ) Invokes the JIT wrapper generator and compiles the result as a cc_library. The macro invokes the JIT wrapper generator on an IR source file. The generated source files are the inputs to a cc_library with its target name identical to this macro. PARAMETERS Name Description Default Value name The name of the cc_library target. none src The path to the IR file. none jit_wrapper_args Arguments of the JIT wrapper tool. Note: argument 'output_name' cannot be defined. {} kwargs Keyword arguments. Named arguments. none","title":"cc_xls_ir_jit_wrapper"},{"location":"bazel_rules_macros/#get_mangled_ir_symbol","text":"get_mangled_ir_symbol( module_name , function_name , parametric_values , is_implicit_token , is_proc_next ) Returns the mangled IR symbol for the module/function combination. \"Mangling\" is the process of turning nicely namedspaced symbols into \"grosser\" (mangled) flat (non hierarchical) symbol, e.g. that lives on a package after IR conversion. To retrieve/execute functions that have been IR converted, we use their mangled names to refer to them in the IR namespace. PARAMETERS Name Description Default Value module_name The DSLX module name that the function is within. none function_name The DSLX function name within the module. none parametric_values Any parametric values used for instantiation (e.g. for a parametric entry point that is known to be instantiated in the IR converted module). This is generally for more advanced use cases like internals testing. The argument is mutually exclusive with argument 'is_proc_next'. None is_implicit_token A boolean flag denoting whether the symbol contains an implicit token. The argument is mutually exclusive with argument 'is_proc_next'. False is_proc_next A boolean flag denoting whether the symbol is a next proc function. The argument is mutually exclusive with arguments: 'parametric_values' and 'is_implicit_token'. False RETURNS The \"mangled\" symbol string.","title":"get_mangled_ir_symbol"},{"location":"bazel_rules_macros/#xls_dslx_cpp_type_library","text":"xls_dslx_cpp_type_library( name , src ) Creates a cc_library target for transpiled DSLX types. This macros invokes the DSLX-to-C++ transpiler and compiles the result as a cc_library with its target name identical to this macro. PARAMETERS Name Description Default Value name The name of the eventual cc_library. none src The DSLX file whose types to compile as C++. none","title":"xls_dslx_cpp_type_library"},{"location":"bazel_rules_macros/#xls_dslx_ir","text":"xls_dslx_ir( name , dslx_top , srcs , deps , library , ir_conv_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule converting a DSLX source file to an IR file. The macro instantiates a rule that converts a DSLX source file to an IR file. The macro also instantiates the 'enable_generated_file_wrapper' function. The generated files are listed in the outs attribute of the rule. Example: An IR conversion with a top entity defined. ``` # Assume a xls_dslx_library target bc_dslx is present. xls_dslx_ir( name = \"d_ir\", srcs = [\"d.x\"], deps = [\":bc_dslx\"], dslx_top = \"d\", ) ``` PARAMETERS Name Description Default Value name The name of the rule. none dslx_top The top entity to perform the IR conversion. none srcs Top level source files for the conversion. Files must have a '.x' extension. There must be single source file. None deps Dependency targets for the files in the 'srcs' argument. None library A DSLX library target where the direct (non-transitive) files of the target are tested. This argument is mutually exclusive with the 'srcs' and 'deps' arguments. None ir_conv_args Arguments of the IR conversion tool. For details on the arguments, refer to the ir_converter_main application at //xls/dslx/ir_converter_main.cc. Note: the 'top' argument is not assigned using this attribute. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none","title":"xls_dslx_ir"},{"location":"bazel_rules_macros/#xls_dslx_opt_ir","text":"xls_dslx_opt_ir( name , dslx_top , srcs , deps , library , ir_conv_args , opt_ir_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule generating an optimized IR file from a DSLX source file. The macro instantiates a build rule that generates an optimized IR file from a DSLX source file. The build rule executes the core functionality of following macros: xls_dslx_ir (converts a DSLX file to an IR), and, xls_ir_opt_ir (optimizes the IR). The macro also instantiates the 'enable_generated_file_wrapper' function. The generated files are listed in the outs attribute of the rule. Examples: A simple example. # Assume a xls_dslx_library target bc_dslx is present. xls_dslx_opt_ir( name = \"d_opt_ir\", srcs = [\"d.x\"], deps = [\":bc_dslx\"], dslx_top = \"d\", ) PARAMETERS Name Description Default Value name The name of the rule. none dslx_top The top entity to perform the IR conversion. none srcs Top level source files for the conversion. Files must have a '.x' extension. There must be single source file. None deps Dependency targets for the files in the 'srcs' argument. None library A DSLX library target where the direct (non-transitive) files of the target are tested. This argument is mutually exclusive with the 'srcs' and 'deps' arguments. None ir_conv_args Arguments of the IR conversion tool. For details on the arguments, refer to the ir_converter_main application at //xls/dslx/ir_converter_main.cc. Note: the 'top' argument is not assigned using this attribute. {} opt_ir_args Arguments of the IR optimizer tool. For details on the arguments, refer to the opt_main application at //xls/tools/opt_main.cc. Note: the 'top' argument is not assigned using this attribute. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none","title":"xls_dslx_opt_ir"},{"location":"bazel_rules_macros/#xls_dslx_verilog","text":"xls_dslx_verilog( name , dslx_top , verilog_file , srcs , deps , library , ir_conv_args , opt_ir_args , codegen_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule generating a Verilog file from a DSLX source file and tests the build. The macro instantiates a build rule that generates a Verilog file from a DSLX source file. The build rule executes the core functionality of following macros: xls_dslx_ir (converts a DSLX file to an IR), xls_ir_opt_ir (optimizes the IR), and, xls_ir_verilog (generated a Verilog file). The macro also instantiates a 'build_test' testing that the build rule generating a Verilog file. If the build is not successful, an error is produced when executing a test command on the target. Examples: A simple example. # Assume a xls_dslx_library target bc_dslx is present. xls_dslx_verilog( name = \"d_verilog\", srcs = [\"d.x\"], deps = [\":bc_dslx\"], codegen_args = { \"pipeline_stages\": \"1\", }, dslx_top = \"d\", ) PARAMETERS Name Description Default Value name The name of the rule. none dslx_top The top entity to perform the IR conversion. none verilog_file The filename of Verilog file generated. The filename must have a '.v' extension. none srcs Top level source files for the conversion. Files must have a '.x' extension. There must be single source file. None deps Dependency targets for the files in the 'srcs' argument. None library A DSLX library target where the direct (non-transitive) files of the target are tested. This argument is mutually exclusive with the 'srcs' and 'deps' arguments. None ir_conv_args Arguments of the IR conversion tool. For details on the arguments, refer to the ir_converter_main application at //xls/dslx/ir_converter_main.cc. Note: the 'top' argument is not assigned using this attribute. {} opt_ir_args Arguments of the IR optimizer tool. For details on the arguments, refer to the opt_main application at //xls/tools/opt_main.cc. Note: the 'top' argument is not assigned using this attribute. {} codegen_args Arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none","title":"xls_dslx_verilog"},{"location":"bazel_rules_macros/#xls_ir_cc_library","text":"xls_ir_cc_library( name , src , top , namespaces ) Invokes the AOT compiles the input IR into a cc_library. Example: xls_ir_opt_ir( name \"foo\", ... ) xls_ir_cc_library_macro( name = \"foo_cc\", src = \":foo.opt.ir\", top = \"bar\", namespaces = \"a,b,c\", ) This will produce a cc_library that will execute the fn bar from the foo IR file. The call itself will be inside the namespace a::b::c . PARAMETERS Name Description Default Value name The name of the resulting library. none src The path to the IR file to compile. none top The entry point in the IR file of interest. None namespaces A comma-separated list of namespaces into which the generated code should go. \"\"","title":"xls_ir_cc_library"},{"location":"bazel_rules_macros/#xls_ir_opt_ir","text":"xls_ir_opt_ir( name , src , opt_ir_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule optimizing an IR file. The macro instantiates a build rule that optimizes an IR file. The macro also instantiates the 'enable_generated_file_wrapper' function. The generated files are listed in the outs attribute of the rule. Examples: A simple example. xls_ir_opt_ir( name = \"a_opt_ir\", src = \"a.ir\", ) Optimizing an IR file with an top entity defined. xls_ir_opt_ir( name = \"a_opt_ir\", src = \"a.ir\", opt_ir_args = { \"inline_procs\" : \"true\", }, ) PARAMETERS Name Description Default Value name The name of the rule. none src The IR source file. A single source file must be provided. The file must have a '.ir' extension. none opt_ir_args Arguments of the IR optimizer tool. For details on the arguments, refer to the opt_main application at //xls/tools/opt_main.cc. Note: the 'top' argument is not assigned using this attribute. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none","title":"xls_ir_opt_ir"},{"location":"bazel_rules_macros/#xls_ir_verilog","text":"xls_ir_verilog( name , src , verilog_file , codegen_args , enable_generated_file , enable_presubmit_generated_file , kwargs ) A macro that instantiates a build rule generating a Verilog file from an IR file and tests the build. The macro instantiates a build rule that generate a Verilog file from an IR file, and a 'build_test' testing that the build rule generating a Verilog file. If the build is not successful, an error is produced when executing a test command on the target. Example: ``` xls_ir_verilog( name = \"a_verilog\", src = \"a.ir\", codegen_args = { \"pipeline_stages\": \"1\", ... }, ) ``` PARAMETERS Name Description Default Value name The name of the rule. none src The IR source file. A single source file must be provided. The file must have a '.ir' extension. none verilog_file The filename of Verilog file generated. The filename must have a '.v' or '.sv', extension. none codegen_args Arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. {} enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. True enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. False kwargs Keyword arguments. Named arguments. none","title":"xls_ir_verilog"},{"location":"build_system/","text":"Build system XLS uses the Bazel build system for itself and all its dependencies. Bazel is an easy to configure and use, and has powerful extension facilities. (It's also well-documented !) XLS provides a number of Starlark rules and macros to define a build flow. Build system Whirlwind Intro To Bazel Where the output files go XLS Project Build Rules Bazel queries Finding transitive dependencies Finding dependees (\"reverse dependencies\") Whirlwind Intro To Bazel Many developers are familiar with a make-style build flow. Bazel, by contrast, provides more built-in structure for where generated files and binary artifacts are placed, in order to keep the source tree unmodified and the build process fully declarative / repeatable. In Bazel, one of the key principles is \"the user should not need to bazel clean \". A typical build command looks like: $ bazel build -c opt //xls/tools:opt_main The -c opt flag is requesting we produce an optimized build. Other options for development are: -c fastbuild : fewer optimizations, quicker turn around time on builds, and -c dbg : debug binaries, minimal optimization level and debug information produced, e.g. for using binaries under gdb Targets are referenced with // as the root of the current repository -- it is generally optional. From there you specify the path to a directory with a BUILD file, and then :target_name to reference a named target within that BUILD file. In the case above, the build target referenced is a C++ binary -- its build definition is described by a cc_binary rule in the xls/tools/BUILD file. Where the output files go The above command notes the following in its output: Target //xls/tools:opt_main up-to-date: bazel-bin/xls/tools/opt_main We can see binary result files go to bazel-bin within our repository's root directory. (Aside: bazel-bin is a convenient symlink to an out-of-tree location where build artifacts are placed.) Generated files that are intermediate entities in the build process are also visible via a similar symlink, bazel-out . Within the following directory: $ ls bazel-out/host/bin/xls/ir/ We can see files that were part of the build of the IR library, like op.h and op.cc . XLS Project Build Rules XLS provides a set of Bazel build rules and macros that allow users to quickly/easily create XLS-based design artifacts -- analogous to the way C++, Python, etc are done in Bazel. For example, dslx_library lets a user make a library target written in XLS' Domain Specific Language frontend. XLS build rules and macros are defined in xls/build_rules/xls_build_defs.bzl . Examples using the rules and macros are found at xls/build_rules/tests/BUILD . A detailed description of the bazel rules/macros can be found here . Bazel queries Understanding the build tree for a new project can be difficult, but fortunately Bazel provides a powerful query mechanism . bazel query enables a user to examine build targets, dependencies between them, and much more. A few usage examples are provided here, but the full documentation (linked above) is comprehensive. Finding transitive dependencies To understand why, for example, the combinational verilog generator depends on the ABSL container algorithm library, one could run: $ bazel query 'somepath(//xls/codegen:combinational_generator, @com_google_absl//absl/algorithm:container)' //xls/codegen:combinational_generator //xls/codegen:vast @com_google_absl//absl/algorithm:container This result shows that one such path goes through the :vast target. Another such path goes through the xls/ir:ir target, then the xls/ir:value target. somepath provides some path, not all paths (that's what allpaths is for). Finding dependees (\"reverse dependencies\") Sometimes it's useful to identify the set of targets depending on some other target - the rdeps query performs this: $ bazel query 'rdeps(//xls/codegen:all, //xls/codegen:combinational_generator)' //xls/codegen:flattening_test //xls/ir:ir_test_base //xls/codegen:combinational_generator_test //xls/codegen:combinational_generator This shows the transitive closure of all dependencies of the combinational generator, with the starting set being all targets in //xls/codegen:all . This set of dependencies can quickly grow to be unmanageable, so keep the initial set (the first argument) as small as possible, and consider specifying a third argument for maximum search depth.","title":"Build System"},{"location":"build_system/#build-system","text":"XLS uses the Bazel build system for itself and all its dependencies. Bazel is an easy to configure and use, and has powerful extension facilities. (It's also well-documented !) XLS provides a number of Starlark rules and macros to define a build flow. Build system Whirlwind Intro To Bazel Where the output files go XLS Project Build Rules Bazel queries Finding transitive dependencies Finding dependees (\"reverse dependencies\")","title":"Build system"},{"location":"build_system/#whirlwind-intro-to-bazel","text":"Many developers are familiar with a make-style build flow. Bazel, by contrast, provides more built-in structure for where generated files and binary artifacts are placed, in order to keep the source tree unmodified and the build process fully declarative / repeatable. In Bazel, one of the key principles is \"the user should not need to bazel clean \". A typical build command looks like: $ bazel build -c opt //xls/tools:opt_main The -c opt flag is requesting we produce an optimized build. Other options for development are: -c fastbuild : fewer optimizations, quicker turn around time on builds, and -c dbg : debug binaries, minimal optimization level and debug information produced, e.g. for using binaries under gdb Targets are referenced with // as the root of the current repository -- it is generally optional. From there you specify the path to a directory with a BUILD file, and then :target_name to reference a named target within that BUILD file. In the case above, the build target referenced is a C++ binary -- its build definition is described by a cc_binary rule in the xls/tools/BUILD file.","title":"Whirlwind Intro To Bazel"},{"location":"build_system/#where-the-output-files-go","text":"The above command notes the following in its output: Target //xls/tools:opt_main up-to-date: bazel-bin/xls/tools/opt_main We can see binary result files go to bazel-bin within our repository's root directory. (Aside: bazel-bin is a convenient symlink to an out-of-tree location where build artifacts are placed.) Generated files that are intermediate entities in the build process are also visible via a similar symlink, bazel-out . Within the following directory: $ ls bazel-out/host/bin/xls/ir/ We can see files that were part of the build of the IR library, like op.h and op.cc .","title":"Where the output files go"},{"location":"build_system/#xls-project-build-rules","text":"XLS provides a set of Bazel build rules and macros that allow users to quickly/easily create XLS-based design artifacts -- analogous to the way C++, Python, etc are done in Bazel. For example, dslx_library lets a user make a library target written in XLS' Domain Specific Language frontend. XLS build rules and macros are defined in xls/build_rules/xls_build_defs.bzl . Examples using the rules and macros are found at xls/build_rules/tests/BUILD . A detailed description of the bazel rules/macros can be found here .","title":"XLS Project Build Rules"},{"location":"build_system/#bazel-queries","text":"Understanding the build tree for a new project can be difficult, but fortunately Bazel provides a powerful query mechanism . bazel query enables a user to examine build targets, dependencies between them, and much more. A few usage examples are provided here, but the full documentation (linked above) is comprehensive.","title":"Bazel queries"},{"location":"build_system/#finding-transitive-dependencies","text":"To understand why, for example, the combinational verilog generator depends on the ABSL container algorithm library, one could run: $ bazel query 'somepath(//xls/codegen:combinational_generator, @com_google_absl//absl/algorithm:container)' //xls/codegen:combinational_generator //xls/codegen:vast @com_google_absl//absl/algorithm:container This result shows that one such path goes through the :vast target. Another such path goes through the xls/ir:ir target, then the xls/ir:value target. somepath provides some path, not all paths (that's what allpaths is for).","title":"Finding transitive dependencies"},{"location":"build_system/#finding-dependees-reverse-dependencies","text":"Sometimes it's useful to identify the set of targets depending on some other target - the rdeps query performs this: $ bazel query 'rdeps(//xls/codegen:all, //xls/codegen:combinational_generator)' //xls/codegen:flattening_test //xls/ir:ir_test_base //xls/codegen:combinational_generator_test //xls/codegen:combinational_generator This shows the transitive closure of all dependencies of the combinational generator, with the starting set being all targets in //xls/codegen:all . This set of dependencies can quickly grow to be unmanageable, so keep the initial set (the first argument) as small as possible, and consider specifying a third argument for maximum search depth.","title":"Finding dependees (\"reverse dependencies\")"},{"location":"codegen_options/","text":"Codegen Options This document outlines some useful knobs for running codegen on an XLS design. Codegen is the process of generating RTL from IR and is where operations are scheduled and mapped into RTL constructs. The output of codegen is suitable for simulation or implementation via standard tools that understand Verilog or SystemVerilog. Codegen Options Input specification Output locations Pipelining and Scheduling Options Naming Reset Signal Configuration Codegen Mapping Format Strings I/O Behavior RAMs (experimental) Optimization Input specification <input.ir> is a positional argument giving the path to the ir file. --top specifies the top function or proc to codegen. Output locations The following flags control where output files are put. In addition to Verilog, codegen can generate files useful for understanding or integrating the RTL. --output_verilog_path is the path to the output Verilog file. --output_schedule_path is the path to a textproto that shows into which pipeline stage the scheduler put IR ops. --output_block_ir_path is the path to the \"block IR\" representation of the design, a post-scheduling IR that is timed and includes registers, ports, etc. --output_signature_path is the path to the signature textproto. The signature describes the ports, channels, external memories, etc. --output_verilog_line_map_path is the path to the verilog line map associating lines of verilog to lines of IR. Pipelining and Scheduling Options The following flags control how XLS maps IR operations to RTL, and if applicable control the scheduler. --generator=... controls which generator to use. The options are pipeline and combinational . The pipeline generator runs a scheduler that partitions the IR ops into pipeline stages. --delay_model=... selects the delay model to use when scheduling. See the page here for more detail. --clock_period_ps=... sets the target clock period. See scheduling for more details on how scheduling works. Note that this option is optional, without specifying clock period XLS will estimate what the clock period should be. --pipeline_stages=... sets the number of pipeline stages to use when --generator=pipeline . --clock_margin_percent=... sets the percentage to reduce the target clock period before scheduling. See scheduling for more details. --period_relaxation_percent=... sets the percentage that the computed minimum clock period is increased. May not be specified with --clock_period_ps . --additional_input_delay_ps=... adds additional input delay to the inputs. This can be helpful to meet timing when integrating XLS designs with other RTL. --io_constraints=... adds constraints to the scheduler. The flag takes a comma-separated list of constraints of the form foo:send:bar:recv:3:5 which means that sends on channel foo must occur between 3 and 5 cycles (inclusive) before receives on channel bar . Note that for a constraint like foo:send:foo:send:3:5 , no constraint will be applied between a node and itself; i.e.: this means all different pairs of nodes sending on foo must be in cycles that differ by between 3 and 5. If the special minimum/maximum value none is used, then the minimum latency will be the lowest representable int64_t , and likewise for maximum latency. For an example of the use of this, see this example and the associated BUILD rule. Naming Some names can be set at codegen via the following flags: --module_name=... sets the name of the generated verilog module For functions, --input_valid_signal=... and --output_valid_signal=... adds and sets the name of valid signals when --generator is set to pipeline . --manual_load_enable_signal=... adds and sets the name of an input that sets the load-enable signals of each pipeline stage. For procs, --streaming_channel_data_suffix=... , --streaming_channel_valid_suffix=... , and --streaming_channel_ready_suffix=... set suffixes to be used on their respective signals in ready/valid channels. For example, --streaming_channel_valid_suffix=_vld for a channel named ABC would result in a valid port called ABC_vld . Reset Signal Configuration --reset=... sets the name of the reset signal. If not specified, no reset signal is used. --reset_active_low sets if the reset is active low or high. Active high by default. --reset_asynchronous sets if the reset is synchronous or asynchronous (synchronous by default). --reset_data_path sets if the datapath should also be reset. True by default. Codegen Mapping --use_system_verilog sets if the output should use SystemVerilog constructs such as SystemVerilog array assignments, @always_comb , @always_ff , asserts, covers, etc. True by default. --separate_lines causes every subexpression to be emitted on a separate line. False by default. Format Strings For some XLS ops, flags can override their default codegen behavior via format string. These format strings use placeholders to fill in relevant information. --gate_format=... sets the format string for gate! ops. Supported placeholders are: - {condition} : Identifier (or expression) of the gate. - {input} : Identifier (or expression) for the data input of the gate. - {output} : Identifier for the output of the gate. - {width} : The bit width of the gate operation. For example, consider a format string which instantiates a particular custom AND gate for gating: my_and gated_{output} [{width}-1:0] (.Z({output}), .A({condition}), .B({input})) And the IR gate operation is: the_result: bits[32] = gate(the_cond, the_data) This results in the following emitted Verilog: my_and gated_the_result [32-1:0] (.Z(the_result), .A(the cond), .B(the_data)); To ensure valid Verilog, the insantiated template must declare a value named {output} (e.g. the_result in the example). --assert_format=... sets the format string for assert statements. Supported placeholders are: - {message} : Message of the assert operation. - {condition} : Condition of the assert. - {label} : Label of the assert operation. It is an error not to use the label placeholder. - {clk} : Name of the clock signal. It is an error not to use the clk placeholder. - {rst} : Name of the reset signal. It is an error not to use the rst placeholder. For example, the format string: {label}: `MY_ASSERT({condition}, \"{message}\") could result in the following emitted Verilog: my_label: `MY_ASSERT(foo < 8'h42, \"Oh noes!\"); --smulp_format=... and --umulp_format=... set the format strings for smulp and umulp ops respectively. These ops perform partial (or split) multiplies. Supported placeholders are: - {input0} and {input1} : The two inputs. - {input0_width} and {input1_width} : The width of the two inputs - {output} : Name of the output. Partial multiply IP generally produces two outputs with the property that the sum of the two outputs is the product of the inputs. {output} should be the concatenation of these two outputs. - {output_width} : Width of the output. For example, the format string: multp #( .x_width({input0_width}), .y_width({input1_width}), .z_width({output_width}>>1) ) {output}_inst ( .x({input0}), .y({input1}), .z0({output}[({output_width}>>1)-1:0]), .z1({output}[({output_width}>>1)*2-1:({output_width}>>1)})]) ); could result in the following emitted Verilog: multp #( .x_width(16), .y_width(16), .z_width(32>>1) ) multp_out_inst ( .x(lhs), .y(rhs), .z0(multp_out[(32>>1)-1:0]), .z1(multp_out[(32>>1)*2-1:(32>>1)]) ); Note the arithmetic performed on output_width to make the two-output multp block fill the concatenated output expected by XLS. I/O Behavior --flop_inputs and --flop_outputs control if inputs and outputs should be flopped respectively. These flags are only used by the pipeline generator. For procs, inputs and outputs are channels with ready/valid signalling and have additional options controlling how inputs and outputs are registered. --flop_inputs_kind=... and --flop_outputs_kind=... flags control what the logic around the outputs and inputs look like respectively. The list below enumerates the possible kinds of output flopping and shows what logic is generated in each case. - flop : Adds a pipeline stage at the beginning or end of the block to hold inputs or outputs. This is essentially a single-element FIFO. - `skid`: Adds a skid buffer at the inputs or outputs of the block. The skid buffer can hold 2 entries. - `zerolatency`: Adds a zero-latency buffer at the beginning or end of the block. This is essentially a single-element FIFO with bypass. --flop_single_value_channels control if single-value channels should be flopped. --add_idle_output adds an additional output port named idle . idle is the NOR of: 1. Pipeline registers storing the valid bit for each pipeline stage. 2. All valid registers stored for the input/output buffers. 3. All valid signals for the input channels. RAMs (experimental) XLS has experimental support for using proc channels to drive an external RAM. For an example usage, see this delay implemented with a single-port RAM ( modeled here ). Note that receives on the response channel must be conditioned on performing a read, otherwise there will be deadlock. The codegen option --ram_configurations takes a comma-separated list of configurations in the format ram_name:ram_kind[:kind-specific-configuration] . For a 1RW RAM, the format is ram_name:1RW:req_channel_name:resp_channel_name[:latency] , where latency is 1 if unspecified. For a 1RW RAM, there are several requirements these channels must satisfy: The request channel must be a tuple type with 4 entries corresponding to (addr, wr_data, we, re) . All entries must have type bits , and we and re must be a single bit. The response channel must be a tuple type with a single entry corresponding to (rd_data) . rd_data must have the same width as wr_data . Instead of the normal channel ports, the codegen option will produce the following ports: {ram_name}_addr {ram_name}_wr_data {ram_name}_we {ram_name}_re {ram_name}_rd_data Note that there are no ready/valid signals as RAMs have fixed latency. There is an internal buffer to catch the response and apply backpressure on requests if needed. When using --ram_configurations , you should generally add a scheduling constraint via --io_constraints to ensure the request-send and response-receive are scheduled to match the RAM's latency. Optimization --gate_recvs emits logic to gate the data value of a receive operation in Verilog. In the XLS IR, the receive operation has the semantics that the data value is zero when the predicate is false . Moreover, for a non-blocking receive, the data value is zero when the data is invalid. When set to true, the data is gated and has the previously described semantics. However, the latter does utilize more resource/area. Setting this value to false may reduce the resource/area utilization, but may also result in mismatches between IR-level evaluation and Verilog simulation. --array_index_bounds_checking : With this option set, an out of bounds array access returns the maximal index element in the array. If this option is not set, the result relies on the semantics of out-of-bounds array access in Verilog which is not well-defined. Setting this option to true may result in more resource/area. Setting this value to false may reduce the resource/area utilization, but may also result in mismatches between IR-level evaluation and Verilog simulation.","title":"Codegen Options"},{"location":"codegen_options/#codegen-options","text":"This document outlines some useful knobs for running codegen on an XLS design. Codegen is the process of generating RTL from IR and is where operations are scheduled and mapped into RTL constructs. The output of codegen is suitable for simulation or implementation via standard tools that understand Verilog or SystemVerilog. Codegen Options Input specification Output locations Pipelining and Scheduling Options Naming Reset Signal Configuration Codegen Mapping Format Strings I/O Behavior RAMs (experimental) Optimization","title":"Codegen Options"},{"location":"codegen_options/#input-specification","text":"<input.ir> is a positional argument giving the path to the ir file. --top specifies the top function or proc to codegen.","title":"Input specification"},{"location":"codegen_options/#output-locations","text":"The following flags control where output files are put. In addition to Verilog, codegen can generate files useful for understanding or integrating the RTL. --output_verilog_path is the path to the output Verilog file. --output_schedule_path is the path to a textproto that shows into which pipeline stage the scheduler put IR ops. --output_block_ir_path is the path to the \"block IR\" representation of the design, a post-scheduling IR that is timed and includes registers, ports, etc. --output_signature_path is the path to the signature textproto. The signature describes the ports, channels, external memories, etc. --output_verilog_line_map_path is the path to the verilog line map associating lines of verilog to lines of IR.","title":"Output locations"},{"location":"codegen_options/#pipelining-and-scheduling-options","text":"The following flags control how XLS maps IR operations to RTL, and if applicable control the scheduler. --generator=... controls which generator to use. The options are pipeline and combinational . The pipeline generator runs a scheduler that partitions the IR ops into pipeline stages. --delay_model=... selects the delay model to use when scheduling. See the page here for more detail. --clock_period_ps=... sets the target clock period. See scheduling for more details on how scheduling works. Note that this option is optional, without specifying clock period XLS will estimate what the clock period should be. --pipeline_stages=... sets the number of pipeline stages to use when --generator=pipeline . --clock_margin_percent=... sets the percentage to reduce the target clock period before scheduling. See scheduling for more details. --period_relaxation_percent=... sets the percentage that the computed minimum clock period is increased. May not be specified with --clock_period_ps . --additional_input_delay_ps=... adds additional input delay to the inputs. This can be helpful to meet timing when integrating XLS designs with other RTL. --io_constraints=... adds constraints to the scheduler. The flag takes a comma-separated list of constraints of the form foo:send:bar:recv:3:5 which means that sends on channel foo must occur between 3 and 5 cycles (inclusive) before receives on channel bar . Note that for a constraint like foo:send:foo:send:3:5 , no constraint will be applied between a node and itself; i.e.: this means all different pairs of nodes sending on foo must be in cycles that differ by between 3 and 5. If the special minimum/maximum value none is used, then the minimum latency will be the lowest representable int64_t , and likewise for maximum latency. For an example of the use of this, see this example and the associated BUILD rule.","title":"Pipelining and Scheduling Options"},{"location":"codegen_options/#naming","text":"Some names can be set at codegen via the following flags: --module_name=... sets the name of the generated verilog module For functions, --input_valid_signal=... and --output_valid_signal=... adds and sets the name of valid signals when --generator is set to pipeline . --manual_load_enable_signal=... adds and sets the name of an input that sets the load-enable signals of each pipeline stage. For procs, --streaming_channel_data_suffix=... , --streaming_channel_valid_suffix=... , and --streaming_channel_ready_suffix=... set suffixes to be used on their respective signals in ready/valid channels. For example, --streaming_channel_valid_suffix=_vld for a channel named ABC would result in a valid port called ABC_vld .","title":"Naming"},{"location":"codegen_options/#reset-signal-configuration","text":"--reset=... sets the name of the reset signal. If not specified, no reset signal is used. --reset_active_low sets if the reset is active low or high. Active high by default. --reset_asynchronous sets if the reset is synchronous or asynchronous (synchronous by default). --reset_data_path sets if the datapath should also be reset. True by default.","title":"Reset Signal Configuration"},{"location":"codegen_options/#codegen-mapping","text":"--use_system_verilog sets if the output should use SystemVerilog constructs such as SystemVerilog array assignments, @always_comb , @always_ff , asserts, covers, etc. True by default. --separate_lines causes every subexpression to be emitted on a separate line. False by default.","title":"Codegen Mapping"},{"location":"codegen_options/#format-strings","text":"For some XLS ops, flags can override their default codegen behavior via format string. These format strings use placeholders to fill in relevant information. --gate_format=... sets the format string for gate! ops. Supported placeholders are: - {condition} : Identifier (or expression) of the gate. - {input} : Identifier (or expression) for the data input of the gate. - {output} : Identifier for the output of the gate. - {width} : The bit width of the gate operation. For example, consider a format string which instantiates a particular custom AND gate for gating: my_and gated_{output} [{width}-1:0] (.Z({output}), .A({condition}), .B({input})) And the IR gate operation is: the_result: bits[32] = gate(the_cond, the_data) This results in the following emitted Verilog: my_and gated_the_result [32-1:0] (.Z(the_result), .A(the cond), .B(the_data)); To ensure valid Verilog, the insantiated template must declare a value named {output} (e.g. the_result in the example). --assert_format=... sets the format string for assert statements. Supported placeholders are: - {message} : Message of the assert operation. - {condition} : Condition of the assert. - {label} : Label of the assert operation. It is an error not to use the label placeholder. - {clk} : Name of the clock signal. It is an error not to use the clk placeholder. - {rst} : Name of the reset signal. It is an error not to use the rst placeholder. For example, the format string: {label}: `MY_ASSERT({condition}, \"{message}\") could result in the following emitted Verilog: my_label: `MY_ASSERT(foo < 8'h42, \"Oh noes!\"); --smulp_format=... and --umulp_format=... set the format strings for smulp and umulp ops respectively. These ops perform partial (or split) multiplies. Supported placeholders are: - {input0} and {input1} : The two inputs. - {input0_width} and {input1_width} : The width of the two inputs - {output} : Name of the output. Partial multiply IP generally produces two outputs with the property that the sum of the two outputs is the product of the inputs. {output} should be the concatenation of these two outputs. - {output_width} : Width of the output. For example, the format string: multp #( .x_width({input0_width}), .y_width({input1_width}), .z_width({output_width}>>1) ) {output}_inst ( .x({input0}), .y({input1}), .z0({output}[({output_width}>>1)-1:0]), .z1({output}[({output_width}>>1)*2-1:({output_width}>>1)})]) ); could result in the following emitted Verilog: multp #( .x_width(16), .y_width(16), .z_width(32>>1) ) multp_out_inst ( .x(lhs), .y(rhs), .z0(multp_out[(32>>1)-1:0]), .z1(multp_out[(32>>1)*2-1:(32>>1)]) ); Note the arithmetic performed on output_width to make the two-output multp block fill the concatenated output expected by XLS.","title":"Format Strings"},{"location":"codegen_options/#io-behavior","text":"--flop_inputs and --flop_outputs control if inputs and outputs should be flopped respectively. These flags are only used by the pipeline generator. For procs, inputs and outputs are channels with ready/valid signalling and have additional options controlling how inputs and outputs are registered. --flop_inputs_kind=... and --flop_outputs_kind=... flags control what the logic around the outputs and inputs look like respectively. The list below enumerates the possible kinds of output flopping and shows what logic is generated in each case. - flop : Adds a pipeline stage at the beginning or end of the block to hold inputs or outputs. This is essentially a single-element FIFO. - `skid`: Adds a skid buffer at the inputs or outputs of the block. The skid buffer can hold 2 entries. - `zerolatency`: Adds a zero-latency buffer at the beginning or end of the block. This is essentially a single-element FIFO with bypass. --flop_single_value_channels control if single-value channels should be flopped. --add_idle_output adds an additional output port named idle . idle is the NOR of: 1. Pipeline registers storing the valid bit for each pipeline stage. 2. All valid registers stored for the input/output buffers. 3. All valid signals for the input channels.","title":"I/O Behavior"},{"location":"codegen_options/#rams-experimental","text":"XLS has experimental support for using proc channels to drive an external RAM. For an example usage, see this delay implemented with a single-port RAM ( modeled here ). Note that receives on the response channel must be conditioned on performing a read, otherwise there will be deadlock. The codegen option --ram_configurations takes a comma-separated list of configurations in the format ram_name:ram_kind[:kind-specific-configuration] . For a 1RW RAM, the format is ram_name:1RW:req_channel_name:resp_channel_name[:latency] , where latency is 1 if unspecified. For a 1RW RAM, there are several requirements these channels must satisfy: The request channel must be a tuple type with 4 entries corresponding to (addr, wr_data, we, re) . All entries must have type bits , and we and re must be a single bit. The response channel must be a tuple type with a single entry corresponding to (rd_data) . rd_data must have the same width as wr_data . Instead of the normal channel ports, the codegen option will produce the following ports: {ram_name}_addr {ram_name}_wr_data {ram_name}_we {ram_name}_re {ram_name}_rd_data Note that there are no ready/valid signals as RAMs have fixed latency. There is an internal buffer to catch the response and apply backpressure on requests if needed. When using --ram_configurations , you should generally add a scheduling constraint via --io_constraints to ensure the request-send and response-receive are scheduled to match the RAM's latency.","title":"RAMs (experimental)"},{"location":"codegen_options/#optimization","text":"--gate_recvs emits logic to gate the data value of a receive operation in Verilog. In the XLS IR, the receive operation has the semantics that the data value is zero when the predicate is false . Moreover, for a non-blocking receive, the data value is zero when the data is invalid. When set to true, the data is gated and has the previously described semantics. However, the latter does utilize more resource/area. Setting this value to false may reduce the resource/area utilization, but may also result in mismatches between IR-level evaluation and Verilog simulation. --array_index_bounds_checking : With this option set, an out of bounds array access returns the maximal index element in the array. If this option is not set, the result relies on the semantics of out-of-bounds array access in Verilog which is not well-defined. Setting this option to true may result in more resource/area. Setting this value to false may reduce the resource/area utilization, but may also result in mismatches between IR-level evaluation and Verilog simulation.","title":"Optimization"},{"location":"contributing/","text":"How to Contribute We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. Community Guidelines This project follows Google's Open Source Community Guidelines . Contributor License Agreement Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again. Code style When writing code contributions to the project, please make sure to follow the style guides: The Google C++ Style Guide and the Google Python Style Guide . There are a few small [XLS clarifications] (https://google.github.io/xls/xls_style/) for local style on this project where the style guide is ambiguous. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. Pull Request Style We ask contributors to squash all the commits in the PR into a single one, in order to have a cleaner revision history. Specifically, when you initially send a PR, please ensure it has a single commit. If you'd like to address review comments by adding commits, 1 please be sure to squash them into one again once the PR is approved (though squashing continuously is also acceptable). Generally, squashing to a single commit can be accomplished by: proj/xls$ # Here we assume origin points to google/xls. proj/xls$ git fetch origin main proj/xls$ git merge-base origin/main my-branch-name # Tells you common ancesor COMMIT_HASH. proj/xls$ git reset --soft $COMMIT_HASH proj/xls$ git commit -a -m \"My awesome squashed commit message!!!1\" proj/xls$ # Now we can more easily rebase our squashed commit on main. proj/xls$ git rebase origin/main Rebased branches can be pushed to their corresponding PRs with --force . See also this Stack Overflow question . Rendering Documentation XLS uses mkdocs to render its documentation, and serves it via GitHub pages at https://google.github.io/xls . To render documentation locally as a preview, you can set up mkdocs as follows: proj/xls$ mkvirtualenv xls-mkdocs-env proj/xls$ pip install mkdocs-material mkdocs-exclude mdx_truly_sane_lists proj/xls$ mkdocs serve This will start a local server that you can browse to and that will update the documentation on the fly as you make changes. Note that the mkvirtualenv command assumes you're using virtualenvwrapper to manage your Python environment. You'll need to adjust these instrutions if you're doing something different. That can include explicitly adding mkdocs to your path, if locally installed Python binaries aren't available by default. DSL snippets in documentation There are a few different language annotations we use in different circumstances in the Markdown docs: dslx : A full code block that should be parsed/typechecked/tested. dslx-snippet : A fragment that should be syntax highlighted, but not parsed/typechecked/tested. dslx-bad : An example of something that we expect to produce an error when parsing/typechecking/testing. GitHub issue google/xls#378 tracks a script that does the parse/typecheck/test that ensures our documentation is up to date and correct. Adding commits preserves the GitHub code review history and makes it easier to review incremental changes, but causes an additional \"round trip\" with the reviewer for the final squash after approval, so there is a small procedural tradeoff. \u21a9","title":"Contributing"},{"location":"contributing/#how-to-contribute","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"How to Contribute"},{"location":"contributing/#community-guidelines","text":"This project follows Google's Open Source Community Guidelines .","title":"Community Guidelines"},{"location":"contributing/#contributor-license-agreement","text":"Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributor License Agreement"},{"location":"contributing/#code-style","text":"When writing code contributions to the project, please make sure to follow the style guides: The Google C++ Style Guide and the Google Python Style Guide . There are a few small [XLS clarifications] (https://google.github.io/xls/xls_style/) for local style on this project where the style guide is ambiguous.","title":"Code style"},{"location":"contributing/#code-reviews","text":"All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.","title":"Code reviews"},{"location":"contributing/#pull-request-style","text":"We ask contributors to squash all the commits in the PR into a single one, in order to have a cleaner revision history. Specifically, when you initially send a PR, please ensure it has a single commit. If you'd like to address review comments by adding commits, 1 please be sure to squash them into one again once the PR is approved (though squashing continuously is also acceptable). Generally, squashing to a single commit can be accomplished by: proj/xls$ # Here we assume origin points to google/xls. proj/xls$ git fetch origin main proj/xls$ git merge-base origin/main my-branch-name # Tells you common ancesor COMMIT_HASH. proj/xls$ git reset --soft $COMMIT_HASH proj/xls$ git commit -a -m \"My awesome squashed commit message!!!1\" proj/xls$ # Now we can more easily rebase our squashed commit on main. proj/xls$ git rebase origin/main Rebased branches can be pushed to their corresponding PRs with --force . See also this Stack Overflow question .","title":"Pull Request Style"},{"location":"contributing/#rendering-documentation","text":"XLS uses mkdocs to render its documentation, and serves it via GitHub pages at https://google.github.io/xls . To render documentation locally as a preview, you can set up mkdocs as follows: proj/xls$ mkvirtualenv xls-mkdocs-env proj/xls$ pip install mkdocs-material mkdocs-exclude mdx_truly_sane_lists proj/xls$ mkdocs serve This will start a local server that you can browse to and that will update the documentation on the fly as you make changes. Note that the mkvirtualenv command assumes you're using virtualenvwrapper to manage your Python environment. You'll need to adjust these instrutions if you're doing something different. That can include explicitly adding mkdocs to your path, if locally installed Python binaries aren't available by default.","title":"Rendering Documentation"},{"location":"contributing/#dsl-snippets-in-documentation","text":"There are a few different language annotations we use in different circumstances in the Markdown docs: dslx : A full code block that should be parsed/typechecked/tested. dslx-snippet : A fragment that should be syntax highlighted, but not parsed/typechecked/tested. dslx-bad : An example of something that we expect to produce an error when parsing/typechecking/testing. GitHub issue google/xls#378 tracks a script that does the parse/typecheck/test that ensures our documentation is up to date and correct. Adding commits preserves the GitHub code review history and makes it easier to review incremental changes, but causes an additional \"round trip\" with the reviewer for the final squash after approval, so there is a small procedural tradeoff. \u21a9","title":"DSL snippets in documentation"},{"location":"data_layout/","text":"Data layout For many uses, XLS types exist within their own conceptual space or domain, so \"portability\" concerns don't exist. When interacting with the JIT, however, XLS and host-native types must interact, so the data layouts of both must be understood and possibly reconciled. Data layout XLS data layout Host data layout JIT data layout Packed views Tuple types XLS data layout The concrete XLS Bits type is the ultimate container of actual data for any XLS IR type: tuples and arrays may be contain any number of tuple, array, or bits types, but whatever the layout of the type tree, all leaf nodes are Bits. When accessing the underlying storage of a Bits via the ToBytes() member function, the results are returned in a little-endian layout, i.e, with the least-significant data elements stored in the lowest addressable location. For example, the 32-bit value 12,345,678 (0xBC614E), would be returned as: High <-- Low 0x 00 BC 61 4E Host data layout Different architectures can use different native layouts. For example, x86 (and descendants) use little-endian (i.e., 0x00 BC 61 4E ), and modern ARM can be configurable as either. (There are actually other layouts, but they're best left to the dustbin of history). JIT data layout From the above, we can see that XLS' native layout differs from that of most modern hosts. When compiling XLS code, the [LLVM] JIT understandably uses the host's native layout. What this means is that any data fed into the JIT from XLS will need to be byte-swapped before ingestion. For Value or unpacked view input, this swapping is handled automatically, in LlvmIrRuntime::PackArgs() (via LlvmIrRuntime::BlitValueToBuffer() ) - and the __un__swapping is also automatically performed in LlvmIrRuntime::UnpackBuffer() . Thus, for these uses, no special action is required of the user. Packed views However , this is not the case for use of packed views. The motivating use case for packed views is to allow users to map native types directly into JIT-usable values - for example, to use an IEEE float32 (e.g., a C float ) directly , without needing to be exploded into a bits[1] for the sign, a bits[8] for the exponent, and a bits[23] for the fractional part. When creating a packed view from a C float , no special action is needed - that float is in native host layout, which is the layout used by the JIT. If, however, data is coming from XLS (perhaps a float converted into a Value, manipulated in some way, then passed into the JIT), then the user must un-swap the bits back to native layout. This is because the JIT has no way of knowing the provenance of that data (if it's from a native type or XLS), so it's up to the provider of that data to ensure proper layout. Distilled into a simple rule of thumb: if packed view data is coming from XLS, it needs to be byte swapped before being passed into the JIT. Tuple types Another wrinkle is the usage of packed tuple views. When XLS emits a tuple type in Verilog, the first element in the tuple declaration is placed in the most significant bits, and so on, with the last-declared element placed in the least significant bits. To match this layout, PackedTupleView elements must be also declared from most significant to least significant element. This way, when running on a host, the in-memory layout of input data matches that expected by XLS tools. That means, using the usual float32 example, that the packed view declaration is: PackedTupleView<PackedBitsView<1>, PackedBitsView<8>, PackedBitsView<23>> (The non-packed-View tuple declaration is much the same, but matters less, as it doesn't directly correspond to in-memory data layout.) Be aware of this layout when accessing elements in a PackedTupleView. In the float32 above, accessing element 0 yields the sign bit (the most significant bit in memory), and accessing element 2 yields the fractional part (the least significant 23 bits in memory), as one would expect given the tuple type declaration order. While this may initially seem confusing, it suffices to remember that PackedTupleView element declaration order is the \"reverse\" of the in-memory order; refer to value_view_test.cc and function_jit_test.cc for test examples, or the [generated] fp32_add_2_jit_wrapper.h/cc and fp32_add_2_test.cc for practical usage.","title":"Data Layout"},{"location":"data_layout/#data-layout","text":"For many uses, XLS types exist within their own conceptual space or domain, so \"portability\" concerns don't exist. When interacting with the JIT, however, XLS and host-native types must interact, so the data layouts of both must be understood and possibly reconciled. Data layout XLS data layout Host data layout JIT data layout Packed views Tuple types","title":"Data layout"},{"location":"data_layout/#xls-data-layout","text":"The concrete XLS Bits type is the ultimate container of actual data for any XLS IR type: tuples and arrays may be contain any number of tuple, array, or bits types, but whatever the layout of the type tree, all leaf nodes are Bits. When accessing the underlying storage of a Bits via the ToBytes() member function, the results are returned in a little-endian layout, i.e, with the least-significant data elements stored in the lowest addressable location. For example, the 32-bit value 12,345,678 (0xBC614E), would be returned as: High <-- Low 0x 00 BC 61 4E","title":"XLS data layout"},{"location":"data_layout/#host-data-layout","text":"Different architectures can use different native layouts. For example, x86 (and descendants) use little-endian (i.e., 0x00 BC 61 4E ), and modern ARM can be configurable as either. (There are actually other layouts, but they're best left to the dustbin of history).","title":"Host data layout"},{"location":"data_layout/#jit-data-layout","text":"From the above, we can see that XLS' native layout differs from that of most modern hosts. When compiling XLS code, the [LLVM] JIT understandably uses the host's native layout. What this means is that any data fed into the JIT from XLS will need to be byte-swapped before ingestion. For Value or unpacked view input, this swapping is handled automatically, in LlvmIrRuntime::PackArgs() (via LlvmIrRuntime::BlitValueToBuffer() ) - and the __un__swapping is also automatically performed in LlvmIrRuntime::UnpackBuffer() . Thus, for these uses, no special action is required of the user.","title":"JIT data layout"},{"location":"data_layout/#packed-views","text":"However , this is not the case for use of packed views. The motivating use case for packed views is to allow users to map native types directly into JIT-usable values - for example, to use an IEEE float32 (e.g., a C float ) directly , without needing to be exploded into a bits[1] for the sign, a bits[8] for the exponent, and a bits[23] for the fractional part. When creating a packed view from a C float , no special action is needed - that float is in native host layout, which is the layout used by the JIT. If, however, data is coming from XLS (perhaps a float converted into a Value, manipulated in some way, then passed into the JIT), then the user must un-swap the bits back to native layout. This is because the JIT has no way of knowing the provenance of that data (if it's from a native type or XLS), so it's up to the provider of that data to ensure proper layout. Distilled into a simple rule of thumb: if packed view data is coming from XLS, it needs to be byte swapped before being passed into the JIT.","title":"Packed views"},{"location":"data_layout/#tuple-types","text":"Another wrinkle is the usage of packed tuple views. When XLS emits a tuple type in Verilog, the first element in the tuple declaration is placed in the most significant bits, and so on, with the last-declared element placed in the least significant bits. To match this layout, PackedTupleView elements must be also declared from most significant to least significant element. This way, when running on a host, the in-memory layout of input data matches that expected by XLS tools. That means, using the usual float32 example, that the packed view declaration is: PackedTupleView<PackedBitsView<1>, PackedBitsView<8>, PackedBitsView<23>> (The non-packed-View tuple declaration is much the same, but matters less, as it doesn't directly correspond to in-memory data layout.) Be aware of this layout when accessing elements in a PackedTupleView. In the float32 above, accessing element 0 yields the sign bit (the most significant bit in memory), and accessing element 2 yields the fractional part (the least significant 23 bits in memory), as one would expect given the tuple type declaration order. While this may initially seem confusing, it suffices to remember that PackedTupleView element declaration order is the \"reverse\" of the in-memory order; refer to value_view_test.cc and function_jit_test.cc for test examples, or the [generated] fp32_add_2_jit_wrapper.h/cc and fp32_add_2_test.cc for practical usage.","title":"Tuple types"},{"location":"delay_estimation/","text":"Delay Estimation Methodology Context This doc describes the delay estimation methodology used in XLS and related background. Estimating delays through networks of CMOS gates is a rich topic, and Static Timing Analysis (STA) is used in chip backend flows to ensure that, even for parts operating at the tail end of the distribution, chips continue to function as specified logically in the netlist. In stark contrast to something with very concrete constraints, like a \"post-global routing, high-accuracy parasitics static timing analysis\", the HLS tool needs to estimate at a high level, reasonably near to the user's behavioral specification, what delay HLS operations will have when they are \"stacked on top\" of each other in a data dependent fashion in the design. This information lets the HLS tool schedule operations into cycles without violating timing constraints; e.g. if the user specifies 1GHz (= 1000ps) as their target clock frequency, the HLS tool may choose to pack as much data dependent work into the 1000ps budget (minus clock uncertainty) as it can on any given cycle. Although what we're estimating is a close relative of Static Timing Analysis, the fact it's being analyzed at a very high level presents a different set of tradeoffs, where coarse granularity estimation and conservative bounds are more relevant. Fine-grained / precise analyses are used later in the chip design process, in the backend flows, but the HLS tool acts more like an RTL designer, creating an RTL output that early timing analysis deems acceptable. Notably, we don't want to be too conservative, as being conservative on timing can lead to designs that take more area and consume more power, as the flops introduced by additional pipeline stages are significant. We want to be as accurate as possible while providing a good experience of being able to close timing quickly (say, in a single pass, or with a small number of iterations in case of a pathological design). Background What XLS is compiling XLS currently supports feed-forward pipelines -- once the user program is unrolled into a big \"sea of nodes\" we must schedule each of those operations (represented by nodes) to occur in a cycle. It seems clear that an operation like add(bits[32], bits[32]) -> bits[32] takes some amount of time to produce a result -- we need to be able to determine what that amount of time is for packing that operation into a given cycle. 1 Note that XLS operations are parametric in their bitwidth, so add(bits[17], bits[17]) -> bits[17] is just as possible as a value like 32 . This ain't C code. 1 : Note that we currently pack operations into cycles atomically -- that is, we don't break an add that would straddle a cycle boundary into add.first_half and add.second_half automatically to pull things as early as possible in the pipeline, but this is future work of interest. Ideally operations would be described structurally in a way that could automatically be cut up according to available delay budget. This would also permit operations in the IR that take more than a single cycle to produce a value (currently they would have to be \"legalized\" into operations that fit within a cycle, but that is not yet done, the user will simply receive a scheduling error). Some operations, such as bit_slice or concat are just wiring \"feng shui\"; however, they still have some relevance for delay calculations! Say we concatenate a value with zeros for zero extension. Even if we could schedule that in \"cycle 0\", if the consumer can only be placed in \"cycle 1\", we would want to \"sink\" the concat down into \"cycle 1\" as well to avoid unnecessary registers being materialized sending the zero values from \"cycle 0\". The delay problem Separately from XLS considerations, there are fundamental considerations in calculating the delay through clouds of functional logic in (generated) RTL. Between each launching and capturing flop is a functional network of logic gates, implemented with standard cells in our ASIC process flows. Chip designs target a particular clock frequency as their operating point, and the functional network has to produce its output value with a delay that meets the timing constraint of the clock frequency. The RTL designer typically has to iterate their design until: timing path delay <= target clock period - clock uncertainty For all timing paths in their design, where clock uncertainty includes setup/hold time constraints, and slop that's built in as margin for later sources of timing variability (like instantiating a clock tree, which can skew the clock signal observed by different flops). In a reasonable model, gate delay is affected by a small handful of properties, as reflected in the \"(Method of) Logical Effort\" book: The transistor network used to implement a logic function (AKA logical effort): on an input pin change, the gate of each transistor must be driven to a point it recognizes whether a 0 or 1 voltage is being presented. More gates to drive, or larger gates, means more work for the driver. The load being driven by the logic function (AKA electrical effort): fanning out to more gates generally means more work to drive them all to their threshold voltages. Being loaded down by bigger gates means more work to drive it to its threshold voltage. Parasitic delays: RC elements in the system that leech useful work, typically in a smaller way compared to the efforts listed above. The logical effort book describes a way to analyze the delays through a network of gates to find the minimal delay, and size transistors in a way that can achieve that minimal delay (namely by geometrically smoothing the ability for gate to drive capacitance). Confounding factors include: Medium/large wires: sizing transistors to smooth capacitance becomes difficult as fixed-capacitance elements (wires) are introduced. It seems that small wires have low enough capacitance they can generally be treated as parasitic. Divergence/reconvergence in the functional logic network (as a DAG). Different numbers of logic levels and different drive currents may be presented from different branches of a fork/join the logic graph, which forces delay analysis into a system of equations to attempt to minimize the overall delay, as observed by the critical path, with transistor sizing and gate choices. (Some simplifications are possible, like buffering non-critical paths until they have the same number of logic levels so they also have plenty of current to supply at join points.) Somewhat orthogonal to the analytical modeling problem, there are also several industry standards for supplying process information to Static Timing Analysis engines for determining delay through a netlist. This information is often given in interpolated tables for each standard cell, for example in the NLDM model describing how delay changes as a function of input transition time and load (load capacitance). These models and supplied pieces of data are important to keep in mind for contrast, as we now ignore it all and do something very simple. Simple Delay Estimation Currently, XLS delay estimation follows a conceptually simple procedure: For every operation in XLS (e.g. binary addition): * For some relevant-seeming set of bitwidths; e.g. {2, 4, 8, 16, ..., 2048} * Find the maximum frequency at which that operation closes timing at that bitwidth, in 100MHz units as determined by the synthesis tool. 2 Call the clock period for this frequency t_best . (Note that we currently just use a single process corner / voltage for this sweep.) * Subtract the clock uncertainty from t_best . * Record that value in a table (with the keys of the table being operation / bitwidth). 2 : The timing report can provide the delay through a path at any clock frequency, but a wrinkle is that synthesis tools potentially only start using their more aggressive techniques as you bump up against the failure-to-close-timing point -- there it'll be more likely to change the structure of the design to make it more delay friendly. The sweep helps to try to cajole it in that way. Inspecting the data acquired in this way we observe all of the plots consist of one or more the following delay components: Constant as a function of bitwidth for a given op (e.g. binary-or just requires a single gate for each bit regardless of the width of the inputs). Logarithmic as a function of bitwidth (e.g. adders may end up using tree-like structures to minimize delay, single-selector muxes end up using a tree to fan out the selector to the muxes, etc.). Linear as a function of bitwidth (e.g., ripple-carry adders and some components of multipliers). So given this observation we fit a curve of the form: a * bitwidth + b * log_2(bitwidth) + c to the sweep data for each operation, giving us (a, b, c) values to use in our XLS delay estimator. The utility delay_model_visualizer under the tools directory renders a graph of the delay model estimate against the measured data points. This graph for add shows a good correspondence to the measured delay. Sweeping multiple dimensions Operations with attributes in addition to bitwidth that affect delay are swept across multiple dimensions. An example is Op::kOneHotSelect which has dimensions of bitwidth and number of cases. For the Op::kOneHotSelect example the formula is: a * bitwidth + b * log_2(bitwidth) + c * (# of cases) + d * log_2(# of cases) + c Below is plot of delay for Op::kOneHotSelect showing the two dimensions of bitwidth and operand count affecting delay: Sources of pessimism/optimism This simple method has both sources of optimism and pessimism, though we hope to employ a method that will be generally conservative, so that users can easily close timing and get a close-to-best-achievable result (say, within tens of percent) with a single HLS iteration. Sources of pessimism (estimate is conservative): The operation sweeps mentioned are not bisecting to the picosecond, so there is inherent slop in the measurement on account of sweep granularity. We expect, in cycles where multiple dependent operations are present, there would be \"K-map style\" logic reductions with adjacent operations. For example, because we don't do cell library mapping in XLS delay estimation, something a user wrote that mapped to an AOI21 cell would be the sum of (and+or+invert) delays. [Unsure] May there be additional logic branch splitting options and earlier-produced results available to the synthesis tool when there are more operations in the graph (vs a lone critical path measured for a single operation)? Sources of optimism (estimate is overeager): For purposes of the sweep the outputs of an operation are only loaded by a single capture flop flop -- when operations have fanout the delay will increase. Note that we do account for fanout within individual operations as part of this sweep; e.g. a 128-bit selector fanout (e.g. 128 ways to 128 muxes) for a select is accounted for the in delay timing of the select operation. It is the output logic layer that is only loaded by a single flop in our characterization. Notably because most of these operations turn into trees of logic, there are \\( \\(log_2(bitcount)\\) \\) layers of logic in which we can potentially smoothly increase drive strength out to the output logic layer, and paths can presumably be replicated by synthesis tools to reduce pointwise fanout when multiple high-level operations are data-dependent within a cycle. (Is it possible for a user to break up their 32-bit select into bitwise pieces in their XLS code to mess with our modeling? Sure, but probably not too expected, so we're currently sort of relying on the notion people are using high level operations instead of compodecomposing them into bitwise pieces in our graph.) A potential way to reconcile this output fanout in the future is to do a delay sweep with a high-capacitive fanout (e.g. four flops of load) and then ensure the IR has a maximum fanout of four for our delay estimation. Wiring delay / load / congestion / length are not estimated. This will need additional analysis / refinement as we run XLS through synthesis tools with advanced timing analysis, as it is certainly not viable for arbitrary designs (tight pipelines may be ok for now, though). Iterative refinement The caveats mentioned above seem somewhat daunting, but this first cut approach appears to work comfortably at target frequenties, in practice, for the real-world blocks being designed as XLS \"first samples\". Notably, human RTL designers fail to close timing on a first cut as well -- HLS estimations like the above assist in getting a numeric understanding (in lieu of an intuitive guess) of something that may close timing on a first cut. As this early model fails, we will continue to refine it; however, there is also a secondary procedure that can assist as the model improves. Let's call the delay estimation described above applied to a program a prediction of its delays. Let's call the first prediction we make p0 : p0 will either meet timing or fail to meet timing. When we meet timing with p0 , there may be additional wins left on the table. If we're willing to put synthesis tool runs \"in the loop\" (say running a \"tuner\" overnight), we can refine XLS's estimates according to the realities of the current program, and, for example, try to squeeze as much as possible into as few cycles as possible if near-optimality of latency/area/power were a large consideration. This loop would generate p1 , p2 , ... as it refined its model according to empirical data observed from the synthesis tool's more refined analysis. When we fail to close timing with p0 , we can feed back the negative slack delays for comparison with our estimates and relax estimates accordingly. Additionally, an \"aggression\" knob could be implemented that backs off delay estimations geometrically (say via a \"fudge factor\" coefficient) in order to ensure HLS developer time is not wasted unnecessarily to ensure. Once a combination of these mechanisms has obtained a design that closes timing, the \"meeting timing, now refine\" procedure can be employed as described above. On Hints To whatever extent possible, XLS should be the tool that reasons about how to target the backend (vs having a tool that sits on top of it and messes with XLS' input in an attempt to achieve a result). User-facing hint systems are typically very fragile, owing to the fact they don't have easily obeyed semantics. XLS, by contrast, knows about its own internals, so can do things with awareness of what's happened upstream and what remains to happen downstream. By contrast, we should continue to add ways for users to provide more semantic information / intent as part of their program (e.g. via more high-level patterns that make high level structure more clear), and make XLS smarter about how to lower those constructs into hardware (and why it should be lowering them that way) in the face of some prioritized objectives (power/area/latency). That being said, because we're probably trying to produce hardware at a given point in time against a given technology, it likely makes sense to permit human users to specify things directly (at a point in time), even if those specifications might be ignored / handled very differently in the future or against different technology nodes. This would be the moral equivalent of what existing EDA tools do as a \"one-off TCL file\" used in a particular design, vs something carried from design to design. Recall, though, that the intent of XLS is to make things easier to carry from design to design and require fewer one-off modifications! Tools XLS provides tools for analyzing its delay estimation model. (Note that the given IR should be in a form suitable for code generation; e.g. it has run through the opt_main binary). $ tools/benchmark_main crc32.opt.ir --clock_period_ps=500 <snip> Return value delay: 1362ps Critical path entry count: 42 Critical path: 1362ps (+ 5ps): not.29: bits[32] = not(xor.205: bits[32], pos=[(0,29,50)]) 1357ps (+ 20ps): xor.205: bits[32] = xor(concat.195: bits[32], and.196: bits[32], pos=[(0,24,19)]) 1337ps (+ 15ps): and.196: bits[32] = and(neg.194: bits[32], literal.304: bits[32], pos=[(0,24,33)]) 1322ps (+134ps): neg.194: bits[32] = neg(concat.191: bits[32], pos=[(0,23,15)]) <snip> 154ps (+ 15ps): and.133: bits[32] = and(neg.131: bits[32], literal.19: bits[32], pos=[(0,24,33)]) 139ps (+134ps): neg.131: bits[32] = neg(concat.219: bits[32], pos=[(0,23,15)]) 5ps (+ 0ps): concat.219: bits[32] = concat(literal.297: bits[31], bit_slice.214: bits[1], pos=[(0,23,21)]) 5ps (+ 0ps): bit_slice.214: bits[1] = bit_slice(not.208: bits[8], start=0, width=1, pos=[(0,23,21)]) 5ps (+ 5ps): not.208: bits[8] = not(message: bits[8], pos=[(0,20,16)]) <snip> In addition to the critical path, the cycle-by-cycle breakdown of which operations have been scheduled is provided in stdout.","title":"Delay Estimation"},{"location":"delay_estimation/#delay-estimation-methodology","text":"","title":"Delay Estimation Methodology"},{"location":"delay_estimation/#context","text":"This doc describes the delay estimation methodology used in XLS and related background. Estimating delays through networks of CMOS gates is a rich topic, and Static Timing Analysis (STA) is used in chip backend flows to ensure that, even for parts operating at the tail end of the distribution, chips continue to function as specified logically in the netlist. In stark contrast to something with very concrete constraints, like a \"post-global routing, high-accuracy parasitics static timing analysis\", the HLS tool needs to estimate at a high level, reasonably near to the user's behavioral specification, what delay HLS operations will have when they are \"stacked on top\" of each other in a data dependent fashion in the design. This information lets the HLS tool schedule operations into cycles without violating timing constraints; e.g. if the user specifies 1GHz (= 1000ps) as their target clock frequency, the HLS tool may choose to pack as much data dependent work into the 1000ps budget (minus clock uncertainty) as it can on any given cycle. Although what we're estimating is a close relative of Static Timing Analysis, the fact it's being analyzed at a very high level presents a different set of tradeoffs, where coarse granularity estimation and conservative bounds are more relevant. Fine-grained / precise analyses are used later in the chip design process, in the backend flows, but the HLS tool acts more like an RTL designer, creating an RTL output that early timing analysis deems acceptable. Notably, we don't want to be too conservative, as being conservative on timing can lead to designs that take more area and consume more power, as the flops introduced by additional pipeline stages are significant. We want to be as accurate as possible while providing a good experience of being able to close timing quickly (say, in a single pass, or with a small number of iterations in case of a pathological design).","title":"Context"},{"location":"delay_estimation/#background","text":"","title":"Background"},{"location":"delay_estimation/#what-xls-is-compiling","text":"XLS currently supports feed-forward pipelines -- once the user program is unrolled into a big \"sea of nodes\" we must schedule each of those operations (represented by nodes) to occur in a cycle. It seems clear that an operation like add(bits[32], bits[32]) -> bits[32] takes some amount of time to produce a result -- we need to be able to determine what that amount of time is for packing that operation into a given cycle. 1 Note that XLS operations are parametric in their bitwidth, so add(bits[17], bits[17]) -> bits[17] is just as possible as a value like 32 . This ain't C code. 1 : Note that we currently pack operations into cycles atomically -- that is, we don't break an add that would straddle a cycle boundary into add.first_half and add.second_half automatically to pull things as early as possible in the pipeline, but this is future work of interest. Ideally operations would be described structurally in a way that could automatically be cut up according to available delay budget. This would also permit operations in the IR that take more than a single cycle to produce a value (currently they would have to be \"legalized\" into operations that fit within a cycle, but that is not yet done, the user will simply receive a scheduling error). Some operations, such as bit_slice or concat are just wiring \"feng shui\"; however, they still have some relevance for delay calculations! Say we concatenate a value with zeros for zero extension. Even if we could schedule that in \"cycle 0\", if the consumer can only be placed in \"cycle 1\", we would want to \"sink\" the concat down into \"cycle 1\" as well to avoid unnecessary registers being materialized sending the zero values from \"cycle 0\".","title":"What XLS is compiling"},{"location":"delay_estimation/#the-delay-problem","text":"Separately from XLS considerations, there are fundamental considerations in calculating the delay through clouds of functional logic in (generated) RTL. Between each launching and capturing flop is a functional network of logic gates, implemented with standard cells in our ASIC process flows. Chip designs target a particular clock frequency as their operating point, and the functional network has to produce its output value with a delay that meets the timing constraint of the clock frequency. The RTL designer typically has to iterate their design until: timing path delay <= target clock period - clock uncertainty For all timing paths in their design, where clock uncertainty includes setup/hold time constraints, and slop that's built in as margin for later sources of timing variability (like instantiating a clock tree, which can skew the clock signal observed by different flops). In a reasonable model, gate delay is affected by a small handful of properties, as reflected in the \"(Method of) Logical Effort\" book: The transistor network used to implement a logic function (AKA logical effort): on an input pin change, the gate of each transistor must be driven to a point it recognizes whether a 0 or 1 voltage is being presented. More gates to drive, or larger gates, means more work for the driver. The load being driven by the logic function (AKA electrical effort): fanning out to more gates generally means more work to drive them all to their threshold voltages. Being loaded down by bigger gates means more work to drive it to its threshold voltage. Parasitic delays: RC elements in the system that leech useful work, typically in a smaller way compared to the efforts listed above. The logical effort book describes a way to analyze the delays through a network of gates to find the minimal delay, and size transistors in a way that can achieve that minimal delay (namely by geometrically smoothing the ability for gate to drive capacitance). Confounding factors include: Medium/large wires: sizing transistors to smooth capacitance becomes difficult as fixed-capacitance elements (wires) are introduced. It seems that small wires have low enough capacitance they can generally be treated as parasitic. Divergence/reconvergence in the functional logic network (as a DAG). Different numbers of logic levels and different drive currents may be presented from different branches of a fork/join the logic graph, which forces delay analysis into a system of equations to attempt to minimize the overall delay, as observed by the critical path, with transistor sizing and gate choices. (Some simplifications are possible, like buffering non-critical paths until they have the same number of logic levels so they also have plenty of current to supply at join points.) Somewhat orthogonal to the analytical modeling problem, there are also several industry standards for supplying process information to Static Timing Analysis engines for determining delay through a netlist. This information is often given in interpolated tables for each standard cell, for example in the NLDM model describing how delay changes as a function of input transition time and load (load capacitance). These models and supplied pieces of data are important to keep in mind for contrast, as we now ignore it all and do something very simple.","title":"The delay problem"},{"location":"delay_estimation/#simple-delay-estimation","text":"Currently, XLS delay estimation follows a conceptually simple procedure: For every operation in XLS (e.g. binary addition): * For some relevant-seeming set of bitwidths; e.g. {2, 4, 8, 16, ..., 2048} * Find the maximum frequency at which that operation closes timing at that bitwidth, in 100MHz units as determined by the synthesis tool. 2 Call the clock period for this frequency t_best . (Note that we currently just use a single process corner / voltage for this sweep.) * Subtract the clock uncertainty from t_best . * Record that value in a table (with the keys of the table being operation / bitwidth). 2 : The timing report can provide the delay through a path at any clock frequency, but a wrinkle is that synthesis tools potentially only start using their more aggressive techniques as you bump up against the failure-to-close-timing point -- there it'll be more likely to change the structure of the design to make it more delay friendly. The sweep helps to try to cajole it in that way. Inspecting the data acquired in this way we observe all of the plots consist of one or more the following delay components: Constant as a function of bitwidth for a given op (e.g. binary-or just requires a single gate for each bit regardless of the width of the inputs). Logarithmic as a function of bitwidth (e.g. adders may end up using tree-like structures to minimize delay, single-selector muxes end up using a tree to fan out the selector to the muxes, etc.). Linear as a function of bitwidth (e.g., ripple-carry adders and some components of multipliers). So given this observation we fit a curve of the form: a * bitwidth + b * log_2(bitwidth) + c to the sweep data for each operation, giving us (a, b, c) values to use in our XLS delay estimator. The utility delay_model_visualizer under the tools directory renders a graph of the delay model estimate against the measured data points. This graph for add shows a good correspondence to the measured delay.","title":"Simple Delay Estimation"},{"location":"delay_estimation/#sweeping-multiple-dimensions","text":"Operations with attributes in addition to bitwidth that affect delay are swept across multiple dimensions. An example is Op::kOneHotSelect which has dimensions of bitwidth and number of cases. For the Op::kOneHotSelect example the formula is: a * bitwidth + b * log_2(bitwidth) + c * (# of cases) + d * log_2(# of cases) + c Below is plot of delay for Op::kOneHotSelect showing the two dimensions of bitwidth and operand count affecting delay:","title":"Sweeping multiple dimensions"},{"location":"delay_estimation/#sources-of-pessimismoptimism","text":"This simple method has both sources of optimism and pessimism, though we hope to employ a method that will be generally conservative, so that users can easily close timing and get a close-to-best-achievable result (say, within tens of percent) with a single HLS iteration. Sources of pessimism (estimate is conservative): The operation sweeps mentioned are not bisecting to the picosecond, so there is inherent slop in the measurement on account of sweep granularity. We expect, in cycles where multiple dependent operations are present, there would be \"K-map style\" logic reductions with adjacent operations. For example, because we don't do cell library mapping in XLS delay estimation, something a user wrote that mapped to an AOI21 cell would be the sum of (and+or+invert) delays. [Unsure] May there be additional logic branch splitting options and earlier-produced results available to the synthesis tool when there are more operations in the graph (vs a lone critical path measured for a single operation)? Sources of optimism (estimate is overeager): For purposes of the sweep the outputs of an operation are only loaded by a single capture flop flop -- when operations have fanout the delay will increase. Note that we do account for fanout within individual operations as part of this sweep; e.g. a 128-bit selector fanout (e.g. 128 ways to 128 muxes) for a select is accounted for the in delay timing of the select operation. It is the output logic layer that is only loaded by a single flop in our characterization. Notably because most of these operations turn into trees of logic, there are \\( \\(log_2(bitcount)\\) \\) layers of logic in which we can potentially smoothly increase drive strength out to the output logic layer, and paths can presumably be replicated by synthesis tools to reduce pointwise fanout when multiple high-level operations are data-dependent within a cycle. (Is it possible for a user to break up their 32-bit select into bitwise pieces in their XLS code to mess with our modeling? Sure, but probably not too expected, so we're currently sort of relying on the notion people are using high level operations instead of compodecomposing them into bitwise pieces in our graph.) A potential way to reconcile this output fanout in the future is to do a delay sweep with a high-capacitive fanout (e.g. four flops of load) and then ensure the IR has a maximum fanout of four for our delay estimation. Wiring delay / load / congestion / length are not estimated. This will need additional analysis / refinement as we run XLS through synthesis tools with advanced timing analysis, as it is certainly not viable for arbitrary designs (tight pipelines may be ok for now, though).","title":"Sources of pessimism/optimism"},{"location":"delay_estimation/#iterative-refinement","text":"The caveats mentioned above seem somewhat daunting, but this first cut approach appears to work comfortably at target frequenties, in practice, for the real-world blocks being designed as XLS \"first samples\". Notably, human RTL designers fail to close timing on a first cut as well -- HLS estimations like the above assist in getting a numeric understanding (in lieu of an intuitive guess) of something that may close timing on a first cut. As this early model fails, we will continue to refine it; however, there is also a secondary procedure that can assist as the model improves. Let's call the delay estimation described above applied to a program a prediction of its delays. Let's call the first prediction we make p0 : p0 will either meet timing or fail to meet timing. When we meet timing with p0 , there may be additional wins left on the table. If we're willing to put synthesis tool runs \"in the loop\" (say running a \"tuner\" overnight), we can refine XLS's estimates according to the realities of the current program, and, for example, try to squeeze as much as possible into as few cycles as possible if near-optimality of latency/area/power were a large consideration. This loop would generate p1 , p2 , ... as it refined its model according to empirical data observed from the synthesis tool's more refined analysis. When we fail to close timing with p0 , we can feed back the negative slack delays for comparison with our estimates and relax estimates accordingly. Additionally, an \"aggression\" knob could be implemented that backs off delay estimations geometrically (say via a \"fudge factor\" coefficient) in order to ensure HLS developer time is not wasted unnecessarily to ensure. Once a combination of these mechanisms has obtained a design that closes timing, the \"meeting timing, now refine\" procedure can be employed as described above.","title":"Iterative refinement"},{"location":"delay_estimation/#on-hints","text":"To whatever extent possible, XLS should be the tool that reasons about how to target the backend (vs having a tool that sits on top of it and messes with XLS' input in an attempt to achieve a result). User-facing hint systems are typically very fragile, owing to the fact they don't have easily obeyed semantics. XLS, by contrast, knows about its own internals, so can do things with awareness of what's happened upstream and what remains to happen downstream. By contrast, we should continue to add ways for users to provide more semantic information / intent as part of their program (e.g. via more high-level patterns that make high level structure more clear), and make XLS smarter about how to lower those constructs into hardware (and why it should be lowering them that way) in the face of some prioritized objectives (power/area/latency). That being said, because we're probably trying to produce hardware at a given point in time against a given technology, it likely makes sense to permit human users to specify things directly (at a point in time), even if those specifications might be ignored / handled very differently in the future or against different technology nodes. This would be the moral equivalent of what existing EDA tools do as a \"one-off TCL file\" used in a particular design, vs something carried from design to design. Recall, though, that the intent of XLS is to make things easier to carry from design to design and require fewer one-off modifications!","title":"On Hints"},{"location":"delay_estimation/#tools","text":"XLS provides tools for analyzing its delay estimation model. (Note that the given IR should be in a form suitable for code generation; e.g. it has run through the opt_main binary). $ tools/benchmark_main crc32.opt.ir --clock_period_ps=500 <snip> Return value delay: 1362ps Critical path entry count: 42 Critical path: 1362ps (+ 5ps): not.29: bits[32] = not(xor.205: bits[32], pos=[(0,29,50)]) 1357ps (+ 20ps): xor.205: bits[32] = xor(concat.195: bits[32], and.196: bits[32], pos=[(0,24,19)]) 1337ps (+ 15ps): and.196: bits[32] = and(neg.194: bits[32], literal.304: bits[32], pos=[(0,24,33)]) 1322ps (+134ps): neg.194: bits[32] = neg(concat.191: bits[32], pos=[(0,23,15)]) <snip> 154ps (+ 15ps): and.133: bits[32] = and(neg.131: bits[32], literal.19: bits[32], pos=[(0,24,33)]) 139ps (+134ps): neg.131: bits[32] = neg(concat.219: bits[32], pos=[(0,23,15)]) 5ps (+ 0ps): concat.219: bits[32] = concat(literal.297: bits[31], bit_slice.214: bits[1], pos=[(0,23,21)]) 5ps (+ 0ps): bit_slice.214: bits[1] = bit_slice(not.208: bits[8], start=0, width=1, pos=[(0,23,21)]) 5ps (+ 5ps): not.208: bits[8] = not(message: bits[8], pos=[(0,20,16)]) <snip> In addition to the critical path, the cycle-by-cycle breakdown of which operations have been scheduled is provided in stdout.","title":"Tools"},{"location":"dslx_bytecode_interpreter/","text":"Bytecode interpreter DSLX provides a bytecode interpreter for expression evaluation. This style of interpreter can be started, stopped, and resumed more easily than an AST-walking native interpreter, as its full state can be captured as {PC, stack} instead of some traversal state in native execution, which makes it very suitable for modeling independent processes, such as Proc s. NOTE: The bytecode interpreter system is under active construction and does not yet support the full set of DSLX functionality. Bytecode interpreter Structure ISA Bytecode generation Implementation details map builtin Structure The interpreter is implemented as a stack virtual machine : it consists of a program counter (PC), a stack of frames, and \"slot\"-based locals within a given stack frame ( conceptually part of the stack frame, but tracked separately in our implementation). Both the stack and local storage hold InterpValues , which can hold all DSLX data types: bits, tuples, and arrays (and others), thus there is no fundamental need for lower-level (i.e., byte) type representation. For the purposes of [de]serialization, this may change in the future. Local data is addressed by integer-typed \"slots\", being backed by a simple std::vector : in other words, slot indices are dense. All slots must be pre-allocated to contain all references to locals in the current function stack frame. On each \"tick\", the interpreter reads the current instruction, as given by the PC (conceptually, the only register in the virtual machine), executes the described operation (usually consuming values from the stack), and places the result on the stack. ISA Each instruction consists of an opcode plus, optionally, some piece of data, either int64 - or InterpValue -typed, depending on the specific opcode. The below opcodes are supported by the interpreter: ADD : Adds the two values at the top of the stack. CALL : Invokes the function given as the optional data argument, consuming a number of arguments from the stack as described by the function signature. The N'th parameter will be present as the N'th value down the stack (such that the last parameter will be the value initially on top of the stack. CREATE_TUPLE : Groups together N items on the stack (given by the optional data argument into a single InterpValue . EXPAND_TUPLE : Expands the N-tuple at stack top by one level, placing leading elements at stack top. In other words, expanding the tuple (a, (b, c)) will result in a stack of (b, c), a , where a is on top of the stack. EQ : Compares the two values on top of the stack for equality. Emits a single-bit value. LOAD : Loads the value from locals slot n , where n is given by the optional data argument. LITERAL : Places a literal value (given in the optional data argument) on top of the stack. STORE : Stores the value at stack top into slot n in locals storage. Bytecode generation The bytecode emitter is responsible for converting a set of DSLX ASTs (one per function)) into a set of linear bytecode representations. It does this via a postorder traversal of the AST, converting XLS ops into bytecode instructions along the way, e.g., converting a DSLX Binop for adding two NameRef s into two LOAD instructions (one for each NameRef ) and one ADD instruction. To do this, the emitter needs access to the full set of resolved type and import information: in other words, it requires a fully-populated ImportData and the top-level TypeInfo for the module containing the function to convert. This places bytecode emission in sequence after typechecking and deduction. Implementation details map builtin The map() function built-in to DSLX accepts an array-typed value x and a mapping function f with the signature T -> U ; that is, it accepts a single value of type T and returns a single value with the type U . In operation, map() applies the mapping function f to every element in x and returns a new array containing the results (with element i in the output corresponding to element i in the input). Conceptually, map() destructures to a for loop over the elements in x , and that's essentially what the interpreter does with these opcodes. To avoid modifying the currently executing bytecode, the interpreter instead creates a new BytecodeFunction consisting of just that destructured for loop over the inputs, followed by a CreateArray op to collect the output(s). Finally, the interpreter begins execution of the new function by creating a new Frame on top of the execution stack.","title":"Interpreter"},{"location":"dslx_bytecode_interpreter/#bytecode-interpreter","text":"DSLX provides a bytecode interpreter for expression evaluation. This style of interpreter can be started, stopped, and resumed more easily than an AST-walking native interpreter, as its full state can be captured as {PC, stack} instead of some traversal state in native execution, which makes it very suitable for modeling independent processes, such as Proc s. NOTE: The bytecode interpreter system is under active construction and does not yet support the full set of DSLX functionality. Bytecode interpreter Structure ISA Bytecode generation Implementation details map builtin","title":"Bytecode interpreter"},{"location":"dslx_bytecode_interpreter/#structure","text":"The interpreter is implemented as a stack virtual machine : it consists of a program counter (PC), a stack of frames, and \"slot\"-based locals within a given stack frame ( conceptually part of the stack frame, but tracked separately in our implementation). Both the stack and local storage hold InterpValues , which can hold all DSLX data types: bits, tuples, and arrays (and others), thus there is no fundamental need for lower-level (i.e., byte) type representation. For the purposes of [de]serialization, this may change in the future. Local data is addressed by integer-typed \"slots\", being backed by a simple std::vector : in other words, slot indices are dense. All slots must be pre-allocated to contain all references to locals in the current function stack frame. On each \"tick\", the interpreter reads the current instruction, as given by the PC (conceptually, the only register in the virtual machine), executes the described operation (usually consuming values from the stack), and places the result on the stack.","title":"Structure"},{"location":"dslx_bytecode_interpreter/#isa","text":"Each instruction consists of an opcode plus, optionally, some piece of data, either int64 - or InterpValue -typed, depending on the specific opcode. The below opcodes are supported by the interpreter: ADD : Adds the two values at the top of the stack. CALL : Invokes the function given as the optional data argument, consuming a number of arguments from the stack as described by the function signature. The N'th parameter will be present as the N'th value down the stack (such that the last parameter will be the value initially on top of the stack. CREATE_TUPLE : Groups together N items on the stack (given by the optional data argument into a single InterpValue . EXPAND_TUPLE : Expands the N-tuple at stack top by one level, placing leading elements at stack top. In other words, expanding the tuple (a, (b, c)) will result in a stack of (b, c), a , where a is on top of the stack. EQ : Compares the two values on top of the stack for equality. Emits a single-bit value. LOAD : Loads the value from locals slot n , where n is given by the optional data argument. LITERAL : Places a literal value (given in the optional data argument) on top of the stack. STORE : Stores the value at stack top into slot n in locals storage.","title":"ISA"},{"location":"dslx_bytecode_interpreter/#bytecode-generation","text":"The bytecode emitter is responsible for converting a set of DSLX ASTs (one per function)) into a set of linear bytecode representations. It does this via a postorder traversal of the AST, converting XLS ops into bytecode instructions along the way, e.g., converting a DSLX Binop for adding two NameRef s into two LOAD instructions (one for each NameRef ) and one ADD instruction. To do this, the emitter needs access to the full set of resolved type and import information: in other words, it requires a fully-populated ImportData and the top-level TypeInfo for the module containing the function to convert. This places bytecode emission in sequence after typechecking and deduction.","title":"Bytecode generation"},{"location":"dslx_bytecode_interpreter/#implementation-details","text":"","title":"Implementation details"},{"location":"dslx_bytecode_interpreter/#map-builtin","text":"The map() function built-in to DSLX accepts an array-typed value x and a mapping function f with the signature T -> U ; that is, it accepts a single value of type T and returns a single value with the type U . In operation, map() applies the mapping function f to every element in x and returns a new array containing the results (with element i in the output corresponding to element i in the input). Conceptually, map() destructures to a for loop over the elements in x , and that's essentially what the interpreter does with these opcodes. To avoid modifying the currently executing bytecode, the interpreter instead creates a new BytecodeFunction consisting of just that destructured for loop over the inputs, followed by a CreateArray op to collect the output(s). Finally, the interpreter begins execution of the new function by creating a new Frame on top of the execution stack.","title":"map builtin"},{"location":"dslx_reference/","text":"DSLX Reference Overview DSLX is a domain specific, dataflow-oriented functional language used to build hardware that can also run effectively as host software. Within the XLS project, DSLX is also referred to as \"the DSL\". The DSL targets the XLS compiler (via conversion to XLS IR) to enable flows for FPGAs and ASICs. DSLX mimics Rust, while being an immutable expression-based dataflow DSL with hardware-oriented features; e.g. arbitrary bitwidths, entirely fixed size objects, fully analyzeable call graph, etc. To avoid arbitrary new syntax/semantics choices, the DSL mimics Rust where it is reasonably possible; for example, integer conversions all follow the same semantics as Rust. Note: There are some unnecessary differences today from Rust syntax due to early experimentation, but they are quickly being removed to converge on Rust syntax. Note that other frontends to XLS core functionality will become available in the future; e.g. xlscc , for users familiar with the C++-and-pragma style of HLS computation. XLS team develops the DSL as part of the XLS project because we believe it can offer significant advantages over the C++-with-pragmas approach. Dataflow DSLs are a good fit for describing hardware, compared to languages whose design assumes von Neumann style computation (global mutable state, sequential mutation by a sequential thread of control). Using a Domain Specific Language (DSL) provides a more hardware-oriented representation of a given computation that matches XLS compiler (IR) constructs closely. The DSL also allows an exploration of HLS without being encumbered by C++ language or compiler limitations such as non-portable pragmas, magic macros, or semantically important syntactic conventions. The language is still experimental and likely to change, but it is already useful for experimentation and exploration. This document provides a reference for DSLX, mostly by example. Before perusing it in detail, we recommend you first read the DSLX tutorials to understand the broad strokes of the language. In this document we use the function to compute a CRC32 checksum to describe language features. The full code is in examples/dslx_intro/crc32_one_byte.x . Comments Just as in languages like Rust/C++, comments start with // and last through the end of the line. Identifiers All identifiers, eg., for function names, parameters, and values, follow the typical naming rules of other languages. The identifiers can start with a character or an underscore, and can then contain more characters, underscores, or numbers. Valid examples are: a // valid CamelCase // valid like_under_scores // valid __also_ok // valid _Ok123_321 // valid _ // valid 2ab // not valid &ade // not valid However, we suggest the following DSLX style rules , which mirror the Rust naming conventions . Functions are written_like_this User-defined data types are NamesLikeThis Constant bindings are NAMES_LIKE_THIS _ is the \"black hole\" identifier -- a name that you can bind to but should never read from, akin to Rust's wildcard pattern match or Python's \"unused identifier\" convention. It should never be referred to in an expression except as a \"sink\". NOTE Since mutable locals are not supported, there is also support for \"tick identifiers\" , where a ' character may appear anywhere after the first character of an identifier to indicate \"prime\"; e.g. let state' = update(state); . By convention ticks usually come at the end of an identifier. Since this is not part of Rust's syntax, it is considered experimental at this time. Functions Function definitions begin with the keyword fn , followed by the function name, a parameter list to the function in parenthesis, followed by an -> and the return type of the function. After this, curly braces denote the begin and end of the function body. The list of parameters can be empty. A single input file can contain many functions. Simple examples: fn ret3 () -> u32 { u32 : 3 // This function always returns 3. } fn add1 ( x : u32 ) -> u32 { x + u32 : 1 // Returns x + 1, but you knew that! } Functions return the result of their last computed expression as their return value. There are no explicit return statements. By implication, functions return exactly one expression; they can't return multiple expressions (but this may change in the future as we migrate towards some Rust semantics). Tuples should be returned if a function needs to return multiple values. Parameters Parameters are written as pairs name followed by a colon : followed by the type of that parameter. Each parameter needs to declare its own type. Examples: // a simple parameter x of type u32 x: u32 // t is a tuple with 2 elements. // the 1st element is of type u32 // the 2nd element is a tuple with 3 elements // the 1st element is of type u8 // the 2nd element is another tuple with 1 element of type u16 // the 3rd element is of type u8 t: (u32, (u8, (u16,), u8)) Parametric Functions DSLX functions can be parameterized in terms of the types of its arguments and in terms of types derived from other parametric values. For instance: fn double ( n : u32 ) -> u32 { n * u32 : 2 } fn self_append < A : u32 , B : u32 = double ( A ) > ( x : bits [ A ]) -> bits [ B ] { x ++ x } fn main () -> bits [ 10 ] { self_append ( u5 : 1 ) } In self_append(bits[5]:1) , we see that A = 5 based off of formal argument instantiation. Using that value, we can evaluate B = double(A=5) . This derived expression is analogous to C++'s constexpr \u2013 a simple expression that can be evaluated at that point in compilation. See advanced understanding for more information on parametricity. Explicit parametric instantiation In some cases, parametric values cannot be inferred from function arguments, such as in the explicit_parametric_simple.x test: fn add_one<E:u32, F:u32, G:u32 = E+F>(lhs: bits[E]) -> bits[G] { ... } For this call to instantiable, both E and F must be specified. Since F can't be inferred from an argument, we must rely on explicit parametrics : add_one < u32 : 1 , { u32 : 2 + u32 : 3 } > ( u1 : 1 ); This invocation will bind 1 to E , 5 to F , and 6 to G . Note the curly braces around the expression-defined parametric: simple literals and constant references do not need braces (but they can have them), but any other expression requires them. Expression ambiguity Without curly braces, explicit parametric expressions could be ambiguous; consider the following, slightly changed from the previous example: add_one < u32 : 1 , u32 : 2 > ( u32 : 3 ) > ( u1 : 1 ); Is the statement above computing add_one<1, (2 > 3)>(1) , or is it computing (add_one<1, 2>(3)) > 1) ? Without additional (and subtle and perhaps surprising) contextual precedence rules, this would be ambiguous and could lead to a parse error or, even worse, unexpected behavior. Fortunately, we can look to Rust for inspiration. Rust's const generics RPF introduced the { } syntax for disambiguating just this case in generic specifications. With this, any expressions present in a parametric specification must be contained within curly braces, as in the original example. At present, if the braces are omitted, some unpredictable error will occur. Work to improve this is tracked in XLS GitHub issue #321 . Function Calls Function calls are expressions and look and feel just like one would expect from other languages. For example: fn callee ( x : bits [ 32 ], y : bits [ 32 ]) -> bits [ 32 ] { x + y } fn caller () -> u32 { callee ( u32 : 2 , u32 : 3 ) } If more than one value should be returned by a function, a tuple type should be returned. Types Bit Type The most fundamental type in DSLX is a variable length bit type denoted as bits[n] , where n is a constant. For example: bits[0] // possible, but, don't do that bits[1] // a single bit uN[1] // explicitly noting single bit is unsigned u1 // convenient shorthand for bits[1] bits[8] // an 8-bit datatype, yes, a byte u8 // convenient shorthand for bits[8] bits[32] // a 32-bit datatype u32 // convenient shorthand for bits[32] bits[256] // a 256-bit datatype DSLX introduces aliases for commonly used types, such as u8 for an 8-wide bit type, or u32 for a 32-bit wide bit type. These are defined up to u64 . All u* , uN[*] , and bits[*] types are interpreted as unsigned integers. Signed integers are specified via s* and sN[*] . Similarly to unsigned numbers, the s* shorthands are defined up to s64 . For example: sN[0] s0 sN[1] s1 sN[64] s64 sN[256] Signed numbers differ in their behavior from unsigned numbers primarily via operations like comparisons, (variable width) multiplications, and divisions. Enum Types DSLX supports enumerations as a way of defining a group of related, scoped, named constants that do not pollute the module namespace. For example: enum Opcode : u3 { FIRE_THE_MISSILES = 0 , BE_TIRED = 1 , TAKE_A_NAP = 2 , } fn get_my_favorite_opcode () -> Opcode { Opcode :: FIRE_THE_MISSILES } Note the use of the double-colon to reference the enum value. This code specifies that the enum behaves like a u3 : its storage and extension (via casting) behavior are defined to be those of a u3 . Attempts to define an enum value outside of the representable u3 range will produce a compile time error. enum Opcode : u3 { FOO = 8 // Causes compile time error! } Enums can be compared for equality/inequality, but they do not permit arithmetic operations, they must be cast to numerical types in order to perform arithmetic: enum Opcode : u3 { NOP = 0 , ADD = 1 , SUB = 2 , MUL = 3 , } fn same_opcode ( x : Opcode , y : Opcode ) -> bool { x == y // ok } fn next_in_sequence ( x : Opcode , y : Opcode ) -> bool { // x+1 == y // does not work, arithmetic! x as u3 + u3 : 1 == ( y as u3 ) // ok, casted first } As mentioned above, casting of enum-values works with the same casting/extension rules that apply to the underlying enum type definition. For example, this cast will sign extend because the source type for the enum is signed. (See numerical conversions for the full description of extension/truncation behavior.) enum MySignedEnum : s3 { LOW = - 1 , ZERO = 0 , HIGH = 1 , } fn extend_to_32b ( x : MySignedEnum ) -> u32 { x as u32 // Sign-extends because the source type is signed. } #[test] fn test_extend_to_32b () { assert_eq ( extend_to_32b ( MySignedEnum :: LOW ), u32 : 0xffffffff ) } Casting to an enum is also permitted. However, in most cases errors from invalid casting can only be found at runtime, e.g., in the DSL interpreter or flagging a fatal error from hardware. Because of that, it is recommended to avoid such casts as much as possible. Tuple Type A tuple is a fixed-size ordered set, containing elements of heterogeneous types. Tuples elements can be any type, e.g. bits, arrays, structs, tuples. Tuples may be empty (an empty tuple is also known as the unit type), or contain one or more types. Examples of tuple values: // The unit type, carries no information. let unit = (); // A tuple containing two bits-typed elements. let pair = ( u3 : 0b100 , u4 : 0b1101 ); Example of a tuple type: // The type of a tuple with 2 elements. // the 1st element is of type u32 // the 2nd element is a tuple with 3 elements // the 1st element is of type u8 // the 2nd element is another tuple with 1 element of type u16 // the 3rd element is of type u8 type MyTuple = ( u32 , ( u8 , ( u16 ,), u8 )); To access individual tuple elements use simple indices, starting at 0. For example, to access the second element of a tuple (index 1): #[test] fn test_tuple_access () { let t = ( u32 : 2 , u8 : 3 ); assert_eq ( u8 : 3 , t . 1 ) } Such indices can only be numeric literals; parametric symbols are not allowed. Tuples can be \"destructured\", similarly to how pattern matching works in match expressions, which provides a convenient syntax to name elements of a tuple for subsequent use. See a and b in the following: #[test] fn test_tuple_destructure () { let t = ( u32 : 2 , u8 : 3 ); let ( a , b ) = t ; let _ = assert_eq ( u32 : 2 , a ); assert_eq ( u8 : 3 , b ) } Just as values can be discarded in a let by using the \"black hole identifier\" _ , don't-care values can also be discarded when destructuring a tuple: #[test] fn test_black_hole () { let t = ( u32 : 2 , u8 : 3 , true ); let ( _ , _ , v ) = t ; assert_eq ( v , true ) } Struct Types Structures are similar to tuples, but provide two additional capabilities: we name the slots (i.e. struct fields have names while tuple elements only have positions), and we introduce a new type. The following syntax is used to define a struct: struct Point { x : u32 , y : u32 } Once a struct is defined it can be constructed by naming the fields in any order: struct Point { x : u32 , y : u32 , } #[test] fn test_struct_equality () { let p0 = Point { x : u32 : 42 , y : u32 : 64 }; let p1 = Point { y : u32 : 64 , x : u32 : 42 }; assert_eq ( p0 , p1 ) } There is a simple syntax when creating a struct whose field names match the names of in-scope values: struct Point { x : u32 , y : u32 , } #[test] fn test_struct_equality () { let x = u32 : 42 ; let y = u32 : 64 ; let p0 = Point { x , y }; let p1 = Point { y , x }; assert_eq ( p0 , p1 ) } Struct fields can also be accessed with \"dot\" syntax: struct Point { x : u32 , y : u32 , } fn f ( p : Point ) -> u32 { p . x + p . y } fn main () -> u32 { f ( Point { x : u32 : 42 , y : u32 : 64 }) } #[test] fn test_main () { assert_eq ( u32 : 106 , main ()) } Note that structs cannot be mutated \"in place\", the user must construct new values by extracting the fields of the original struct mixed together with new field values, as in the following: struct Point3 { x : u32 , y : u32 , z : u32 , } fn update_y ( p : Point3 , new_y : u32 ) -> Point3 { Point3 { x : p . x , y : new_y , z : p . z } } fn main () -> Point3 { let p = Point3 { x : u32 : 42 , y : u32 : 64 , z : u32 : 256 }; update_y ( p , u32 : 128 ) } #[test] fn test_main () { let want = Point3 { x : u32 : 42 , y : u32 : 128 , z : u32 : 256 }; assert_eq ( want , main ()) } Struct Update Syntax The DSL has syntax for conveniently producing a new value with a subset of fields updated to reduce verbosity. The \"struct update\" syntax is: struct Point3 { x : u32 , y : u32 , z : u32 , } fn update_y ( p : Point3 ) -> Point3 { Point3 { y : u32 : 42 , .. p } } fn update_x_and_y ( p : Point3 ) -> Point3 { Point3 { x : u32 : 42 , y : u32 : 42 , .. p } } Parametric Structs DSLX also supports parametric structs. For more information on how type-parametricity works, see the parametric functions section. fn double ( n : u32 ) -> u32 { n * u32 : 2 } struct Point < N : u32 , M : u32 = double ( N ) > { x : bits [ N ], y : bits [ M ], } fn make_point < A : u32 , B : u32 > ( x : bits [ A ], y : bits [ B ]) -> Point < A , B > { Point { x , y } } #[test] fn test_struct_construction () { let p = make_point ( u16 : 42 , u32 : 42 ); assert_eq ( u16 : 42 , p . x ) } Understanding Nominal Typing As mentioned above, a struct definition introduces a new type. Structs are nominally typed, as opposed to structurally typed (note that tuples are structurally typed). This means that structs with different names have different types, regardless of whether those structs have the same structure (i.e. even when all the fields of two structures are identical, those structures are a different type when they have a different name). struct Point { x : u32 , y : u32 , } struct Coordinate { x : u32 , y : u32 , } fn f ( p : Point ) -> u32 { p . x + p . y } #[test] fn test_ok () { assert_eq ( f ( Point { x : u32 : 42 , y : u32 : 64 }), u32 : 106 ) } #[test] fn test_type_checker_error () { assert_eq ( f ( Coordinate { x : u32 : 42 , y : u32 : 64 }), u32 : 106 ) } Array Type Arrays can be constructed via bracket notation. All values that make up the array must have the same type. Arrays can be indexed with indexing notation ( a[i] ) to retrieve a single element. fn main ( a : u32 [ 2 ], i : u1 ) -> u32 { a [ i ] } #[test] fn test_main () { let x = u32 : 42 ; let y = u32 : 64 ; // Make an array with \"bracket notation\". let my_array : u32 [ 2 ] = [ x , y ]; let _ = assert_eq ( main ( my_array , u1 : 0 ), x ); let _ = assert_eq ( main ( my_array , u1 : 1 ), y ); () } Because arrays with repeated trailing elements are common, the DSL supports ellipsis ( ... ) at the end of an array to fill the remainder of the array with the last noted element. Because the compiler must know how many elements to fill, in order to use the ellipsis the type must be annotated explicitly as shown. fn make_array ( x : u32 ) -> u32 [ 3 ] { u32 [ 3 ] : [ u32 : 42 , x , .. .] } #[test] fn test_make_array () { let _ = assert_eq ( u32 [ 3 ] : [ u32 : 42 , u32 : 42 , u32 : 42 ], make_array ( u32 : 42 )); let _ = assert_eq ( u32 [ 3 ] : [ u32 : 42 , u32 : 64 , u32 : 64 ], make_array ( u32 : 64 )); () } TODO(meheff): Explain arrays and the intricacies of our bits type interpretation and how it affects arrays of bits etc. Character String Constants Character strings are a special case of array types, being implicitly-sized arrays of u8 elements. String constants can be used just as traditional arrays: fn add_one < N : u32 > ( input : u8 [ N ]) -> u8 [ N ] { for ( i , result ) : ( u32 , u8 [ N ]) in u32 : 0 .. N { update ( result , i , result [ i ] + u8 : 1 ) }( input ) } #[test] fn test_main () { assert_eq ( \"bcdef\" , add_one ( \"abcde\" )) } DSLX string constants support the full Rust set of escape sequences - for multi-byte sequences, i.e., Unicode escapes, the resulting byte sequence will be in printed order. In other words, the sequence \\u{89ab} will result in an array with the (binary) values 1000 1001 1010 1011 in sequence. Type Aliases DLSX supports the definition of type aliases. Type aliases can be used to provide a more human-readable name for an existing type. The new name is on the left, the existing name on the right: type Weight = u6 ; We can create an alias for an imported type: // Note: this imports an external file in the codebase. import xls . dslx . tests . mod_imported type MyEnum = mod_imported :: MyEnum ; fn main ( x : u8 ) -> MyEnum { x as MyEnum } #[test] fn test_main () { let _ = assert_eq ( main ( u8 : 42 ), MyEnum :: FOO ); let _ = assert_eq ( main ( u8 : 64 ), MyEnum :: BAR ); () } Type aliases can also provide a descriptive name for a tuple type (which is otherwise anonymous). For example, to define a tuple type that represents a float number with a sign bit, an 8-bit mantissa, and a 23-bit mantissa, one would write: type F32 = ( u1 , u8 , u23 ); After this definition, the F32 may be used as a type annotation interchangeably with (u1, u8, u23) . Note, however, that structs are generally preferred for this purpose, as they are more readable and users do not need to rely on tuple elements having a stable order in the future (i.e., they are resilient to refactoring). Type Casting Bit types can be cast from one bit-width to another with the as keyword. Types can be widened (increasing bit-width), narrowed (decreasing bit-width) and/or changed between signed and unsigned. Some examples are found below. See Numerical Conversions for a description of the semantics. #[test] fn test_narrow_cast () { let twelve = u4 : 0b1100 ; assert_eq ( twelve as u2 , u2 : 0 ) } #[test] fn test_widen_cast () { let three = u2 : 0b11 ; assert_eq ( three as u4 , u4 : 3 ) } #[test] fn test_narrow_signed_cast () { let negative_seven = s4 : 0b1001 ; assert_eq ( negative_seven as u2 , u2 : 1 ) } #[test] fn test_widen_signed_cast () { let negative_one = s2 : 0b11 ; assert_eq ( negative_one as s4 , s4 : - 1 ) } #[test] fn test_widen_to_unsigned () { let negative_one = s2 : 0b11 ; assert_eq ( negative_one as u3 , u3 : 0b111 ) } #[test] fn test_widen_to_signed () { let three = u2 : 0b11 ; assert_eq ( three as u3 , u3 : 0b011 ) } Type Checking and Inference DSLX performs type checking and produces an error if types in an expression don't match up. let expressions also perform type inference, which is quite convenient. For example, instead of writing: let ch : u32 = ( e & f ) ^ (( ! e ) & g ); let ( h , g , f ) : ( u32 , u32 , u32 ) = ( g , f , e ); one can write the following, as long as the types can be properly inferred: let ch = ( e & f ) ^ (( ! e ) & g ); let ( h , g , f ) = ( g , f , e ); Note that type annotations can still be added and be used for program understanding, as they they will be checked by DSLX. Type Inference Details Type Inference Background All expressions in the language's expression grammar have a deductive type inference rule. The types must be known for inputs to an operator/function 1 and every expression has a way to determine its type from its operand expressions. DSLX uses deductive type inference to check the types present in the program. Deductive type inference is a set of (typically straight-forward) deduction rules: Hindley-Milner style deductive type inference determines the result type of a function with a rule that only observes the input types to that function. (Note that operators like '+' are just slightly special functions in that they have pre-defined special-syntax-rule names.) Bindings and Environment In DSLX code, the \"environment\" where names are bound (sometimes also referred to as a symbol table) is called the Bindings -- it maps identifiers to the AST node that defines the name ( {string: AstNode} ), which can be combined with a mapping from AST node to its deduced type ( {AstNode: ConcreteType} ) to resolve the type of an identifier in the program. Let is one of the key nodes that populates these Bindings , but anything that creates a bound name does as well (e.g. parameters, for loop induction variables, etc.). Operator Example For example, consider the binary (meaning takes two operands) / infix (meaning it syntactically is placed in the center of its operands) '+' operator. The simple deductive type inference rule for '+' is: (T, T) -> T Meaning that the left hand side operand to the '+' operator is of some type (call it T), the right hand side operand to the '+' operator must be of that same type, T, and the result of that operator is then (deduced) to be of the same type as its operands, T. Let's instantiate this rule in a function: fn add_wrapper ( x : bits [ 2 ], y : bits [ 2 ]) -> bits [ 2 ] { x + y } This function wraps the '+' operator. It presents two arguments to the '+' operator and then checks that the annotated return type on add_wrapper matches the deduced type for the body of that function; that is, we ask the following question of the '+' operator (since the type of the operands must be known at the point the add is performed): (bits[2], bits[2]) -> ? To resolve the '?' the following procedure is being used: Pattern match the rule given above (T, T) -> T to determine the type T: the left hand side operand is bits[2] , called T. Check that the right hand side is also that same T, which it is: another bits[2] . Deduce that the result type is that same type T: bits[2] . That becomes the return type of the body of the function. Check that it is the same type as the annotated return type for the function, and it is! The function is annotated to return bits[2] , and the deduced type of the body is also bits[2] . Qed. Type errors A type error would occur in the following: fn add_wrapper ( x : bits [ 2 ], y : bits [ 3 ]) -> bits [ 2 ] { x + y } Applying the type deduction rule for '+' finds an inconsistency. The left hand side operand has type bits[2] , called T, but the right hand side is bits[3] , which is not the same as T. Because the deductive type inference rule does not say what to do when the operand types are different, it results in a type error which is flagged at this point in the program. Let Bindings, Names, and the Environment In the DSL, let is an expression. It may not seem obvious at a glance, but it is! As a primer see the type inference background and how names are resolved in an environment . \"let\" expressions are of the (Rust-inspired) form: let $name: $annotated_type = $expr; $subexpr $name gets \"bound\" to a value of type $annotated_type . The let typecheck rule must both check that $expr is of type $annotated_type , as well as determine the type of $subexpr , which is the type of the overall \"let expression\". In this example, the result of the let expression is the return value -- $subexpr ( x+x ) can use the $name ( x ) which was \"bound\": fn main ( y : u32 ) -> u64 { let x : u64 = y as u64 ; x + x } If we invoke main(u32:2) we will the evaluate let expression -- it creates a binding of x to the value u64:2 , and then evaluates the expression x+x in that environment, so the result of the let expression's $subexpr is u64:4 . Statements Imports DSLX modules can import other modules via the import keyword. Circular imports are not permitted (the dependencies among DSLX modules must form a DAG, as in languages like Go). The import statement takes the following form (note the lack of semicolon): import path.to.my.imported_module With that statement, the module will be accessible as (the trailing identifier after the last dot) imported_module ; e.g. the program can refer to imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT . NOTE Imports are relative to the Bazel \"depot root\" -- for external use of the tools a DSLX_PATH will be exposed, akin to a PYTHONPATH , for users to indicate paths where were should attempt module discovery. NOTE Importing does not introduce any names into the current file other than the one referred to by the import statement. That is, if imported_module had a constant defined in it FOO , this is referred to via imported_module::FOO , FOO does not \"magically\" get put in the current scope. This is analogous to how wildcard imports are discouraged in other languages (e.g. from import * in Python) on account of leading to \"namespace pollution\" and needing to specify what happens when names conflict. If you want to change the name of the imported module (for reference inside of the importing file) you can use the as keyword: import path . to . my . imported_module as im Just using the above construct, imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT is not valid, only im::IMPORTED_MODULE_PUBLIC_CONSTANT . However, both statements can be used on different lines: import path . to . my . imported_module import path . to . my . imported_module as im In this case, either im::IMPORTED_MODULE_PUBLIC_CONSTANT or imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT can be used to refer to the same thing. Here is an example using the same function via two different aliases for the same module: // Note: this imports an external file in the codebase under two different // names. import xls . dslx . tests . mod_imported import xls . dslx . tests . mod_imported as mi fn main ( x : u3 ) -> u1 { mod_imported :: my_lsb ( x ) || mi :: my_lsb ( x ) } #[test] fn test_main () { assert_eq ( u1 : 0b1 , main ( u3 : 0b001 )) } Public module members Module members are private by default and not accessible from any importing module. To make a member public/visible to importing modules, the pub keyword must be added as a prefix; e.g. const FOO = u32 : 42 ; // Not accessible to importing modules. pub const BAR = u32 : 64 ; // Accessible to importing modules. This applies to other things defined at module scope as well: functions, enums, type aliases, etc. import xls . dslx . tests . mod_imported import xls . dslx . tests . mod_imported as mi fn main ( x : u3 ) -> u1 { mod_imported :: my_lsb ( x ) || mi :: my_lsb ( x ) } #[test] fn test_main () { assert_eq ( u1 : 0b1 , main ( u3 : 0b001 )) } Const The const keyword is used to define module-level constant values. Named constants should be usable anywhere a literal value can be used: const FOO = u8 : 42 ; fn match_const ( x : u8 ) -> u8 { match x { FOO => u8 : 0 , _ => u8 : 42 , } } #[test] fn test_match_const_not_binding () { let _ = assert_eq ( u8 : 42 , match_const ( u8 : 0 )); let _ = assert_eq ( u8 : 42 , match_const ( u8 : 1 )); let _ = assert_eq ( u8 : 0 , match_const ( u8 : 42 )); () } fn h ( t : ( u8 , ( u16 , u32 ))) -> u32 { match t { ( FOO , ( x , y )) => ( x as u32 ) + y , ( _ , ( y , u32 : 42 )) => y as u32 , _ => u32 : 7 , } } #[test] fn test_match_nested () { let _ = assert_eq ( u32 : 3 , h (( u8 : 42 , ( u16 : 1 , u32 : 2 )))); let _ = assert_eq ( u32 : 1 , h (( u8 : 0 , ( u16 : 1 , u32 : 42 )))); let _ = assert_eq ( u32 : 7 , h (( u8 : 0 , ( u16 : 1 , u32 : 0 )))); () } Expressions Literals DSLX supports construction of literals using the syntax Type:Value . For example u16:1 is a 16-wide bit array with its least significant bit set to one. Similarly s8:12 is an 8-wide bit array with its least significant four bits set to 1100 . DSLX supports initializing using binary, hex or decimal syntax. So #[test] fn test_literal_initialization () { let _ = assert_eq ( u8 : 12 , u8 : 0b00001100 ); let _ = assert_eq ( u8 : 12 , u8 : 0x0c ); () } When constructing literals DSLX will trigger an error if the constant will not fit in a bit array of the annotated sized, so for example trying to construct the literal u8:256 will trigger an error of the form: TypeInferenceError: uN[8] Value '256' does not fit in the bitwidth of a uN[8] (8) But what about s8:128 ? This is a valid literal, even though a signed 8-bit integer cannot represent it. The following code offers a clue. #[test] fn test_signed_literal_initialization () { let _ = assert_eq ( s8 : 128 , s8 : - 128 ); let _ = assert_eq ( s8 : 128 , s8 : 0b10000000 ); () } What is happening here is that, 128 is being used as a bit pattern rather than as the number 128 to initialize the literal. It is only when the bit pattern cannot fit in the width of the iteral that an error is triggered. Note that behaviour is different from Rust, where it will trigger an error, and the fact that DSLX considers this valid may change in the future . Unary Expressions DSLX supports three types of unary expressions: bit-wise not (the ! operator) negate (the - operator, computes the two's complement negation) Binary Expressions DSLX supports a familiar set of binary expressions. There are two categories of binary expressions. A category where both operands to the expression must be of the same bit type (i.e., not arrays or tuples), and a category where the operands can be of arbitrary bit types (i.e. shift expressions). shift-right ( >> ) shift-left ( << ) bit-wise or ( | ) bit-wise and ( & ) add ( + ) subtract ( - ) xor ( ^ ) multiply ( * ) logical or ( || ) logical and ( && ) Shift Expressions Shift expressions include: shift-right (logical) and shift-left. These are binary operations that don't require the same type on the left and right hand side. The right hand side must be unsigned, but it does not need to be the same type or width as the left hand side, i.e. the type signature for these operations is: (xN[M], uN[N]) -> xN[M] . If the right hand side is a literal value it does not need to be type annotated. For example: fn shr_two ( x : s32 ) -> s32 { x >> 2 } Note that, as in Rust, the semantics of the shift-right ( >> ) operation depends on the signedness of the left hand side. For a signed-type left hand side, the shift-right ( >> ) operation performs a shift-right arithmetic and, for a unsigned-type left hand side, the shift-right ( >> ) operation performs a shift-right (logical). Comparison Expressions For comparison expressions the types of both operands must match. However these operations return a result of type bits[1] , aka bool . equal ( == ) not-equal ( != ) greater-equal ( >= ) greater ( > ) less-equal ( <= ) less ( < ) Concat Expression Bitwise concatenation is performed with the ++ operator. The value on the left hand side becomes the most significant bits, the value on the right hand side becomes the least significant bits. These may be chained together as shown below: #[test] fn test_bits_concat () { let _ = assert_eq ( u8 : 0b11000000 , u2 : 0b11 ++ u6 : 0b000000 ); let _ = assert_eq ( u8 : 0b00000111 , u2 : 0b00 ++ u6 : 0b000111 ); let _ = assert_eq ( u6 : 0b100111 , u1 : 1 ++ u2 : 0b00 ++ u3 : 0b111 ); let _ = assert_eq ( u6 : 0b001000 , u1 : 0 ++ u2 : 0b01 ++ u3 : 0b000 ); let _ = assert_eq ( u32 : 0xdeadbeef , u16 : 0xdead ++ u16 : 0xbeef ); () } Block Expressions Block expressions enable subordinate scopes to be defined, e.g.: let a = { let b = u32:1; b + u32:3 }; The value of a block expression is that of its last contained expression, or (), if a final expression is omitted: let a = { let b = u32:1; }; In the above case, a is equal to () . Since DSLX does not currently have the concept of lifetimes, and since names can be rebound (i.e., there's no concept of mutability, allowing let a = u32:0; let a = u32:1; ), blocks are primarily for readability at this time, (side from their use as the \"body\" of functions and loops). Match Expression Match expressions permit \"pattern matching\" on data, like a souped-up switch statement. It can both test for values (like a conditional guard) and bind values to identifiers for subsequent use. For example: fn f ( t : ( u8 , u32 )) -> u32 { match t { ( u8 : 42 , y ) => y , ( _ , y ) => y + u32 : 77 } } If the first member of the tuple is the value is 42 , we pass the second tuple member back as-is from the function. Otherwise, we add 77 to the value and return that. The _ symbolizes \"I don't care about this value\". Just like literal constants, pattern matching can also match via named constants; For example, consider this variation on the above: const MY_FAVORITE_NUMBER = u8 : 42 ; fn f ( t : ( u8 , u32 )) -> u32 { match t { ( MY_FAVORITE_NUMBER , y ) => y , ( _ , y ) => y + u32 : 77 } } This also works with nested tuples; for example: const MY_FAVORITE_NUMBER = u8 : 42 ; fn f ( t : ( u8 , ( u16 , u32 ))) -> u32 { match t { ( MY_FAVORITE_NUMBER , ( y , z )) => y as u32 + z , ( _ , ( y , u32 : 42 )) => y as u32 , _ => u32 : 7 } } Here we use a \"catch all\" wildcard pattern in the last match arm to ensure the match expression always matches the input somehow. Redundant Patterns match will flag an error if a syntactically identical pattern is typed twice; e.g. const FOO = u32 : 42 ; fn f ( x : u32 ) -> u2 { match x { FOO => u2 : 0 , FOO => u2 : 1 , // Identical pattern! _ => u2 : 2 , } } Only the first pattern will ever match, so it is fully redundant (and therefore likely a user error they'd like to be informed of). Note that equivalent but not syntactically identical patterns will not be flagged in this way. const FOO = u32 : 42 ; const BAR = u32 : 42 ; // Compares `==` to `FOO`. fn f ( x : u32 ) -> u2 { match x { FOO => u2 : 0 , BAR => u2 : 1 , // _Equivalent_ pattern, but not syntactically identical. _ => u2 : 2 , } } let Expression let expressions work the same way as let expressions in other functional languages (such as the ML family languages). let expressions provide a nested, lexically-scoped, list of binding definitions. The scope of the binding is the expression on the right hand side of the declaration. For example: let a : u32 = u32 : 1 + u32 : 2 ; let b : u32 = a + u32 : 3 ; b would bind (and return as a value) the value 6 which corresponds to b when evaluated. In effect there is little difference to other languages like C/C++ or Python, where the same result would be achieved with code similar to this: a = 1 + 2 b = a + 3 return b However, let expressions are lexically scoped. In above example, the value 3 is bound to a only during the combined let expression sequence. There is no other type of scoping in DSLX. Ternary If Expression Note: ternary expression syntax is expected to change to mimic Rust's, see #318 . DSLX offers a ternary if expression, which is very similar to the Rust ternary if expression. Blueprint: if condition { consequent } else { alternate } This corresponds to the C/C++ ternary ?: operator: condition ? consequent : alternate Note: both the if and else are required to be present, as with the ?: operator, unlike a C++ if statement. This is because it is an expression that produces a result value, not a statement that causes a mutating effect. For example, in the FP adder module (modules/fp32_add_2.x), there is code like the following: [...] let result_fraction = if wide_exponent < u9:255 { result_fraction } else { u23:0 }; let result_exponent = if wide_exponent < u9:255 { wide_exponent as u8 } else { u8:255 }; Iterable Expression Iterable expressions are used in counted for loops. DSLX currently supports two types of iterable expressions, range and enumerate . The range expression m..n represents a range of values from m to n-1. This example will run from 0 to 4 (exclusive): for (i, accum): (u32, u32) in u32:0..u32:4 { There also exists a range() builtin function that performs the same operation. enumerate iterates over the elements of an array type and produces pairs of (index, value) , similar to enumeration constructs in languages like Python or Go. In the example below, the loop will iterate 8 times, following the array dimension of x . Each iteration produces a tuple with the current index ( i ranging from 0 to 7) and the value at the index ( e = x[i] ). fn prefix_scan_eq(x: u32[8]) -> bits[8,3] { let (_, _, result) = for ((i, e), (prior, count, result)): ((u32, u32), (u32, u3, bits[8,3])) in enumerate(x) {... for Expression DSLX currently supports synthesis of \"counted\" for loops (loops that have a clear upper bound on their number of iterations). These loops are capable of being generated as unrolled pipeline stages: when generating a pipeline, the XLS compiler will unroll and specialize the iterations. NOTE In the future support for loops with an unbounded number of iterations may be permitted, but will only be possible to synthesize as a time-multiplexed implementation, since pipelines cannot be unrolled indefinitely. Blueprint for (index, accumulator): (type-of-index, type-of-accumulator) in iterable { body-expression } (initial-accumulator-value) The type annotation in the above \"blueprint\" is optional, but often helpful to include for increased clarity. Because DSLX is a pure dataflow description, a for loop is an expression that produces a value. As a result, you grab the output of a for loop just like any other expression: let final_accum = for ( i , accum ) in u32 : 0 .. u32 : 8 { let new_accum = f ( accum ); new_accum }( init_accum ); Conceptually the for loop \"evolves\" the accumulator as it iterates, and ultimately pops it out as the result of its evaluation. Examples Add up all values from 0 to 4 (exclusive). Note that we pass the accumulator's initial value in as a parameter to this expression. for ( i , accum ) : ( u32 , u32 ) in u32 : 0 .. u32 : 4 { accum + i }( u32 : 0 ) To add up values from 7 to 11 (exclusive), one would write: let base = u32 : 7 ; for ( i , accum ) : ( u32 , u32 ) in u32 : 0 .. u32 : 4 { accum + base + i }( u32 : 0 ) \"Loop invariant\" values (values that do not change as the loop runs) can be used in the loop body, for example, note the use of outer_thing below: let outer_thing : u32 = u32 : 42 ; for ( i , accum ) : ( u32 , u32 ) in u32 : 0 .. u32 : 4 { accum + i + outer_thing }( u32 : 0 ) Both the index and accumulator can be of any valid type, in particular, the accumulator can be a tuple type, which is useful for evolving a bunch of values. For example, this for loop \"evolves\" two arrays: for ( i , ( xs , ys )) : ( u32 , ( u16 [ 3 ], u8 [ 3 ])) in u32 : 0 .. u32 : 4 { .. . }(( init_xs , init_ys )) Note in the above example arrays are dataflow values just like anything else. To conditionally update an array every other iteration: let result : u4 [ 8 ] = for ( i , array ) in u32 : 0 .. u32 : 8 { // Update every other cell with the square of the index. if i % 2 == 0 { update ( array , i , i * i ) } else { array } }( u4 [ 8 ] : [ 0 , .. .]); Numerical Conversions DSLX adopts the Rust rules for semantics of numeric casts: Casting from larger bit-widths to smaller bit-widths will truncate (to the LSbs). * This means that truncating signed values does not preserve the previous value of the sign bit. Casting from a smaller bit-width to a larger bit-width will zero-extend if the source is unsigned, sign-extend if the source is signed. Casting from a bit-width to its own bit-width, between signed/unsigned, is a no-op. #[test] fn test_numerical_conversions () { let s8_m2 = s8 : - 2 ; let u8_m2 = u8 : 0xfe ; // Sign extension (source type is signed). let _ = assert_eq ( s32 : - 2 , s8_m2 as s32 ); let _ = assert_eq ( u32 : 0xfffffffe , s8_m2 as u32 ); let _ = assert_eq ( s16 : - 2 , s8_m2 as s16 ); let _ = assert_eq ( u16 : 0xfffe , s8_m2 as u16 ); // Zero extension (source type is unsigned). let _ = assert_eq ( u32 : 0xfe , u8_m2 as u32 ); let _ = assert_eq ( s32 : 0xfe , u8_m2 as s32 ); // Nop (bitwidth is unchanged). let _ = assert_eq ( s8 : - 2 , s8_m2 as s8 ); let _ = assert_eq ( s8 : - 2 , u8_m2 as s8 ); let _ = assert_eq ( u8 : 0xfe , u8_m2 as u8 ); let _ = assert_eq ( s8 : - 2 , u8_m2 as s8 ); () } Array Conversions Casting to an array takes bits from the MSb to the LSb; that is, the group of bits including the MSb ends up as element 0, the next group ends up as element 1, and so on. Casting from an array to bits performs the inverse operation: element 0 becomes the MSbs of the resulting value. All casts between arrays and bits must have the same total bit count. fn cast_to_array ( x : u6 ) -> u2 [ 3 ] { x as u2 [ 3 ] } fn cast_from_array ( a : u2 [ 3 ]) -> u6 { a as u6 } fn concat_arrays ( a : u2 [ 3 ], b : u2 [ 3 ]) -> u2 [ 6 ] { a ++ b } #[test] fn test_cast_to_array () { let a_value : u6 = u6 : 0b011011 ; let a : u2 [ 3 ] = cast_to_array ( a_value ); let a_array = u2 [ 3 ] : [ 1 , 2 , 3 ]; let _ = assert_eq ( a , a_array ); // Note: converting back from array to bits gives the original value. let _ = assert_eq ( a_value , cast_from_array ( a )); let b_value : u6 = u6 : 0b111001 ; let b_array : u2 [ 3 ] = u2 [ 3 ] : [ 3 , 2 , 1 ]; let b : u2 [ 3 ] = cast_to_array ( b_value ); let _ = assert_eq ( b , b_array ); let _ = assert_eq ( b_value , cast_from_array ( b )); // Concatenation of bits is analogous to concatenation of their converted // arrays. That is: // // convert(concat(a, b)) == concat(convert(a), convert(b)) let concat_value : u12 = a_value ++ b_value ; let concat_array : u2 [ 6 ] = concat_value as u2 [ 6 ]; let _ = assert_eq ( concat_array , concat_arrays ( a_array , b_array )); // Show a few classic \"endianness\" example using 8-bit array values. let x = u32 : 0xdeadbeef ; let _ = assert_eq ( x as u8 [ 4 ], u8 [ 4 ] : [ 0xde , 0xad , 0xbe , 0xef ]); let y = u16 : 0xbeef ; let _ = assert_eq ( y as u8 [ 2 ], u8 [ 2 ] : [ 0xbe , 0xef ]); () } Bit Slice Expressions DSLX supports Python-style bit slicing over unsigned bits types. Note that bits are numbered 0..N starting \"from the right (as you would write it on paper)\" -- least significant bit, AKA LSb -- for example, for the value u7:0b100_0111 : Bit 6 5 4 3 2 1 0 Value 1 0 0 0 1 1 1 A slice expression [N:M] means to get from bit N (inclusive) to bit M exclusive. The start and limit in the slice expression must be signed integral values. Aside: This can be confusing, because the N stands to the left of M in the expression, but bit N would be to the 'right' of M in the classical bit numbering. Additionally, this is not the case in the classical array visualization, where element 0 is usually drawn on the left. For example, the expression [0:2] would yield: Bit 6 5 4 3 2 1 0 Value 1 0 0 0 1 1 1 ^ ^ included ^ excluded Result: 0b11 Note that, as of now, the indices for this [N:M] form must be literal numbers (so the compiler can determine the width of the result). To perform a slice with a non-literal-number start position, see the +: form described below. The slicing operation also support the python style slices with offsets from start or end. To visualize, one can think of x[ : -1] as the equivalent of x[from the start : bitwidth - 1] . Correspondingly, x[-1 : ] can be visualized as [ bitwidth - 1 : to the end] . For example, to get all bits, except the MSb (from the beginning, until the top element minus 1): x [ : - 1 ] Or to get the two most significant bits: x [ - 2 : ] This results in the nice property that a the original complete value can be sliced into complementary slices such as :-2 (all but the two most significant bits) and -2: (the two most significant bits): #[test] fn slice_into_two_pieces () { let x = u5 : 0b11000 ; let ( lo , hi ) : ( u3 , u2 ) = ( x [ : - 2 ], x [ - 2 : ]); let _ = assert_eq ( hi , u2 : 0b11 ); let _ = assert_eq ( lo , u3 : 0b000 ); () } Width Slice There is also a \"width slice\" form x[start +: bits[N]] - starting from a specified bit, slice out the next N bits. This is equivalent to: bits[N]:(x >> start) . The type can be specified as either signed or unsigned; e.g. [start +: s8] will produce an 8-bit signed value starting at start , whereas [start +: u4] will produce a 4-bit unsigned number starting at start . Here are many more examples : Bit Slice Examples // Identity function helper. fn id < N : u32 > ( x : bits [ N ]) -> bits [ N ] { x } #[test] fn test_bit_slice_syntax () { let x = u6 : 0b100111 ; // Slice out two bits. let _ = assert_eq ( u2 : 0b11 , x [ 0 : 2 ]); let _ = assert_eq ( u2 : 0b11 , x [ 1 : 3 ]); let _ = assert_eq ( u2 : 0b01 , x [ 2 : 4 ]); let _ = assert_eq ( u2 : 0b00 , x [ 3 : 5 ]); // Slice out three bits. let _ = assert_eq ( u3 : 0b111 , x [ 0 : 3 ]); let _ = assert_eq ( u3 : 0b011 , x [ 1 : 4 ]); let _ = assert_eq ( u3 : 0b001 , x [ 2 : 5 ]); let _ = assert_eq ( u3 : 0b100 , x [ 3 : 6 ]); // Slice out from the end. let _ = assert_eq ( u1 : 0b1 , x [ - 1 : ]); let _ = assert_eq ( u1 : 0b1 , x [ - 1 : 6 ]); let _ = assert_eq ( u2 : 0b10 , x [ - 2 : ]); let _ = assert_eq ( u2 : 0b10 , x [ - 2 : 6 ]); let _ = assert_eq ( u3 : 0b100 , x [ - 3 : ]); let _ = assert_eq ( u3 : 0b100 , x [ - 3 : 6 ]); let _ = assert_eq ( u4 : 0b1001 , x [ - 4 : ]); let _ = assert_eq ( u4 : 0b1001 , x [ - 4 : 6 ]); // Slice both relative to the end (MSb). let _ = assert_eq ( u2 : 0b01 , x [ - 4 : - 2 ]); let _ = assert_eq ( u2 : 0b11 , x [ - 6 : - 4 ]); // Slice out from the beginning (LSb). let _ = assert_eq ( u5 : 0b00111 , x [ : - 1 ]); let _ = assert_eq ( u4 : 0b0111 , x [ : - 2 ]); let _ = assert_eq ( u3 : 0b111 , x [ : - 3 ]); let _ = assert_eq ( u2 : 0b11 , x [ : - 4 ]); let _ = assert_eq ( u1 : 0b1 , x [ : - 5 ]); // Slicing past the end just means we hit the end (as in Python). let _ = assert_eq ( u1 : 0b1 , x [ 5 : 7 ]); let _ = assert_eq ( u1 : 0b1 , x [ - 7 : 1 ]); let _ = assert_eq ( bits [ 0 ] : 0 , x [ - 7 : - 6 ]); let _ = assert_eq ( bits [ 0 ] : 0 , x [ - 6 : - 6 ]); let _ = assert_eq ( bits [ 0 ] : 0 , x [ 6 : 6 ]); let _ = assert_eq ( bits [ 0 ] : 0 , x [ 6 : 7 ]); let _ = assert_eq ( u1 : 1 , x [ - 6 : - 5 ]); // Slice of a slice. let _ = assert_eq ( u2 : 0b11 , x [ : 4 ][ 1 : 3 ]); // Slice of an invocation. let _ = assert_eq ( u2 : 0b01 , id ( x )[ 2 : 4 ]); // Explicit-width slices. let _ = assert_eq ( u2 : 0b01 , x [ 2 + : u2 ]); let _ = assert_eq ( s3 : 0b100 , x [ 3 + : s3 ]); let _ = assert_eq ( u3 : 0b001 , x [ 5 + : u3 ]); () } Advanced Understanding: Parametricity, Constraints, and Unification An infamous wrinkle is introduced for parametric functions: consider the following function: // (Note: DSLX does not currently support the `T: type` construct shown here, // it is for example purposes only.) fn add_wrapper < T : type , U : type > ( x : T , y : U ) -> T { x + y } Based on the inference rule, we know that '+' can only type check when the operand types are the same. This means we can conclude that type T is the same as type U . Once we determine this, we need to make sure anywhere U is used it is consistent with the fact it is the same as T . In a sense the + operator is \"adding a constraint\" that T is equivalent to U , and trying to check that fact is valid is under the purview of type inference. The fact that the constraint is added that T and U are the same type is referred to as \"unification\", as what was previously two entities with potentially different constraints now has a single set of constraints that comes from the union of its operand types. DSLX's typechecker will go through the body of parametric functions per invocation. As such, the typechecker will always have the invocation's parametric values for use in asserting type consistency against \"constraints\" such as derived parametric expressions, body vs. annotated return type equality, and expression inference rules. Operator Precedence DSLX's operator precedence matches Rust's. Listed below are DSLX's operators in descending precedence order. Binary operators at the same level share the same associativity and will be grouped accordingly. Operator Associativity Unary - ! n/a as Left to right * / % Left to right + - Left to right << >> >>> Left to right & Left to right ^ Left to right \\| Left to right == != < > <= >= Left to right && Left to right \\|\\| Left to right Builtins This section describes the built-in functions provided for use in the DSL that do not need to be explicitly imported. A note on \"Parallel Primitives\": the DSL is expected to grow additional support for use of high-level parallel primitives over time, adding operators for order-insensitive reductions, scans, groupings, and similar. By making these operations known to the compiler in their high level form, we potentially enable optimizations and analyses on their higher level (\"lifted\") form. As of now, map is the sole parallel-primitive-oriented built-in. add_with_carry Operation that produces the result of the add, as well as the carry bit as an output. The binary add operators works similar to software programming languages, preserving the length of the input operands, so this builtin can assist when easy access to the carry out value is desired. Has the following signature: fn add_with_carry<N>(x: uN[N], y: uN[N]) -> (u1, uN[N]) smulp and umulp smulp and umulp perform signed and unsigned partial multiplications. These operations return a two-element tuple with the property that the sum of the two elements is equal to the product of the original inputs. Performing a partial multiplication allows for a pipeline stage in the middle of a multiply. These operations have the following signatures: fn smulp<N>(lhs: sN[N], rhs: sN[N]) -> (sN[N], sN[N]) fn umulp<N>(lhs: uN[N], rhs: uN[N]) -> (uN[N], uN[N]) map map , similarly to other languages, executes a transformation function on all the elements of an original array to produce the resulting \"mapped' array. For example : taking the absolute value of each element in an input array: import std fn main ( x : s3 [ 3 ]) -> s3 [ 3 ] { let y : s3 [ 3 ] = map ( x , std :: abs ); y } #[test] fn main_test () { let got : s3 [ 3 ] = main ( s3 [ 3 ] : [ - 1 , 1 , 0 ]); assert_eq ( s3 [ 3 ] : [ 1 , 1 , 0 ], got ) } Note that map is special, in that we can pass it a callee as if it were a value. As a function that \"takes\" a function as an argument, map is a special builtin -- in language implementor parlance it is a higher order function . Implementation note: Functions are not first class values in the DSL, so the name of the function must be referred to directly. Note: Novel higher order functions (e.g. if a user wanted to write their own map ) cannot currently be written in user-level DSL code. clz , ctz DSLX provides the common \"count leading zeroes\" and \"count trailing zeroes\" functions: let x0 = u32 : 0x0FFFFFF8 ; let x1 = clz ( x0 ); let x2 = ctz ( x0 ); let _ = assert_eq ( u32 : 4 , x1 ); assert_eq ( u32 : 3 , x2 ) one_hot Converts a value to one-hot form. Has the following signature: fn one_hot<N, NP1=N+1>(x: uN[N], lsb_is_prio: bool) -> uN[NP1] When lsb_is_prio is true, the least significant bit that is set becomes the one-hot bit in the result. When it is false, the most significant bit that is set becomes the one-hot bit in the result. When all bits in the input are unset, the additional bit present in the output value (MSb) becomes set. Example usage: dslx/tests/one_hot.x . See also the IR semantics for the one_hot op . signex Casting has well-defined extension rules, but in some cases it is necessary to be explicit about sign-extensions, if just for code readability. For this, there is the signex builtin. To invoke the signex builtin, provide it with the operand to sign extend (lhs), as well as the target type to extend to: these operands may be either signed or unsigned. Note that the value of the right hand side is ignored, only its type is used to determine the result type of the sign extension. #[test] fn test_signex () { let x = u8 : 0xff ; let s : s32 = signex ( x , s32 : 0 ); let u : u32 = signex ( x , u32 : 0 ); assert_eq ( s as u32 , u ) } Note that both s and u contain the same bits in the above example. slice Array-slice builtin operation. Note that the \"want\" argument is not used as a value, but is just used to reflect the desired slice type. (Prior to constexprs being passed to builtin functions, this was the canonical way to reflect a constexpr in the type system.) Has the following signature: fn slice<T: type, N, M, S>(xs: T[N], start: uN[M], want: T[S]) -> T[S] rev rev is used to reverse the bits in an unsigned bits value. The LSb in the input becomes the MSb in the result, the 2nd LSb becomes the 2nd MSb in the result, and so on. // (Dummy) wrapper around reverse. fn wrapper < N : u32 > ( x : bits [ N ]) -> bits [ N ] { rev ( x ) } // Target for IR conversion that works on u3s. fn main ( x : u3 ) -> u3 { wrapper ( x ) } // Reverse examples. #[test] fn test_reverse () { let _ = assert_eq ( u3 : 0b100 , main ( u3 : 0b001 )); let _ = assert_eq ( u3 : 0b001 , main ( u3 : 0b100 )); let _ = assert_eq ( bits [ 0 ] : 0 , rev ( bits [ 0 ] : 0 )); let _ = assert_eq ( u1 : 1 , rev ( u1 : 1 )); let _ = assert_eq ( u2 : 0b10 , rev ( u2 : 0b01 )); let _ = assert_eq ( u2 : 0b00 , rev ( u2 : 0b00 )); () } bit_slice_update bit_slice_update(subject, start, value) returns a copy of the bits-typed value subject where the contiguous bits starting at index start (where 0 is the least-significant bit) are replaced with value . The bit-width of the returned value is the same as the bit-width of subject . Any updated bit indices which are out of bounds (if start + bit-width(value) >= bit-width(subject) ) are ignored. Example usage: dslx/tests/bit_slice_update.x . Bitwise reduction builtins: and_reduce, or_reduce, xor_reduce These are unary reduction operations applied to a bits-typed value: and_reduce : evaluates to bool:1 if all bits of the input are set, and 0 otherwise. or_reduce : evaluates to bool:1 if any bit of the input is set, and 0 otherwise. xor_reduce : evaluates to bool:1 if there is an odd number of bits set in the input, and 0 otherwise. These functions return the identity element of the respective operation for trivial (0 bit wide) inputs: #[test] fn test_trivial_reduce () { let _ = assert_eq ( and_reduce ( bits [ 0 ] : 0 ), true ); let _ = assert_eq ( or_reduce ( bits [ 0 ] : 0 ), false ); let _ = assert_eq ( xor_reduce ( bits [ 0 ] : 0 ), false ); () } update update(array, index, new_value) returns a copy of array where array[index] has been replaced with new_value , and all other elements are unchanged. Note that this is not an in-place update of the array, it is an \"evolution\" of array . It is the compiler's responsibility to optimize by using mutation instead of copying, when it's safe to do. The compiler makes a best effort to do this, but can't guarantee the optimization is always made. assert_eq, assert_lt In a unit test pseudo function all valid DSLX code is allowed. To evaluate test results DSLX provides the assert_eq primitive (we'll add more of those in the future). Here is an example of a divceil implementation with its corresponding tests: fn divceil ( x : u32 , y : u32 ) -> u32 { ( x - u32 : 1 ) / y + u32 : 1 } #[test] fn test_divceil () { let _ = assert_eq ( u32 : 3 , divceil ( u32 : 5 , u32 : 2 )); let _ = assert_eq ( u32 : 2 , divceil ( u32 : 4 , u32 : 2 )); let _ = assert_eq ( u32 : 2 , divceil ( u32 : 3 , u32 : 2 )); let _ = assert_eq ( u32 : 1 , divceil ( u32 : 2 , u32 : 2 )); _ } Note that in this example, the final let _ = ... in _ construct could be omitted. assert_eq cannot be synthesized into equivalent Verilog. Because of that it is recommended to use it within test constructs (interpretation) only. trace_fmt! DSLX supports printf-style debugging via the trace_fmt! builtin, which allows dumping of current values to stdout. For example: // Note: to see `trace_fmt!` output you need to be seeing `INFO` level logging, // enabled by adding the '--alsologtostderr' flag to the command line (among // other means). For example: // bazel run -c opt //xls/dslx:interpreter_main /path/to/dslx/file.x -- --alsologtostderr fn shifty ( x : u8 , y : u3 ) -> u8 { let _ = trace_fmt ! ( \"x: {:x} y: {}\" , x , y ); // Note: y looks different as a negative number when the high bit is set. let _ = trace_fmt ! ( \"y as s8: {}\" , y as s2 ); x << y } #[test] fn test_shifty () { let _ = assert_eq ( shifty ( u8 : 0x42 , u3 : 4 ), u8 : 0x20 ); let _ = assert_eq ( shifty ( u8 : 0x42 , u3 : 7 ), u8 : 0 ); () } would produce the following output, with each trace being annotated with its corresponding source position: [...] [ RUN UNITTEST ] test_shifty I0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] x: 42 y: 4 I0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] y as s8: 4 I0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] x: 42 y: 7 I0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] y as s8: -1 [ OK ] [...] Note: trace! currently exists as a builtin but is in the process of being removed, as it provided the user with only a \"global flag\" way of specifying the desired format for output values -- trace_fmt! is more powerful. fail! NOTE: this section describes work-in-progress functionality, currently fail! will only trigger in DSL interpretation (it is discarded in IR conversion). Support for converting fail! to XLS assert IR is tracked in #232 -- support for indicating the assertion was triggered in the JIT is tracked in #308 The fail! builtin indicates dataflow that should not be occurring in practice. Its general signature is: fail!(label, fallback_value) The fail! builtin can be thought of as a \"fatal assertion macro\". It is used to annotate dataflow that should not occur in practice and, if triggered, should raise a fatal error in simulation (e.g. via a JIT-execution failure status or a Verilog assertion when running in RTL simulation). Note, however, that XLS will permit users to avoid inserting fatal-error-signaling hardware that correspond to this fail! -- assuming it will not be triggered in practice minimizes its cost in synthesized form. In this situation, when it is \"erased\", it acts as the identity function , propagating the fallback_value . This allows XLS to keep well defined semantics even when fatal assertion hardware is not present. Example: if only these two enum values shown should be possible (say, as a documented precondition for main ): enum EnumType : u2 { FIRST = 0 , SECOND = 1 , } fn main ( x : EnumType ) -> u32 { match x { EnumType :: FIRST => u32 : 0 , EnumType :: SECOND => u32 : 1 , _ => fail ! ( \"unknown_EnumType\" , u32 : 0 ), } } The fail!(\"unknown_EnumType\", u32:0) above indicates that a) that match arm should not be reached (and if it is in the JIT or RTL simulation it will cause an error status or assertion failure respectively), but b) provides a fallback value to use (of the appropriate type) in case it were to happen in synthesized gates which did not insert fatal-error-indicating hardware. The associated label (the first argument) must be a valid Verilog identifier and is used for identifying the failure when lowered to SystemVerilog. At higher levels in the stack, it's unused. cover! NOTE: Currently, cover! has no effect in RTL simulators supported in XLS open source (i.e. iverilog). See google/xls#436 . The cover! builtin tracks how often some condition is satisfied. It desugars into SystemVerilog cover points. Its signature is: cover!(<name>, <condition>); Where name is a function-unique literal string identifying the coverpoint and condition is a boolean element. When condition is true, a counter with the given name is incremented that can be inspected upon program termination. Coverpoints can be used to give an indication of code \"coverage\", i.e. to see what paths of a design are exercised in practice. The name of the coverpoint must begin with either a letter or underscore, and its remainder must consist of letters, digits, underscores, or dollar signs. gate! The gate! builtin is used for operand gating, of the form: let gated_value = gate!(<pass_value>, <value>); This will generally use a special Verilog macro to avoid the underlying synthesis tool doing boolean optimization, and will turn gated_value to 0 when the predicate pass_value is false . This can be used in attempts to manually avoid toggles based on the gating predicate. It is expected that XLS will grow facilities to inserting gating ops automatically, but manual user insertion is a practical step in this direction. Additionally, it is expected that if, in the resulting Verilog, gating occurs on a value that originates from a flip flop, the operand gating may be promoted to register-based load-enable gating. Testing and Debugging DSLX allows specifying tests right in the implementation file via the test and quickcheck directives. Having key test code in the implementation file serves two purposes. It helps to ensure the code behaves as expected. Additionally it serves as 'executable' documentation, similar in spirit to Python doc strings. Unit Tests Unit tests are specified by the test directive, as seen below: #[test] fn test_reverse () { let _ = assert_eq ( u1 : 1 , rev ( u1 : 1 )); let _ = assert_eq ( u2 : 0b10 , rev ( u2 : 0b01 )); let _ = assert_eq ( u2 : 0b00 , rev ( u2 : 0b00 )); () } The DSLX interpreter will execute all functions that are proceeded by a test directive. These functions should be non-parametric, take no arguments, and should return a unit-type. Unless otherwise specified in the implementation's build configs, functions called by unit tests are also converted to XLS IR and run through the toolchain's LLVM JIT. The resulting values from the DSLX interpreter and the LLVM JIT are compared against each other to assert equality. This is to ensure DSLX implementations are IR-convertable and that IR translation is correct. QuickCheck QuickCheck is a testing framework concept founded on property-based testing. Instead of specifying expected and test values, QuickCheck asks for properties of the implementation that should hold true against any input of the specified type(s). In DSLX, we use the quickcheck directive to designate functions to be run via the toolchain's QuickCheck framework. Here is an example that complements the unit testing of DSLX's rev implementation from above: // Reversing a value twice gets you the original value. #[quickcheck] fn prop_double_reverse ( x : u32 ) -> bool { x == rev ( rev ( x )) } The DSLX interpreter will also execute all functions that are proceeded by a quickcheck directive. These functions should be non-parametric and return a bool . The framework will provide randomized input based on the types of the arguments to the function (e.g. above, the framework will provided randomized u32 's as x ). By default, the framework will run the function against 1000 sets of randomized inputs. This default may be changed by specifying the test_count key in the quickcheck directive before a particular test: #[quickcheck(test_count=50000)] The framework also allows programmers to specify a seed to use in generating the random inputs, as opposed to letting the framework pick one. The seed chosen for production can be found in the execution log. For determinism, the DSLX interpreter should be run with the seed flag: ./interpreter_main --seed=1234 <DSLX source file> Otherwise there'd be a use-before-definition error. \u21a9","title":"Reference"},{"location":"dslx_reference/#dslx-reference","text":"","title":"DSLX Reference"},{"location":"dslx_reference/#overview","text":"DSLX is a domain specific, dataflow-oriented functional language used to build hardware that can also run effectively as host software. Within the XLS project, DSLX is also referred to as \"the DSL\". The DSL targets the XLS compiler (via conversion to XLS IR) to enable flows for FPGAs and ASICs. DSLX mimics Rust, while being an immutable expression-based dataflow DSL with hardware-oriented features; e.g. arbitrary bitwidths, entirely fixed size objects, fully analyzeable call graph, etc. To avoid arbitrary new syntax/semantics choices, the DSL mimics Rust where it is reasonably possible; for example, integer conversions all follow the same semantics as Rust. Note: There are some unnecessary differences today from Rust syntax due to early experimentation, but they are quickly being removed to converge on Rust syntax. Note that other frontends to XLS core functionality will become available in the future; e.g. xlscc , for users familiar with the C++-and-pragma style of HLS computation. XLS team develops the DSL as part of the XLS project because we believe it can offer significant advantages over the C++-with-pragmas approach. Dataflow DSLs are a good fit for describing hardware, compared to languages whose design assumes von Neumann style computation (global mutable state, sequential mutation by a sequential thread of control). Using a Domain Specific Language (DSL) provides a more hardware-oriented representation of a given computation that matches XLS compiler (IR) constructs closely. The DSL also allows an exploration of HLS without being encumbered by C++ language or compiler limitations such as non-portable pragmas, magic macros, or semantically important syntactic conventions. The language is still experimental and likely to change, but it is already useful for experimentation and exploration. This document provides a reference for DSLX, mostly by example. Before perusing it in detail, we recommend you first read the DSLX tutorials to understand the broad strokes of the language. In this document we use the function to compute a CRC32 checksum to describe language features. The full code is in examples/dslx_intro/crc32_one_byte.x .","title":"Overview"},{"location":"dslx_reference/#comments","text":"Just as in languages like Rust/C++, comments start with // and last through the end of the line.","title":"Comments"},{"location":"dslx_reference/#identifiers","text":"All identifiers, eg., for function names, parameters, and values, follow the typical naming rules of other languages. The identifiers can start with a character or an underscore, and can then contain more characters, underscores, or numbers. Valid examples are: a // valid CamelCase // valid like_under_scores // valid __also_ok // valid _Ok123_321 // valid _ // valid 2ab // not valid &ade // not valid However, we suggest the following DSLX style rules , which mirror the Rust naming conventions . Functions are written_like_this User-defined data types are NamesLikeThis Constant bindings are NAMES_LIKE_THIS _ is the \"black hole\" identifier -- a name that you can bind to but should never read from, akin to Rust's wildcard pattern match or Python's \"unused identifier\" convention. It should never be referred to in an expression except as a \"sink\". NOTE Since mutable locals are not supported, there is also support for \"tick identifiers\" , where a ' character may appear anywhere after the first character of an identifier to indicate \"prime\"; e.g. let state' = update(state); . By convention ticks usually come at the end of an identifier. Since this is not part of Rust's syntax, it is considered experimental at this time.","title":"Identifiers"},{"location":"dslx_reference/#functions","text":"Function definitions begin with the keyword fn , followed by the function name, a parameter list to the function in parenthesis, followed by an -> and the return type of the function. After this, curly braces denote the begin and end of the function body. The list of parameters can be empty. A single input file can contain many functions. Simple examples: fn ret3 () -> u32 { u32 : 3 // This function always returns 3. } fn add1 ( x : u32 ) -> u32 { x + u32 : 1 // Returns x + 1, but you knew that! } Functions return the result of their last computed expression as their return value. There are no explicit return statements. By implication, functions return exactly one expression; they can't return multiple expressions (but this may change in the future as we migrate towards some Rust semantics). Tuples should be returned if a function needs to return multiple values.","title":"Functions"},{"location":"dslx_reference/#parameters","text":"Parameters are written as pairs name followed by a colon : followed by the type of that parameter. Each parameter needs to declare its own type. Examples: // a simple parameter x of type u32 x: u32 // t is a tuple with 2 elements. // the 1st element is of type u32 // the 2nd element is a tuple with 3 elements // the 1st element is of type u8 // the 2nd element is another tuple with 1 element of type u16 // the 3rd element is of type u8 t: (u32, (u8, (u16,), u8))","title":"Parameters"},{"location":"dslx_reference/#parametric-functions","text":"DSLX functions can be parameterized in terms of the types of its arguments and in terms of types derived from other parametric values. For instance: fn double ( n : u32 ) -> u32 { n * u32 : 2 } fn self_append < A : u32 , B : u32 = double ( A ) > ( x : bits [ A ]) -> bits [ B ] { x ++ x } fn main () -> bits [ 10 ] { self_append ( u5 : 1 ) } In self_append(bits[5]:1) , we see that A = 5 based off of formal argument instantiation. Using that value, we can evaluate B = double(A=5) . This derived expression is analogous to C++'s constexpr \u2013 a simple expression that can be evaluated at that point in compilation. See advanced understanding for more information on parametricity.","title":"Parametric Functions"},{"location":"dslx_reference/#explicit-parametric-instantiation","text":"In some cases, parametric values cannot be inferred from function arguments, such as in the explicit_parametric_simple.x test: fn add_one<E:u32, F:u32, G:u32 = E+F>(lhs: bits[E]) -> bits[G] { ... } For this call to instantiable, both E and F must be specified. Since F can't be inferred from an argument, we must rely on explicit parametrics : add_one < u32 : 1 , { u32 : 2 + u32 : 3 } > ( u1 : 1 ); This invocation will bind 1 to E , 5 to F , and 6 to G . Note the curly braces around the expression-defined parametric: simple literals and constant references do not need braces (but they can have them), but any other expression requires them.","title":"Explicit parametric instantiation"},{"location":"dslx_reference/#expression-ambiguity","text":"Without curly braces, explicit parametric expressions could be ambiguous; consider the following, slightly changed from the previous example: add_one < u32 : 1 , u32 : 2 > ( u32 : 3 ) > ( u1 : 1 ); Is the statement above computing add_one<1, (2 > 3)>(1) , or is it computing (add_one<1, 2>(3)) > 1) ? Without additional (and subtle and perhaps surprising) contextual precedence rules, this would be ambiguous and could lead to a parse error or, even worse, unexpected behavior. Fortunately, we can look to Rust for inspiration. Rust's const generics RPF introduced the { } syntax for disambiguating just this case in generic specifications. With this, any expressions present in a parametric specification must be contained within curly braces, as in the original example. At present, if the braces are omitted, some unpredictable error will occur. Work to improve this is tracked in XLS GitHub issue #321 .","title":"Expression ambiguity"},{"location":"dslx_reference/#function-calls","text":"Function calls are expressions and look and feel just like one would expect from other languages. For example: fn callee ( x : bits [ 32 ], y : bits [ 32 ]) -> bits [ 32 ] { x + y } fn caller () -> u32 { callee ( u32 : 2 , u32 : 3 ) } If more than one value should be returned by a function, a tuple type should be returned.","title":"Function Calls"},{"location":"dslx_reference/#types","text":"","title":"Types"},{"location":"dslx_reference/#bit-type","text":"The most fundamental type in DSLX is a variable length bit type denoted as bits[n] , where n is a constant. For example: bits[0] // possible, but, don't do that bits[1] // a single bit uN[1] // explicitly noting single bit is unsigned u1 // convenient shorthand for bits[1] bits[8] // an 8-bit datatype, yes, a byte u8 // convenient shorthand for bits[8] bits[32] // a 32-bit datatype u32 // convenient shorthand for bits[32] bits[256] // a 256-bit datatype DSLX introduces aliases for commonly used types, such as u8 for an 8-wide bit type, or u32 for a 32-bit wide bit type. These are defined up to u64 . All u* , uN[*] , and bits[*] types are interpreted as unsigned integers. Signed integers are specified via s* and sN[*] . Similarly to unsigned numbers, the s* shorthands are defined up to s64 . For example: sN[0] s0 sN[1] s1 sN[64] s64 sN[256] Signed numbers differ in their behavior from unsigned numbers primarily via operations like comparisons, (variable width) multiplications, and divisions.","title":"Bit Type"},{"location":"dslx_reference/#enum-types","text":"DSLX supports enumerations as a way of defining a group of related, scoped, named constants that do not pollute the module namespace. For example: enum Opcode : u3 { FIRE_THE_MISSILES = 0 , BE_TIRED = 1 , TAKE_A_NAP = 2 , } fn get_my_favorite_opcode () -> Opcode { Opcode :: FIRE_THE_MISSILES } Note the use of the double-colon to reference the enum value. This code specifies that the enum behaves like a u3 : its storage and extension (via casting) behavior are defined to be those of a u3 . Attempts to define an enum value outside of the representable u3 range will produce a compile time error. enum Opcode : u3 { FOO = 8 // Causes compile time error! } Enums can be compared for equality/inequality, but they do not permit arithmetic operations, they must be cast to numerical types in order to perform arithmetic: enum Opcode : u3 { NOP = 0 , ADD = 1 , SUB = 2 , MUL = 3 , } fn same_opcode ( x : Opcode , y : Opcode ) -> bool { x == y // ok } fn next_in_sequence ( x : Opcode , y : Opcode ) -> bool { // x+1 == y // does not work, arithmetic! x as u3 + u3 : 1 == ( y as u3 ) // ok, casted first } As mentioned above, casting of enum-values works with the same casting/extension rules that apply to the underlying enum type definition. For example, this cast will sign extend because the source type for the enum is signed. (See numerical conversions for the full description of extension/truncation behavior.) enum MySignedEnum : s3 { LOW = - 1 , ZERO = 0 , HIGH = 1 , } fn extend_to_32b ( x : MySignedEnum ) -> u32 { x as u32 // Sign-extends because the source type is signed. } #[test] fn test_extend_to_32b () { assert_eq ( extend_to_32b ( MySignedEnum :: LOW ), u32 : 0xffffffff ) } Casting to an enum is also permitted. However, in most cases errors from invalid casting can only be found at runtime, e.g., in the DSL interpreter or flagging a fatal error from hardware. Because of that, it is recommended to avoid such casts as much as possible.","title":"Enum Types"},{"location":"dslx_reference/#tuple-type","text":"A tuple is a fixed-size ordered set, containing elements of heterogeneous types. Tuples elements can be any type, e.g. bits, arrays, structs, tuples. Tuples may be empty (an empty tuple is also known as the unit type), or contain one or more types. Examples of tuple values: // The unit type, carries no information. let unit = (); // A tuple containing two bits-typed elements. let pair = ( u3 : 0b100 , u4 : 0b1101 ); Example of a tuple type: // The type of a tuple with 2 elements. // the 1st element is of type u32 // the 2nd element is a tuple with 3 elements // the 1st element is of type u8 // the 2nd element is another tuple with 1 element of type u16 // the 3rd element is of type u8 type MyTuple = ( u32 , ( u8 , ( u16 ,), u8 )); To access individual tuple elements use simple indices, starting at 0. For example, to access the second element of a tuple (index 1): #[test] fn test_tuple_access () { let t = ( u32 : 2 , u8 : 3 ); assert_eq ( u8 : 3 , t . 1 ) } Such indices can only be numeric literals; parametric symbols are not allowed. Tuples can be \"destructured\", similarly to how pattern matching works in match expressions, which provides a convenient syntax to name elements of a tuple for subsequent use. See a and b in the following: #[test] fn test_tuple_destructure () { let t = ( u32 : 2 , u8 : 3 ); let ( a , b ) = t ; let _ = assert_eq ( u32 : 2 , a ); assert_eq ( u8 : 3 , b ) } Just as values can be discarded in a let by using the \"black hole identifier\" _ , don't-care values can also be discarded when destructuring a tuple: #[test] fn test_black_hole () { let t = ( u32 : 2 , u8 : 3 , true ); let ( _ , _ , v ) = t ; assert_eq ( v , true ) }","title":"Tuple Type"},{"location":"dslx_reference/#struct-types","text":"Structures are similar to tuples, but provide two additional capabilities: we name the slots (i.e. struct fields have names while tuple elements only have positions), and we introduce a new type. The following syntax is used to define a struct: struct Point { x : u32 , y : u32 } Once a struct is defined it can be constructed by naming the fields in any order: struct Point { x : u32 , y : u32 , } #[test] fn test_struct_equality () { let p0 = Point { x : u32 : 42 , y : u32 : 64 }; let p1 = Point { y : u32 : 64 , x : u32 : 42 }; assert_eq ( p0 , p1 ) } There is a simple syntax when creating a struct whose field names match the names of in-scope values: struct Point { x : u32 , y : u32 , } #[test] fn test_struct_equality () { let x = u32 : 42 ; let y = u32 : 64 ; let p0 = Point { x , y }; let p1 = Point { y , x }; assert_eq ( p0 , p1 ) } Struct fields can also be accessed with \"dot\" syntax: struct Point { x : u32 , y : u32 , } fn f ( p : Point ) -> u32 { p . x + p . y } fn main () -> u32 { f ( Point { x : u32 : 42 , y : u32 : 64 }) } #[test] fn test_main () { assert_eq ( u32 : 106 , main ()) } Note that structs cannot be mutated \"in place\", the user must construct new values by extracting the fields of the original struct mixed together with new field values, as in the following: struct Point3 { x : u32 , y : u32 , z : u32 , } fn update_y ( p : Point3 , new_y : u32 ) -> Point3 { Point3 { x : p . x , y : new_y , z : p . z } } fn main () -> Point3 { let p = Point3 { x : u32 : 42 , y : u32 : 64 , z : u32 : 256 }; update_y ( p , u32 : 128 ) } #[test] fn test_main () { let want = Point3 { x : u32 : 42 , y : u32 : 128 , z : u32 : 256 }; assert_eq ( want , main ()) }","title":"Struct Types"},{"location":"dslx_reference/#struct-update-syntax","text":"The DSL has syntax for conveniently producing a new value with a subset of fields updated to reduce verbosity. The \"struct update\" syntax is: struct Point3 { x : u32 , y : u32 , z : u32 , } fn update_y ( p : Point3 ) -> Point3 { Point3 { y : u32 : 42 , .. p } } fn update_x_and_y ( p : Point3 ) -> Point3 { Point3 { x : u32 : 42 , y : u32 : 42 , .. p } }","title":"Struct Update Syntax"},{"location":"dslx_reference/#parametric-structs","text":"DSLX also supports parametric structs. For more information on how type-parametricity works, see the parametric functions section. fn double ( n : u32 ) -> u32 { n * u32 : 2 } struct Point < N : u32 , M : u32 = double ( N ) > { x : bits [ N ], y : bits [ M ], } fn make_point < A : u32 , B : u32 > ( x : bits [ A ], y : bits [ B ]) -> Point < A , B > { Point { x , y } } #[test] fn test_struct_construction () { let p = make_point ( u16 : 42 , u32 : 42 ); assert_eq ( u16 : 42 , p . x ) }","title":"Parametric Structs"},{"location":"dslx_reference/#understanding-nominal-typing","text":"As mentioned above, a struct definition introduces a new type. Structs are nominally typed, as opposed to structurally typed (note that tuples are structurally typed). This means that structs with different names have different types, regardless of whether those structs have the same structure (i.e. even when all the fields of two structures are identical, those structures are a different type when they have a different name). struct Point { x : u32 , y : u32 , } struct Coordinate { x : u32 , y : u32 , } fn f ( p : Point ) -> u32 { p . x + p . y } #[test] fn test_ok () { assert_eq ( f ( Point { x : u32 : 42 , y : u32 : 64 }), u32 : 106 ) } #[test] fn test_type_checker_error () { assert_eq ( f ( Coordinate { x : u32 : 42 , y : u32 : 64 }), u32 : 106 ) }","title":"Understanding Nominal Typing"},{"location":"dslx_reference/#array-type","text":"Arrays can be constructed via bracket notation. All values that make up the array must have the same type. Arrays can be indexed with indexing notation ( a[i] ) to retrieve a single element. fn main ( a : u32 [ 2 ], i : u1 ) -> u32 { a [ i ] } #[test] fn test_main () { let x = u32 : 42 ; let y = u32 : 64 ; // Make an array with \"bracket notation\". let my_array : u32 [ 2 ] = [ x , y ]; let _ = assert_eq ( main ( my_array , u1 : 0 ), x ); let _ = assert_eq ( main ( my_array , u1 : 1 ), y ); () } Because arrays with repeated trailing elements are common, the DSL supports ellipsis ( ... ) at the end of an array to fill the remainder of the array with the last noted element. Because the compiler must know how many elements to fill, in order to use the ellipsis the type must be annotated explicitly as shown. fn make_array ( x : u32 ) -> u32 [ 3 ] { u32 [ 3 ] : [ u32 : 42 , x , .. .] } #[test] fn test_make_array () { let _ = assert_eq ( u32 [ 3 ] : [ u32 : 42 , u32 : 42 , u32 : 42 ], make_array ( u32 : 42 )); let _ = assert_eq ( u32 [ 3 ] : [ u32 : 42 , u32 : 64 , u32 : 64 ], make_array ( u32 : 64 )); () } TODO(meheff): Explain arrays and the intricacies of our bits type interpretation and how it affects arrays of bits etc.","title":"Array Type"},{"location":"dslx_reference/#character-string-constants","text":"Character strings are a special case of array types, being implicitly-sized arrays of u8 elements. String constants can be used just as traditional arrays: fn add_one < N : u32 > ( input : u8 [ N ]) -> u8 [ N ] { for ( i , result ) : ( u32 , u8 [ N ]) in u32 : 0 .. N { update ( result , i , result [ i ] + u8 : 1 ) }( input ) } #[test] fn test_main () { assert_eq ( \"bcdef\" , add_one ( \"abcde\" )) } DSLX string constants support the full Rust set of escape sequences - for multi-byte sequences, i.e., Unicode escapes, the resulting byte sequence will be in printed order. In other words, the sequence \\u{89ab} will result in an array with the (binary) values 1000 1001 1010 1011 in sequence.","title":"Character String Constants"},{"location":"dslx_reference/#type-aliases","text":"DLSX supports the definition of type aliases. Type aliases can be used to provide a more human-readable name for an existing type. The new name is on the left, the existing name on the right: type Weight = u6 ; We can create an alias for an imported type: // Note: this imports an external file in the codebase. import xls . dslx . tests . mod_imported type MyEnum = mod_imported :: MyEnum ; fn main ( x : u8 ) -> MyEnum { x as MyEnum } #[test] fn test_main () { let _ = assert_eq ( main ( u8 : 42 ), MyEnum :: FOO ); let _ = assert_eq ( main ( u8 : 64 ), MyEnum :: BAR ); () } Type aliases can also provide a descriptive name for a tuple type (which is otherwise anonymous). For example, to define a tuple type that represents a float number with a sign bit, an 8-bit mantissa, and a 23-bit mantissa, one would write: type F32 = ( u1 , u8 , u23 ); After this definition, the F32 may be used as a type annotation interchangeably with (u1, u8, u23) . Note, however, that structs are generally preferred for this purpose, as they are more readable and users do not need to rely on tuple elements having a stable order in the future (i.e., they are resilient to refactoring).","title":"Type Aliases"},{"location":"dslx_reference/#type-casting","text":"Bit types can be cast from one bit-width to another with the as keyword. Types can be widened (increasing bit-width), narrowed (decreasing bit-width) and/or changed between signed and unsigned. Some examples are found below. See Numerical Conversions for a description of the semantics. #[test] fn test_narrow_cast () { let twelve = u4 : 0b1100 ; assert_eq ( twelve as u2 , u2 : 0 ) } #[test] fn test_widen_cast () { let three = u2 : 0b11 ; assert_eq ( three as u4 , u4 : 3 ) } #[test] fn test_narrow_signed_cast () { let negative_seven = s4 : 0b1001 ; assert_eq ( negative_seven as u2 , u2 : 1 ) } #[test] fn test_widen_signed_cast () { let negative_one = s2 : 0b11 ; assert_eq ( negative_one as s4 , s4 : - 1 ) } #[test] fn test_widen_to_unsigned () { let negative_one = s2 : 0b11 ; assert_eq ( negative_one as u3 , u3 : 0b111 ) } #[test] fn test_widen_to_signed () { let three = u2 : 0b11 ; assert_eq ( three as u3 , u3 : 0b011 ) }","title":"Type Casting"},{"location":"dslx_reference/#type-checking-and-inference","text":"DSLX performs type checking and produces an error if types in an expression don't match up. let expressions also perform type inference, which is quite convenient. For example, instead of writing: let ch : u32 = ( e & f ) ^ (( ! e ) & g ); let ( h , g , f ) : ( u32 , u32 , u32 ) = ( g , f , e ); one can write the following, as long as the types can be properly inferred: let ch = ( e & f ) ^ (( ! e ) & g ); let ( h , g , f ) = ( g , f , e ); Note that type annotations can still be added and be used for program understanding, as they they will be checked by DSLX.","title":"Type Checking and Inference"},{"location":"dslx_reference/#type-inference-details","text":"","title":"Type Inference Details"},{"location":"dslx_reference/#type-inference-background","text":"All expressions in the language's expression grammar have a deductive type inference rule. The types must be known for inputs to an operator/function 1 and every expression has a way to determine its type from its operand expressions. DSLX uses deductive type inference to check the types present in the program. Deductive type inference is a set of (typically straight-forward) deduction rules: Hindley-Milner style deductive type inference determines the result type of a function with a rule that only observes the input types to that function. (Note that operators like '+' are just slightly special functions in that they have pre-defined special-syntax-rule names.)","title":"Type Inference Background"},{"location":"dslx_reference/#bindings-and-environment","text":"In DSLX code, the \"environment\" where names are bound (sometimes also referred to as a symbol table) is called the Bindings -- it maps identifiers to the AST node that defines the name ( {string: AstNode} ), which can be combined with a mapping from AST node to its deduced type ( {AstNode: ConcreteType} ) to resolve the type of an identifier in the program. Let is one of the key nodes that populates these Bindings , but anything that creates a bound name does as well (e.g. parameters, for loop induction variables, etc.).","title":"Bindings and Environment"},{"location":"dslx_reference/#operator-example","text":"For example, consider the binary (meaning takes two operands) / infix (meaning it syntactically is placed in the center of its operands) '+' operator. The simple deductive type inference rule for '+' is: (T, T) -> T Meaning that the left hand side operand to the '+' operator is of some type (call it T), the right hand side operand to the '+' operator must be of that same type, T, and the result of that operator is then (deduced) to be of the same type as its operands, T. Let's instantiate this rule in a function: fn add_wrapper ( x : bits [ 2 ], y : bits [ 2 ]) -> bits [ 2 ] { x + y } This function wraps the '+' operator. It presents two arguments to the '+' operator and then checks that the annotated return type on add_wrapper matches the deduced type for the body of that function; that is, we ask the following question of the '+' operator (since the type of the operands must be known at the point the add is performed): (bits[2], bits[2]) -> ? To resolve the '?' the following procedure is being used: Pattern match the rule given above (T, T) -> T to determine the type T: the left hand side operand is bits[2] , called T. Check that the right hand side is also that same T, which it is: another bits[2] . Deduce that the result type is that same type T: bits[2] . That becomes the return type of the body of the function. Check that it is the same type as the annotated return type for the function, and it is! The function is annotated to return bits[2] , and the deduced type of the body is also bits[2] . Qed.","title":"Operator Example"},{"location":"dslx_reference/#type-errors","text":"A type error would occur in the following: fn add_wrapper ( x : bits [ 2 ], y : bits [ 3 ]) -> bits [ 2 ] { x + y } Applying the type deduction rule for '+' finds an inconsistency. The left hand side operand has type bits[2] , called T, but the right hand side is bits[3] , which is not the same as T. Because the deductive type inference rule does not say what to do when the operand types are different, it results in a type error which is flagged at this point in the program.","title":"Type errors"},{"location":"dslx_reference/#let-bindings-names-and-the-environment","text":"In the DSL, let is an expression. It may not seem obvious at a glance, but it is! As a primer see the type inference background and how names are resolved in an environment . \"let\" expressions are of the (Rust-inspired) form: let $name: $annotated_type = $expr; $subexpr $name gets \"bound\" to a value of type $annotated_type . The let typecheck rule must both check that $expr is of type $annotated_type , as well as determine the type of $subexpr , which is the type of the overall \"let expression\". In this example, the result of the let expression is the return value -- $subexpr ( x+x ) can use the $name ( x ) which was \"bound\": fn main ( y : u32 ) -> u64 { let x : u64 = y as u64 ; x + x } If we invoke main(u32:2) we will the evaluate let expression -- it creates a binding of x to the value u64:2 , and then evaluates the expression x+x in that environment, so the result of the let expression's $subexpr is u64:4 .","title":"Let Bindings, Names, and the Environment"},{"location":"dslx_reference/#statements","text":"","title":"Statements"},{"location":"dslx_reference/#imports","text":"DSLX modules can import other modules via the import keyword. Circular imports are not permitted (the dependencies among DSLX modules must form a DAG, as in languages like Go). The import statement takes the following form (note the lack of semicolon): import path.to.my.imported_module With that statement, the module will be accessible as (the trailing identifier after the last dot) imported_module ; e.g. the program can refer to imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT . NOTE Imports are relative to the Bazel \"depot root\" -- for external use of the tools a DSLX_PATH will be exposed, akin to a PYTHONPATH , for users to indicate paths where were should attempt module discovery. NOTE Importing does not introduce any names into the current file other than the one referred to by the import statement. That is, if imported_module had a constant defined in it FOO , this is referred to via imported_module::FOO , FOO does not \"magically\" get put in the current scope. This is analogous to how wildcard imports are discouraged in other languages (e.g. from import * in Python) on account of leading to \"namespace pollution\" and needing to specify what happens when names conflict. If you want to change the name of the imported module (for reference inside of the importing file) you can use the as keyword: import path . to . my . imported_module as im Just using the above construct, imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT is not valid, only im::IMPORTED_MODULE_PUBLIC_CONSTANT . However, both statements can be used on different lines: import path . to . my . imported_module import path . to . my . imported_module as im In this case, either im::IMPORTED_MODULE_PUBLIC_CONSTANT or imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT can be used to refer to the same thing. Here is an example using the same function via two different aliases for the same module: // Note: this imports an external file in the codebase under two different // names. import xls . dslx . tests . mod_imported import xls . dslx . tests . mod_imported as mi fn main ( x : u3 ) -> u1 { mod_imported :: my_lsb ( x ) || mi :: my_lsb ( x ) } #[test] fn test_main () { assert_eq ( u1 : 0b1 , main ( u3 : 0b001 )) }","title":"Imports"},{"location":"dslx_reference/#public-module-members","text":"Module members are private by default and not accessible from any importing module. To make a member public/visible to importing modules, the pub keyword must be added as a prefix; e.g. const FOO = u32 : 42 ; // Not accessible to importing modules. pub const BAR = u32 : 64 ; // Accessible to importing modules. This applies to other things defined at module scope as well: functions, enums, type aliases, etc. import xls . dslx . tests . mod_imported import xls . dslx . tests . mod_imported as mi fn main ( x : u3 ) -> u1 { mod_imported :: my_lsb ( x ) || mi :: my_lsb ( x ) } #[test] fn test_main () { assert_eq ( u1 : 0b1 , main ( u3 : 0b001 )) }","title":"Public module members"},{"location":"dslx_reference/#const","text":"The const keyword is used to define module-level constant values. Named constants should be usable anywhere a literal value can be used: const FOO = u8 : 42 ; fn match_const ( x : u8 ) -> u8 { match x { FOO => u8 : 0 , _ => u8 : 42 , } } #[test] fn test_match_const_not_binding () { let _ = assert_eq ( u8 : 42 , match_const ( u8 : 0 )); let _ = assert_eq ( u8 : 42 , match_const ( u8 : 1 )); let _ = assert_eq ( u8 : 0 , match_const ( u8 : 42 )); () } fn h ( t : ( u8 , ( u16 , u32 ))) -> u32 { match t { ( FOO , ( x , y )) => ( x as u32 ) + y , ( _ , ( y , u32 : 42 )) => y as u32 , _ => u32 : 7 , } } #[test] fn test_match_nested () { let _ = assert_eq ( u32 : 3 , h (( u8 : 42 , ( u16 : 1 , u32 : 2 )))); let _ = assert_eq ( u32 : 1 , h (( u8 : 0 , ( u16 : 1 , u32 : 42 )))); let _ = assert_eq ( u32 : 7 , h (( u8 : 0 , ( u16 : 1 , u32 : 0 )))); () }","title":"Const"},{"location":"dslx_reference/#expressions","text":"","title":"Expressions"},{"location":"dslx_reference/#literals","text":"DSLX supports construction of literals using the syntax Type:Value . For example u16:1 is a 16-wide bit array with its least significant bit set to one. Similarly s8:12 is an 8-wide bit array with its least significant four bits set to 1100 . DSLX supports initializing using binary, hex or decimal syntax. So #[test] fn test_literal_initialization () { let _ = assert_eq ( u8 : 12 , u8 : 0b00001100 ); let _ = assert_eq ( u8 : 12 , u8 : 0x0c ); () } When constructing literals DSLX will trigger an error if the constant will not fit in a bit array of the annotated sized, so for example trying to construct the literal u8:256 will trigger an error of the form: TypeInferenceError: uN[8] Value '256' does not fit in the bitwidth of a uN[8] (8) But what about s8:128 ? This is a valid literal, even though a signed 8-bit integer cannot represent it. The following code offers a clue. #[test] fn test_signed_literal_initialization () { let _ = assert_eq ( s8 : 128 , s8 : - 128 ); let _ = assert_eq ( s8 : 128 , s8 : 0b10000000 ); () } What is happening here is that, 128 is being used as a bit pattern rather than as the number 128 to initialize the literal. It is only when the bit pattern cannot fit in the width of the iteral that an error is triggered. Note that behaviour is different from Rust, where it will trigger an error, and the fact that DSLX considers this valid may change in the future .","title":"Literals"},{"location":"dslx_reference/#unary-expressions","text":"DSLX supports three types of unary expressions: bit-wise not (the ! operator) negate (the - operator, computes the two's complement negation)","title":"Unary Expressions"},{"location":"dslx_reference/#binary-expressions","text":"DSLX supports a familiar set of binary expressions. There are two categories of binary expressions. A category where both operands to the expression must be of the same bit type (i.e., not arrays or tuples), and a category where the operands can be of arbitrary bit types (i.e. shift expressions). shift-right ( >> ) shift-left ( << ) bit-wise or ( | ) bit-wise and ( & ) add ( + ) subtract ( - ) xor ( ^ ) multiply ( * ) logical or ( || ) logical and ( && )","title":"Binary Expressions"},{"location":"dslx_reference/#shift-expressions","text":"Shift expressions include: shift-right (logical) and shift-left. These are binary operations that don't require the same type on the left and right hand side. The right hand side must be unsigned, but it does not need to be the same type or width as the left hand side, i.e. the type signature for these operations is: (xN[M], uN[N]) -> xN[M] . If the right hand side is a literal value it does not need to be type annotated. For example: fn shr_two ( x : s32 ) -> s32 { x >> 2 } Note that, as in Rust, the semantics of the shift-right ( >> ) operation depends on the signedness of the left hand side. For a signed-type left hand side, the shift-right ( >> ) operation performs a shift-right arithmetic and, for a unsigned-type left hand side, the shift-right ( >> ) operation performs a shift-right (logical).","title":"Shift Expressions"},{"location":"dslx_reference/#comparison-expressions","text":"For comparison expressions the types of both operands must match. However these operations return a result of type bits[1] , aka bool . equal ( == ) not-equal ( != ) greater-equal ( >= ) greater ( > ) less-equal ( <= ) less ( < )","title":"Comparison Expressions"},{"location":"dslx_reference/#concat-expression","text":"Bitwise concatenation is performed with the ++ operator. The value on the left hand side becomes the most significant bits, the value on the right hand side becomes the least significant bits. These may be chained together as shown below: #[test] fn test_bits_concat () { let _ = assert_eq ( u8 : 0b11000000 , u2 : 0b11 ++ u6 : 0b000000 ); let _ = assert_eq ( u8 : 0b00000111 , u2 : 0b00 ++ u6 : 0b000111 ); let _ = assert_eq ( u6 : 0b100111 , u1 : 1 ++ u2 : 0b00 ++ u3 : 0b111 ); let _ = assert_eq ( u6 : 0b001000 , u1 : 0 ++ u2 : 0b01 ++ u3 : 0b000 ); let _ = assert_eq ( u32 : 0xdeadbeef , u16 : 0xdead ++ u16 : 0xbeef ); () }","title":"Concat Expression"},{"location":"dslx_reference/#block-expressions","text":"Block expressions enable subordinate scopes to be defined, e.g.: let a = { let b = u32:1; b + u32:3 }; The value of a block expression is that of its last contained expression, or (), if a final expression is omitted: let a = { let b = u32:1; }; In the above case, a is equal to () . Since DSLX does not currently have the concept of lifetimes, and since names can be rebound (i.e., there's no concept of mutability, allowing let a = u32:0; let a = u32:1; ), blocks are primarily for readability at this time, (side from their use as the \"body\" of functions and loops).","title":"Block Expressions"},{"location":"dslx_reference/#match-expression","text":"Match expressions permit \"pattern matching\" on data, like a souped-up switch statement. It can both test for values (like a conditional guard) and bind values to identifiers for subsequent use. For example: fn f ( t : ( u8 , u32 )) -> u32 { match t { ( u8 : 42 , y ) => y , ( _ , y ) => y + u32 : 77 } } If the first member of the tuple is the value is 42 , we pass the second tuple member back as-is from the function. Otherwise, we add 77 to the value and return that. The _ symbolizes \"I don't care about this value\". Just like literal constants, pattern matching can also match via named constants; For example, consider this variation on the above: const MY_FAVORITE_NUMBER = u8 : 42 ; fn f ( t : ( u8 , u32 )) -> u32 { match t { ( MY_FAVORITE_NUMBER , y ) => y , ( _ , y ) => y + u32 : 77 } } This also works with nested tuples; for example: const MY_FAVORITE_NUMBER = u8 : 42 ; fn f ( t : ( u8 , ( u16 , u32 ))) -> u32 { match t { ( MY_FAVORITE_NUMBER , ( y , z )) => y as u32 + z , ( _ , ( y , u32 : 42 )) => y as u32 , _ => u32 : 7 } } Here we use a \"catch all\" wildcard pattern in the last match arm to ensure the match expression always matches the input somehow.","title":"Match Expression"},{"location":"dslx_reference/#redundant-patterns","text":"match will flag an error if a syntactically identical pattern is typed twice; e.g. const FOO = u32 : 42 ; fn f ( x : u32 ) -> u2 { match x { FOO => u2 : 0 , FOO => u2 : 1 , // Identical pattern! _ => u2 : 2 , } } Only the first pattern will ever match, so it is fully redundant (and therefore likely a user error they'd like to be informed of). Note that equivalent but not syntactically identical patterns will not be flagged in this way. const FOO = u32 : 42 ; const BAR = u32 : 42 ; // Compares `==` to `FOO`. fn f ( x : u32 ) -> u2 { match x { FOO => u2 : 0 , BAR => u2 : 1 , // _Equivalent_ pattern, but not syntactically identical. _ => u2 : 2 , } }","title":"Redundant Patterns"},{"location":"dslx_reference/#let-expression","text":"let expressions work the same way as let expressions in other functional languages (such as the ML family languages). let expressions provide a nested, lexically-scoped, list of binding definitions. The scope of the binding is the expression on the right hand side of the declaration. For example: let a : u32 = u32 : 1 + u32 : 2 ; let b : u32 = a + u32 : 3 ; b would bind (and return as a value) the value 6 which corresponds to b when evaluated. In effect there is little difference to other languages like C/C++ or Python, where the same result would be achieved with code similar to this: a = 1 + 2 b = a + 3 return b However, let expressions are lexically scoped. In above example, the value 3 is bound to a only during the combined let expression sequence. There is no other type of scoping in DSLX.","title":"let Expression"},{"location":"dslx_reference/#ternary-if-expression","text":"Note: ternary expression syntax is expected to change to mimic Rust's, see #318 . DSLX offers a ternary if expression, which is very similar to the Rust ternary if expression. Blueprint: if condition { consequent } else { alternate } This corresponds to the C/C++ ternary ?: operator: condition ? consequent : alternate Note: both the if and else are required to be present, as with the ?: operator, unlike a C++ if statement. This is because it is an expression that produces a result value, not a statement that causes a mutating effect. For example, in the FP adder module (modules/fp32_add_2.x), there is code like the following: [...] let result_fraction = if wide_exponent < u9:255 { result_fraction } else { u23:0 }; let result_exponent = if wide_exponent < u9:255 { wide_exponent as u8 } else { u8:255 };","title":"Ternary If Expression"},{"location":"dslx_reference/#iterable-expression","text":"Iterable expressions are used in counted for loops. DSLX currently supports two types of iterable expressions, range and enumerate . The range expression m..n represents a range of values from m to n-1. This example will run from 0 to 4 (exclusive): for (i, accum): (u32, u32) in u32:0..u32:4 { There also exists a range() builtin function that performs the same operation. enumerate iterates over the elements of an array type and produces pairs of (index, value) , similar to enumeration constructs in languages like Python or Go. In the example below, the loop will iterate 8 times, following the array dimension of x . Each iteration produces a tuple with the current index ( i ranging from 0 to 7) and the value at the index ( e = x[i] ). fn prefix_scan_eq(x: u32[8]) -> bits[8,3] { let (_, _, result) = for ((i, e), (prior, count, result)): ((u32, u32), (u32, u3, bits[8,3])) in enumerate(x) {...","title":"Iterable Expression"},{"location":"dslx_reference/#for-expression","text":"DSLX currently supports synthesis of \"counted\" for loops (loops that have a clear upper bound on their number of iterations). These loops are capable of being generated as unrolled pipeline stages: when generating a pipeline, the XLS compiler will unroll and specialize the iterations. NOTE In the future support for loops with an unbounded number of iterations may be permitted, but will only be possible to synthesize as a time-multiplexed implementation, since pipelines cannot be unrolled indefinitely.","title":"for Expression"},{"location":"dslx_reference/#blueprint","text":"for (index, accumulator): (type-of-index, type-of-accumulator) in iterable { body-expression } (initial-accumulator-value) The type annotation in the above \"blueprint\" is optional, but often helpful to include for increased clarity. Because DSLX is a pure dataflow description, a for loop is an expression that produces a value. As a result, you grab the output of a for loop just like any other expression: let final_accum = for ( i , accum ) in u32 : 0 .. u32 : 8 { let new_accum = f ( accum ); new_accum }( init_accum ); Conceptually the for loop \"evolves\" the accumulator as it iterates, and ultimately pops it out as the result of its evaluation.","title":"Blueprint"},{"location":"dslx_reference/#examples","text":"Add up all values from 0 to 4 (exclusive). Note that we pass the accumulator's initial value in as a parameter to this expression. for ( i , accum ) : ( u32 , u32 ) in u32 : 0 .. u32 : 4 { accum + i }( u32 : 0 ) To add up values from 7 to 11 (exclusive), one would write: let base = u32 : 7 ; for ( i , accum ) : ( u32 , u32 ) in u32 : 0 .. u32 : 4 { accum + base + i }( u32 : 0 ) \"Loop invariant\" values (values that do not change as the loop runs) can be used in the loop body, for example, note the use of outer_thing below: let outer_thing : u32 = u32 : 42 ; for ( i , accum ) : ( u32 , u32 ) in u32 : 0 .. u32 : 4 { accum + i + outer_thing }( u32 : 0 ) Both the index and accumulator can be of any valid type, in particular, the accumulator can be a tuple type, which is useful for evolving a bunch of values. For example, this for loop \"evolves\" two arrays: for ( i , ( xs , ys )) : ( u32 , ( u16 [ 3 ], u8 [ 3 ])) in u32 : 0 .. u32 : 4 { .. . }(( init_xs , init_ys )) Note in the above example arrays are dataflow values just like anything else. To conditionally update an array every other iteration: let result : u4 [ 8 ] = for ( i , array ) in u32 : 0 .. u32 : 8 { // Update every other cell with the square of the index. if i % 2 == 0 { update ( array , i , i * i ) } else { array } }( u4 [ 8 ] : [ 0 , .. .]);","title":"Examples"},{"location":"dslx_reference/#numerical-conversions","text":"DSLX adopts the Rust rules for semantics of numeric casts: Casting from larger bit-widths to smaller bit-widths will truncate (to the LSbs). * This means that truncating signed values does not preserve the previous value of the sign bit. Casting from a smaller bit-width to a larger bit-width will zero-extend if the source is unsigned, sign-extend if the source is signed. Casting from a bit-width to its own bit-width, between signed/unsigned, is a no-op. #[test] fn test_numerical_conversions () { let s8_m2 = s8 : - 2 ; let u8_m2 = u8 : 0xfe ; // Sign extension (source type is signed). let _ = assert_eq ( s32 : - 2 , s8_m2 as s32 ); let _ = assert_eq ( u32 : 0xfffffffe , s8_m2 as u32 ); let _ = assert_eq ( s16 : - 2 , s8_m2 as s16 ); let _ = assert_eq ( u16 : 0xfffe , s8_m2 as u16 ); // Zero extension (source type is unsigned). let _ = assert_eq ( u32 : 0xfe , u8_m2 as u32 ); let _ = assert_eq ( s32 : 0xfe , u8_m2 as s32 ); // Nop (bitwidth is unchanged). let _ = assert_eq ( s8 : - 2 , s8_m2 as s8 ); let _ = assert_eq ( s8 : - 2 , u8_m2 as s8 ); let _ = assert_eq ( u8 : 0xfe , u8_m2 as u8 ); let _ = assert_eq ( s8 : - 2 , u8_m2 as s8 ); () }","title":"Numerical Conversions"},{"location":"dslx_reference/#array-conversions","text":"Casting to an array takes bits from the MSb to the LSb; that is, the group of bits including the MSb ends up as element 0, the next group ends up as element 1, and so on. Casting from an array to bits performs the inverse operation: element 0 becomes the MSbs of the resulting value. All casts between arrays and bits must have the same total bit count. fn cast_to_array ( x : u6 ) -> u2 [ 3 ] { x as u2 [ 3 ] } fn cast_from_array ( a : u2 [ 3 ]) -> u6 { a as u6 } fn concat_arrays ( a : u2 [ 3 ], b : u2 [ 3 ]) -> u2 [ 6 ] { a ++ b } #[test] fn test_cast_to_array () { let a_value : u6 = u6 : 0b011011 ; let a : u2 [ 3 ] = cast_to_array ( a_value ); let a_array = u2 [ 3 ] : [ 1 , 2 , 3 ]; let _ = assert_eq ( a , a_array ); // Note: converting back from array to bits gives the original value. let _ = assert_eq ( a_value , cast_from_array ( a )); let b_value : u6 = u6 : 0b111001 ; let b_array : u2 [ 3 ] = u2 [ 3 ] : [ 3 , 2 , 1 ]; let b : u2 [ 3 ] = cast_to_array ( b_value ); let _ = assert_eq ( b , b_array ); let _ = assert_eq ( b_value , cast_from_array ( b )); // Concatenation of bits is analogous to concatenation of their converted // arrays. That is: // // convert(concat(a, b)) == concat(convert(a), convert(b)) let concat_value : u12 = a_value ++ b_value ; let concat_array : u2 [ 6 ] = concat_value as u2 [ 6 ]; let _ = assert_eq ( concat_array , concat_arrays ( a_array , b_array )); // Show a few classic \"endianness\" example using 8-bit array values. let x = u32 : 0xdeadbeef ; let _ = assert_eq ( x as u8 [ 4 ], u8 [ 4 ] : [ 0xde , 0xad , 0xbe , 0xef ]); let y = u16 : 0xbeef ; let _ = assert_eq ( y as u8 [ 2 ], u8 [ 2 ] : [ 0xbe , 0xef ]); () }","title":"Array Conversions"},{"location":"dslx_reference/#bit-slice-expressions","text":"DSLX supports Python-style bit slicing over unsigned bits types. Note that bits are numbered 0..N starting \"from the right (as you would write it on paper)\" -- least significant bit, AKA LSb -- for example, for the value u7:0b100_0111 : Bit 6 5 4 3 2 1 0 Value 1 0 0 0 1 1 1 A slice expression [N:M] means to get from bit N (inclusive) to bit M exclusive. The start and limit in the slice expression must be signed integral values. Aside: This can be confusing, because the N stands to the left of M in the expression, but bit N would be to the 'right' of M in the classical bit numbering. Additionally, this is not the case in the classical array visualization, where element 0 is usually drawn on the left. For example, the expression [0:2] would yield: Bit 6 5 4 3 2 1 0 Value 1 0 0 0 1 1 1 ^ ^ included ^ excluded Result: 0b11 Note that, as of now, the indices for this [N:M] form must be literal numbers (so the compiler can determine the width of the result). To perform a slice with a non-literal-number start position, see the +: form described below. The slicing operation also support the python style slices with offsets from start or end. To visualize, one can think of x[ : -1] as the equivalent of x[from the start : bitwidth - 1] . Correspondingly, x[-1 : ] can be visualized as [ bitwidth - 1 : to the end] . For example, to get all bits, except the MSb (from the beginning, until the top element minus 1): x [ : - 1 ] Or to get the two most significant bits: x [ - 2 : ] This results in the nice property that a the original complete value can be sliced into complementary slices such as :-2 (all but the two most significant bits) and -2: (the two most significant bits): #[test] fn slice_into_two_pieces () { let x = u5 : 0b11000 ; let ( lo , hi ) : ( u3 , u2 ) = ( x [ : - 2 ], x [ - 2 : ]); let _ = assert_eq ( hi , u2 : 0b11 ); let _ = assert_eq ( lo , u3 : 0b000 ); () }","title":"Bit Slice Expressions"},{"location":"dslx_reference/#width-slice","text":"There is also a \"width slice\" form x[start +: bits[N]] - starting from a specified bit, slice out the next N bits. This is equivalent to: bits[N]:(x >> start) . The type can be specified as either signed or unsigned; e.g. [start +: s8] will produce an 8-bit signed value starting at start , whereas [start +: u4] will produce a 4-bit unsigned number starting at start . Here are many more examples :","title":"Width Slice"},{"location":"dslx_reference/#bit-slice-examples","text":"// Identity function helper. fn id < N : u32 > ( x : bits [ N ]) -> bits [ N ] { x } #[test] fn test_bit_slice_syntax () { let x = u6 : 0b100111 ; // Slice out two bits. let _ = assert_eq ( u2 : 0b11 , x [ 0 : 2 ]); let _ = assert_eq ( u2 : 0b11 , x [ 1 : 3 ]); let _ = assert_eq ( u2 : 0b01 , x [ 2 : 4 ]); let _ = assert_eq ( u2 : 0b00 , x [ 3 : 5 ]); // Slice out three bits. let _ = assert_eq ( u3 : 0b111 , x [ 0 : 3 ]); let _ = assert_eq ( u3 : 0b011 , x [ 1 : 4 ]); let _ = assert_eq ( u3 : 0b001 , x [ 2 : 5 ]); let _ = assert_eq ( u3 : 0b100 , x [ 3 : 6 ]); // Slice out from the end. let _ = assert_eq ( u1 : 0b1 , x [ - 1 : ]); let _ = assert_eq ( u1 : 0b1 , x [ - 1 : 6 ]); let _ = assert_eq ( u2 : 0b10 , x [ - 2 : ]); let _ = assert_eq ( u2 : 0b10 , x [ - 2 : 6 ]); let _ = assert_eq ( u3 : 0b100 , x [ - 3 : ]); let _ = assert_eq ( u3 : 0b100 , x [ - 3 : 6 ]); let _ = assert_eq ( u4 : 0b1001 , x [ - 4 : ]); let _ = assert_eq ( u4 : 0b1001 , x [ - 4 : 6 ]); // Slice both relative to the end (MSb). let _ = assert_eq ( u2 : 0b01 , x [ - 4 : - 2 ]); let _ = assert_eq ( u2 : 0b11 , x [ - 6 : - 4 ]); // Slice out from the beginning (LSb). let _ = assert_eq ( u5 : 0b00111 , x [ : - 1 ]); let _ = assert_eq ( u4 : 0b0111 , x [ : - 2 ]); let _ = assert_eq ( u3 : 0b111 , x [ : - 3 ]); let _ = assert_eq ( u2 : 0b11 , x [ : - 4 ]); let _ = assert_eq ( u1 : 0b1 , x [ : - 5 ]); // Slicing past the end just means we hit the end (as in Python). let _ = assert_eq ( u1 : 0b1 , x [ 5 : 7 ]); let _ = assert_eq ( u1 : 0b1 , x [ - 7 : 1 ]); let _ = assert_eq ( bits [ 0 ] : 0 , x [ - 7 : - 6 ]); let _ = assert_eq ( bits [ 0 ] : 0 , x [ - 6 : - 6 ]); let _ = assert_eq ( bits [ 0 ] : 0 , x [ 6 : 6 ]); let _ = assert_eq ( bits [ 0 ] : 0 , x [ 6 : 7 ]); let _ = assert_eq ( u1 : 1 , x [ - 6 : - 5 ]); // Slice of a slice. let _ = assert_eq ( u2 : 0b11 , x [ : 4 ][ 1 : 3 ]); // Slice of an invocation. let _ = assert_eq ( u2 : 0b01 , id ( x )[ 2 : 4 ]); // Explicit-width slices. let _ = assert_eq ( u2 : 0b01 , x [ 2 + : u2 ]); let _ = assert_eq ( s3 : 0b100 , x [ 3 + : s3 ]); let _ = assert_eq ( u3 : 0b001 , x [ 5 + : u3 ]); () }","title":"Bit Slice Examples"},{"location":"dslx_reference/#advanced-understanding-parametricity-constraints-and-unification","text":"An infamous wrinkle is introduced for parametric functions: consider the following function: // (Note: DSLX does not currently support the `T: type` construct shown here, // it is for example purposes only.) fn add_wrapper < T : type , U : type > ( x : T , y : U ) -> T { x + y } Based on the inference rule, we know that '+' can only type check when the operand types are the same. This means we can conclude that type T is the same as type U . Once we determine this, we need to make sure anywhere U is used it is consistent with the fact it is the same as T . In a sense the + operator is \"adding a constraint\" that T is equivalent to U , and trying to check that fact is valid is under the purview of type inference. The fact that the constraint is added that T and U are the same type is referred to as \"unification\", as what was previously two entities with potentially different constraints now has a single set of constraints that comes from the union of its operand types. DSLX's typechecker will go through the body of parametric functions per invocation. As such, the typechecker will always have the invocation's parametric values for use in asserting type consistency against \"constraints\" such as derived parametric expressions, body vs. annotated return type equality, and expression inference rules.","title":"Advanced Understanding: Parametricity, Constraints, and Unification"},{"location":"dslx_reference/#operator-precedence","text":"DSLX's operator precedence matches Rust's. Listed below are DSLX's operators in descending precedence order. Binary operators at the same level share the same associativity and will be grouped accordingly. Operator Associativity Unary - ! n/a as Left to right * / % Left to right + - Left to right << >> >>> Left to right & Left to right ^ Left to right \\| Left to right == != < > <= >= Left to right && Left to right \\|\\| Left to right","title":"Operator Precedence"},{"location":"dslx_reference/#builtins","text":"This section describes the built-in functions provided for use in the DSL that do not need to be explicitly imported. A note on \"Parallel Primitives\": the DSL is expected to grow additional support for use of high-level parallel primitives over time, adding operators for order-insensitive reductions, scans, groupings, and similar. By making these operations known to the compiler in their high level form, we potentially enable optimizations and analyses on their higher level (\"lifted\") form. As of now, map is the sole parallel-primitive-oriented built-in.","title":"Builtins"},{"location":"dslx_reference/#add_with_carry","text":"Operation that produces the result of the add, as well as the carry bit as an output. The binary add operators works similar to software programming languages, preserving the length of the input operands, so this builtin can assist when easy access to the carry out value is desired. Has the following signature: fn add_with_carry<N>(x: uN[N], y: uN[N]) -> (u1, uN[N])","title":"add_with_carry"},{"location":"dslx_reference/#smulp-and-umulp","text":"smulp and umulp perform signed and unsigned partial multiplications. These operations return a two-element tuple with the property that the sum of the two elements is equal to the product of the original inputs. Performing a partial multiplication allows for a pipeline stage in the middle of a multiply. These operations have the following signatures: fn smulp<N>(lhs: sN[N], rhs: sN[N]) -> (sN[N], sN[N]) fn umulp<N>(lhs: uN[N], rhs: uN[N]) -> (uN[N], uN[N])","title":"smulp and umulp"},{"location":"dslx_reference/#map","text":"map , similarly to other languages, executes a transformation function on all the elements of an original array to produce the resulting \"mapped' array. For example : taking the absolute value of each element in an input array: import std fn main ( x : s3 [ 3 ]) -> s3 [ 3 ] { let y : s3 [ 3 ] = map ( x , std :: abs ); y } #[test] fn main_test () { let got : s3 [ 3 ] = main ( s3 [ 3 ] : [ - 1 , 1 , 0 ]); assert_eq ( s3 [ 3 ] : [ 1 , 1 , 0 ], got ) } Note that map is special, in that we can pass it a callee as if it were a value. As a function that \"takes\" a function as an argument, map is a special builtin -- in language implementor parlance it is a higher order function . Implementation note: Functions are not first class values in the DSL, so the name of the function must be referred to directly. Note: Novel higher order functions (e.g. if a user wanted to write their own map ) cannot currently be written in user-level DSL code.","title":"map"},{"location":"dslx_reference/#clz-ctz","text":"DSLX provides the common \"count leading zeroes\" and \"count trailing zeroes\" functions: let x0 = u32 : 0x0FFFFFF8 ; let x1 = clz ( x0 ); let x2 = ctz ( x0 ); let _ = assert_eq ( u32 : 4 , x1 ); assert_eq ( u32 : 3 , x2 )","title":"clz, ctz"},{"location":"dslx_reference/#one_hot","text":"Converts a value to one-hot form. Has the following signature: fn one_hot<N, NP1=N+1>(x: uN[N], lsb_is_prio: bool) -> uN[NP1] When lsb_is_prio is true, the least significant bit that is set becomes the one-hot bit in the result. When it is false, the most significant bit that is set becomes the one-hot bit in the result. When all bits in the input are unset, the additional bit present in the output value (MSb) becomes set. Example usage: dslx/tests/one_hot.x . See also the IR semantics for the one_hot op .","title":"one_hot"},{"location":"dslx_reference/#signex","text":"Casting has well-defined extension rules, but in some cases it is necessary to be explicit about sign-extensions, if just for code readability. For this, there is the signex builtin. To invoke the signex builtin, provide it with the operand to sign extend (lhs), as well as the target type to extend to: these operands may be either signed or unsigned. Note that the value of the right hand side is ignored, only its type is used to determine the result type of the sign extension. #[test] fn test_signex () { let x = u8 : 0xff ; let s : s32 = signex ( x , s32 : 0 ); let u : u32 = signex ( x , u32 : 0 ); assert_eq ( s as u32 , u ) } Note that both s and u contain the same bits in the above example.","title":"signex"},{"location":"dslx_reference/#slice","text":"Array-slice builtin operation. Note that the \"want\" argument is not used as a value, but is just used to reflect the desired slice type. (Prior to constexprs being passed to builtin functions, this was the canonical way to reflect a constexpr in the type system.) Has the following signature: fn slice<T: type, N, M, S>(xs: T[N], start: uN[M], want: T[S]) -> T[S]","title":"slice"},{"location":"dslx_reference/#rev","text":"rev is used to reverse the bits in an unsigned bits value. The LSb in the input becomes the MSb in the result, the 2nd LSb becomes the 2nd MSb in the result, and so on. // (Dummy) wrapper around reverse. fn wrapper < N : u32 > ( x : bits [ N ]) -> bits [ N ] { rev ( x ) } // Target for IR conversion that works on u3s. fn main ( x : u3 ) -> u3 { wrapper ( x ) } // Reverse examples. #[test] fn test_reverse () { let _ = assert_eq ( u3 : 0b100 , main ( u3 : 0b001 )); let _ = assert_eq ( u3 : 0b001 , main ( u3 : 0b100 )); let _ = assert_eq ( bits [ 0 ] : 0 , rev ( bits [ 0 ] : 0 )); let _ = assert_eq ( u1 : 1 , rev ( u1 : 1 )); let _ = assert_eq ( u2 : 0b10 , rev ( u2 : 0b01 )); let _ = assert_eq ( u2 : 0b00 , rev ( u2 : 0b00 )); () }","title":"rev"},{"location":"dslx_reference/#bit_slice_update","text":"bit_slice_update(subject, start, value) returns a copy of the bits-typed value subject where the contiguous bits starting at index start (where 0 is the least-significant bit) are replaced with value . The bit-width of the returned value is the same as the bit-width of subject . Any updated bit indices which are out of bounds (if start + bit-width(value) >= bit-width(subject) ) are ignored. Example usage: dslx/tests/bit_slice_update.x .","title":"bit_slice_update"},{"location":"dslx_reference/#bitwise-reduction-builtins-and_reduce-or_reduce-xor_reduce","text":"These are unary reduction operations applied to a bits-typed value: and_reduce : evaluates to bool:1 if all bits of the input are set, and 0 otherwise. or_reduce : evaluates to bool:1 if any bit of the input is set, and 0 otherwise. xor_reduce : evaluates to bool:1 if there is an odd number of bits set in the input, and 0 otherwise. These functions return the identity element of the respective operation for trivial (0 bit wide) inputs: #[test] fn test_trivial_reduce () { let _ = assert_eq ( and_reduce ( bits [ 0 ] : 0 ), true ); let _ = assert_eq ( or_reduce ( bits [ 0 ] : 0 ), false ); let _ = assert_eq ( xor_reduce ( bits [ 0 ] : 0 ), false ); () }","title":"Bitwise reduction builtins: and_reduce, or_reduce, xor_reduce"},{"location":"dslx_reference/#update","text":"update(array, index, new_value) returns a copy of array where array[index] has been replaced with new_value , and all other elements are unchanged. Note that this is not an in-place update of the array, it is an \"evolution\" of array . It is the compiler's responsibility to optimize by using mutation instead of copying, when it's safe to do. The compiler makes a best effort to do this, but can't guarantee the optimization is always made.","title":"update"},{"location":"dslx_reference/#assert_eq-assert_lt","text":"In a unit test pseudo function all valid DSLX code is allowed. To evaluate test results DSLX provides the assert_eq primitive (we'll add more of those in the future). Here is an example of a divceil implementation with its corresponding tests: fn divceil ( x : u32 , y : u32 ) -> u32 { ( x - u32 : 1 ) / y + u32 : 1 } #[test] fn test_divceil () { let _ = assert_eq ( u32 : 3 , divceil ( u32 : 5 , u32 : 2 )); let _ = assert_eq ( u32 : 2 , divceil ( u32 : 4 , u32 : 2 )); let _ = assert_eq ( u32 : 2 , divceil ( u32 : 3 , u32 : 2 )); let _ = assert_eq ( u32 : 1 , divceil ( u32 : 2 , u32 : 2 )); _ } Note that in this example, the final let _ = ... in _ construct could be omitted. assert_eq cannot be synthesized into equivalent Verilog. Because of that it is recommended to use it within test constructs (interpretation) only.","title":"assert_eq, assert_lt"},{"location":"dslx_reference/#trace_fmt","text":"DSLX supports printf-style debugging via the trace_fmt! builtin, which allows dumping of current values to stdout. For example: // Note: to see `trace_fmt!` output you need to be seeing `INFO` level logging, // enabled by adding the '--alsologtostderr' flag to the command line (among // other means). For example: // bazel run -c opt //xls/dslx:interpreter_main /path/to/dslx/file.x -- --alsologtostderr fn shifty ( x : u8 , y : u3 ) -> u8 { let _ = trace_fmt ! ( \"x: {:x} y: {}\" , x , y ); // Note: y looks different as a negative number when the high bit is set. let _ = trace_fmt ! ( \"y as s8: {}\" , y as s2 ); x << y } #[test] fn test_shifty () { let _ = assert_eq ( shifty ( u8 : 0x42 , u3 : 4 ), u8 : 0x20 ); let _ = assert_eq ( shifty ( u8 : 0x42 , u3 : 7 ), u8 : 0 ); () } would produce the following output, with each trace being annotated with its corresponding source position: [...] [ RUN UNITTEST ] test_shifty I0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] x: 42 y: 4 I0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] y as s8: 4 I0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] x: 42 y: 7 I0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] y as s8: -1 [ OK ] [...] Note: trace! currently exists as a builtin but is in the process of being removed, as it provided the user with only a \"global flag\" way of specifying the desired format for output values -- trace_fmt! is more powerful.","title":"trace_fmt!"},{"location":"dslx_reference/#fail","text":"NOTE: this section describes work-in-progress functionality, currently fail! will only trigger in DSL interpretation (it is discarded in IR conversion). Support for converting fail! to XLS assert IR is tracked in #232 -- support for indicating the assertion was triggered in the JIT is tracked in #308 The fail! builtin indicates dataflow that should not be occurring in practice. Its general signature is: fail!(label, fallback_value) The fail! builtin can be thought of as a \"fatal assertion macro\". It is used to annotate dataflow that should not occur in practice and, if triggered, should raise a fatal error in simulation (e.g. via a JIT-execution failure status or a Verilog assertion when running in RTL simulation). Note, however, that XLS will permit users to avoid inserting fatal-error-signaling hardware that correspond to this fail! -- assuming it will not be triggered in practice minimizes its cost in synthesized form. In this situation, when it is \"erased\", it acts as the identity function , propagating the fallback_value . This allows XLS to keep well defined semantics even when fatal assertion hardware is not present. Example: if only these two enum values shown should be possible (say, as a documented precondition for main ): enum EnumType : u2 { FIRST = 0 , SECOND = 1 , } fn main ( x : EnumType ) -> u32 { match x { EnumType :: FIRST => u32 : 0 , EnumType :: SECOND => u32 : 1 , _ => fail ! ( \"unknown_EnumType\" , u32 : 0 ), } } The fail!(\"unknown_EnumType\", u32:0) above indicates that a) that match arm should not be reached (and if it is in the JIT or RTL simulation it will cause an error status or assertion failure respectively), but b) provides a fallback value to use (of the appropriate type) in case it were to happen in synthesized gates which did not insert fatal-error-indicating hardware. The associated label (the first argument) must be a valid Verilog identifier and is used for identifying the failure when lowered to SystemVerilog. At higher levels in the stack, it's unused.","title":"fail!"},{"location":"dslx_reference/#cover","text":"NOTE: Currently, cover! has no effect in RTL simulators supported in XLS open source (i.e. iverilog). See google/xls#436 . The cover! builtin tracks how often some condition is satisfied. It desugars into SystemVerilog cover points. Its signature is: cover!(<name>, <condition>); Where name is a function-unique literal string identifying the coverpoint and condition is a boolean element. When condition is true, a counter with the given name is incremented that can be inspected upon program termination. Coverpoints can be used to give an indication of code \"coverage\", i.e. to see what paths of a design are exercised in practice. The name of the coverpoint must begin with either a letter or underscore, and its remainder must consist of letters, digits, underscores, or dollar signs.","title":"cover!"},{"location":"dslx_reference/#gate","text":"The gate! builtin is used for operand gating, of the form: let gated_value = gate!(<pass_value>, <value>); This will generally use a special Verilog macro to avoid the underlying synthesis tool doing boolean optimization, and will turn gated_value to 0 when the predicate pass_value is false . This can be used in attempts to manually avoid toggles based on the gating predicate. It is expected that XLS will grow facilities to inserting gating ops automatically, but manual user insertion is a practical step in this direction. Additionally, it is expected that if, in the resulting Verilog, gating occurs on a value that originates from a flip flop, the operand gating may be promoted to register-based load-enable gating.","title":"gate!"},{"location":"dslx_reference/#testing-and-debugging","text":"DSLX allows specifying tests right in the implementation file via the test and quickcheck directives. Having key test code in the implementation file serves two purposes. It helps to ensure the code behaves as expected. Additionally it serves as 'executable' documentation, similar in spirit to Python doc strings.","title":"Testing and Debugging"},{"location":"dslx_reference/#unit-tests","text":"Unit tests are specified by the test directive, as seen below: #[test] fn test_reverse () { let _ = assert_eq ( u1 : 1 , rev ( u1 : 1 )); let _ = assert_eq ( u2 : 0b10 , rev ( u2 : 0b01 )); let _ = assert_eq ( u2 : 0b00 , rev ( u2 : 0b00 )); () } The DSLX interpreter will execute all functions that are proceeded by a test directive. These functions should be non-parametric, take no arguments, and should return a unit-type. Unless otherwise specified in the implementation's build configs, functions called by unit tests are also converted to XLS IR and run through the toolchain's LLVM JIT. The resulting values from the DSLX interpreter and the LLVM JIT are compared against each other to assert equality. This is to ensure DSLX implementations are IR-convertable and that IR translation is correct.","title":"Unit Tests"},{"location":"dslx_reference/#quickcheck","text":"QuickCheck is a testing framework concept founded on property-based testing. Instead of specifying expected and test values, QuickCheck asks for properties of the implementation that should hold true against any input of the specified type(s). In DSLX, we use the quickcheck directive to designate functions to be run via the toolchain's QuickCheck framework. Here is an example that complements the unit testing of DSLX's rev implementation from above: // Reversing a value twice gets you the original value. #[quickcheck] fn prop_double_reverse ( x : u32 ) -> bool { x == rev ( rev ( x )) } The DSLX interpreter will also execute all functions that are proceeded by a quickcheck directive. These functions should be non-parametric and return a bool . The framework will provide randomized input based on the types of the arguments to the function (e.g. above, the framework will provided randomized u32 's as x ). By default, the framework will run the function against 1000 sets of randomized inputs. This default may be changed by specifying the test_count key in the quickcheck directive before a particular test: #[quickcheck(test_count=50000)] The framework also allows programmers to specify a seed to use in generating the random inputs, as opposed to letting the framework pick one. The seed chosen for production can be found in the execution log. For determinism, the DSLX interpreter should be run with the seed flag: ./interpreter_main --seed=1234 <DSLX source file> Otherwise there'd be a use-before-definition error. \u21a9","title":"QuickCheck"},{"location":"dslx_std/","text":"Standard Library This page documents the DSLX standard library. Standard Library std.x std::bounded_minus_1 std::abs std::is_pow2 std::?mul std::iterative_div std::div_pow2 std::mod_pow2 std::ceil_div std::round_up_to_nearest std::?pow std::clog2 std::?max std::umin Signed comparison std::find_index std::lsb std::convert_to_bits std::mask_bits std::concat3 std::rrot acm_random.x acm_random::rng_deterministic_seed acm_random::rng_new acm_random::rng_next acm_random::rng_next64 std.x std::bounded_minus_1 pub fn bounded_minus_1 < N : u32 > ( x : uN [ N ]) -> uN [ N ] Returns the value of x - 1 with saturation at 0 . std::abs pub fn abs < BITS : u32 > ( x : sN [ BITS ]) -> sN [ BITS ] Returns the absolute value of x as a signed number. std::is_pow2 pub fn is_pow2 < N : u32 > ( x : uN [ N ]) -> bool Returns true when x is a non-zero power-of-two. std::?mul pub fn umul < N : u32 , M : u32 , R : u32 = N + M > ( x : uN [ N ], y : uN [ M ]) -> uN [ R ] pub fn smul < N : u32 , M : u32 , R : u32 = N + M > ( x : sN [ N ], y : sN [ M ]) -> sN [ R ] Returns product of x ( N bits) and y ( M bits) as an N+M bit value. std::iterative_div pub fn iterative_div < N : u32 , DN : u32 = N * u32 : 2 > ( x : uN [ N ], y : uN [ N ]) -> uN [ N ] Calculate x / y one bit at a time. This is an alternative to using the division operator '/' which may not synthesize nicely. std::div_pow2 pub fn div_pow2 < N : u32 > ( x : bits [ N ], y : bits [ N ]) -> bits [ N ] Returns x / y where y must be a non-zero power-of-two. std::mod_pow2 pub fn mod_pow2 < N : u32 > ( x : bits [ N ], y : bits [ N ]) -> bits [ N ] Returns x % y where y must be a non-zero power-of-two. std::ceil_div pub fn ceil_div < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> uN [ N ] Returns the ceiling of (x divided by y). std::round_up_to_nearest pub fn round_up_to_nearest(x: u32, y: u32) -> u32 Returns x rounded up to the nearest multiple of y . std::?pow pub fn upow < N : u32 > ( x : uN [ N ], n : uN [ N ]) -> uN [ N ] pub fn spow < N : u32 > ( x : sN [ N ], n : uN [ N ]) -> sN [ N ] Performs integer exponentiation as in Hacker's Delight, Section 11-3. Only non-negative exponents are allowed, hence the uN parameter for spow. std::clog2 pub fn clog2 < N : u32 > ( x : bits [ N ]) -> bits [ N ] Returns ceiling(log2(x)) , with one exception: When x = 0 , this function differs from the true mathematical function: clog2(0) = 0 where as ceil(log2(0)) = -infinity This function is frequently used to calculate the number of bits required to represent x possibilities. With this interpretation, it is sensible to define clog2(0) = 0 . Example: clog2(7) = 3 . std::?max pub fn smax<N: u32>(x: sN[N], y: sN[N]) -> sN[N] pub fn umax<N: u32>(x: uN[N], y: uN[N]) -> uN[N] Returns the maximum of two integers. std::umin pub fn umin < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> uN [ N ] Returns the minimum of two unsigned integers. Signed comparison pub fn sge < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> bool pub fn sgt < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> bool pub fn sle < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> bool pub fn slt < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> bool Explicit signed comparison helpers for working with unsigned values, can be a bit more convenient and a bit more explicit intent than doing casting of left hand side and right hand side. std::find_index pub fn find_index < BITS : u32 , ELEMS : u32 > ( array : uN [ BITS ][ ELEMS ], x : uN [ BITS ]) -> ( bool , u32 ) Returns ( found , index ) given an array and the element to find within the array. Note that when found is false, the index is 0 -- 0 is provided instead of a value like -1 to prevent out-of-bounds accesses from occurring if the index is used in a match expression (which will eagerly evaluate all of its arms), to prevent it from creating an error at simulation time if the value is ultimately discarded from the unselected match arm. std::lsb pub fn lsb < N : u32 > ( x : uN [ N ]) -> u1 Extracts the LSB (Least Significant Bit) from the value x and returns it. std::convert_to_bits pub fn convert_to_bits<N: u32>(x: bool[N]) -> uN[N] Converts an array of N bools to a bits[N] value. Note well: the boolean value at index 0 of the array becomes the most significant bit in the resulting bit value. Similarly, the last index of the array becomes the least significant bit in the resulting bit value. import std #[test] fn convert_to_bits_test () { let _ = assert_eq ( u3 : 0b001 , convert_to_bits ( bool [ 3 ] : [ false , false , true ])); let _ = assert_eq ( u3 : 0b100 , convert_to_bits ( bool [ 3 ] : [ true , false , false ])); () } There's always a source of confusion in these orderings: Mathematically we often indicate the least significant digit as \"digit 0\" But , in a number as we write the digits from left-to-right on a piece of paper, if you made an array from the written characters, the digit at \"array index 0\" would be the most significant bit. So, it's somewhat ambiguous whether \"index 0\" in the array would become the least significant bit or the most significant bit. This routine uses the \"as it looks on paper\" conversion; e.g. [true, false, false] becomes 0b100 . std::mask_bits pub fn mask_bits < X : u32 > () -> bits [ X ] Returns a value with X bits set (of type bits[X]). std::concat3 pub fn concat3 < X : u32 , Y : u32 , Z : u32 , R : u32 = X + Y + Z > ( x : bits [ X ], y : bits [ Y ], z : bits [ Z ]) -> bits [ R ] Concatenates 3 values of arbitrary bitwidths to a single value. std::rrot pub fn rrot<N: u32>(x: bits[N], y: bits[N]) -> bits[N] Rotate x right by y bits. acm_random.x Port of ACM random number generator to DSLX. DO NOT use acm_random.x for any application where security -- unpredictability of subsequent output and previous output -- is needed. ACMRandom is in NO WAY a cryptographically secure pseudorandom number generator, and using it where recipients of its output may wish to guess earlier/later output values would be very bad. acm_random::rng_deterministic_seed pub fn rng_deterministic_seed () -> u32 Returns a fixed seed for use in the random number generator. acm_random::rng_new pub fn rng_new ( seed : u32 ) -> State Create the state for a new random number generator using the given seed. acm_random::rng_next pub fn rng_next ( s : State ) -> ( State , u32 ) Returns a pseudo-random number in the range [1, 2^31-2] . Note that this is one number short on both ends of the full range of non-negative 32-bit integers, which range from 0 to 2^31-1 . acm_random::rng_next64 pub fn rng_next ( s : State ) -> ( State , u64 ) Returns a pseudo random number in the range [1, (2^31-2)^2] . Note that this does not cover all non-negative values of int64, which range from 0 to 2^63-1 . The top two bits are ALWAYS ZERO .","title":"Standard Library"},{"location":"dslx_std/#standard-library","text":"This page documents the DSLX standard library. Standard Library std.x std::bounded_minus_1 std::abs std::is_pow2 std::?mul std::iterative_div std::div_pow2 std::mod_pow2 std::ceil_div std::round_up_to_nearest std::?pow std::clog2 std::?max std::umin Signed comparison std::find_index std::lsb std::convert_to_bits std::mask_bits std::concat3 std::rrot acm_random.x acm_random::rng_deterministic_seed acm_random::rng_new acm_random::rng_next acm_random::rng_next64","title":"Standard Library"},{"location":"dslx_std/#stdx","text":"","title":"std.x"},{"location":"dslx_std/#stdbounded_minus_1","text":"pub fn bounded_minus_1 < N : u32 > ( x : uN [ N ]) -> uN [ N ] Returns the value of x - 1 with saturation at 0 .","title":"std::bounded_minus_1"},{"location":"dslx_std/#stdabs","text":"pub fn abs < BITS : u32 > ( x : sN [ BITS ]) -> sN [ BITS ] Returns the absolute value of x as a signed number.","title":"std::abs"},{"location":"dslx_std/#stdis_pow2","text":"pub fn is_pow2 < N : u32 > ( x : uN [ N ]) -> bool Returns true when x is a non-zero power-of-two.","title":"std::is_pow2"},{"location":"dslx_std/#stdmul","text":"pub fn umul < N : u32 , M : u32 , R : u32 = N + M > ( x : uN [ N ], y : uN [ M ]) -> uN [ R ] pub fn smul < N : u32 , M : u32 , R : u32 = N + M > ( x : sN [ N ], y : sN [ M ]) -> sN [ R ] Returns product of x ( N bits) and y ( M bits) as an N+M bit value.","title":"std::?mul"},{"location":"dslx_std/#stditerative_div","text":"pub fn iterative_div < N : u32 , DN : u32 = N * u32 : 2 > ( x : uN [ N ], y : uN [ N ]) -> uN [ N ] Calculate x / y one bit at a time. This is an alternative to using the division operator '/' which may not synthesize nicely.","title":"std::iterative_div"},{"location":"dslx_std/#stddiv_pow2","text":"pub fn div_pow2 < N : u32 > ( x : bits [ N ], y : bits [ N ]) -> bits [ N ] Returns x / y where y must be a non-zero power-of-two.","title":"std::div_pow2"},{"location":"dslx_std/#stdmod_pow2","text":"pub fn mod_pow2 < N : u32 > ( x : bits [ N ], y : bits [ N ]) -> bits [ N ] Returns x % y where y must be a non-zero power-of-two.","title":"std::mod_pow2"},{"location":"dslx_std/#stdceil_div","text":"pub fn ceil_div < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> uN [ N ] Returns the ceiling of (x divided by y).","title":"std::ceil_div"},{"location":"dslx_std/#stdround_up_to_nearest","text":"pub fn round_up_to_nearest(x: u32, y: u32) -> u32 Returns x rounded up to the nearest multiple of y .","title":"std::round_up_to_nearest"},{"location":"dslx_std/#stdpow","text":"pub fn upow < N : u32 > ( x : uN [ N ], n : uN [ N ]) -> uN [ N ] pub fn spow < N : u32 > ( x : sN [ N ], n : uN [ N ]) -> sN [ N ] Performs integer exponentiation as in Hacker's Delight, Section 11-3. Only non-negative exponents are allowed, hence the uN parameter for spow.","title":"std::?pow"},{"location":"dslx_std/#stdclog2","text":"pub fn clog2 < N : u32 > ( x : bits [ N ]) -> bits [ N ] Returns ceiling(log2(x)) , with one exception: When x = 0 , this function differs from the true mathematical function: clog2(0) = 0 where as ceil(log2(0)) = -infinity This function is frequently used to calculate the number of bits required to represent x possibilities. With this interpretation, it is sensible to define clog2(0) = 0 . Example: clog2(7) = 3 .","title":"std::clog2"},{"location":"dslx_std/#stdmax","text":"pub fn smax<N: u32>(x: sN[N], y: sN[N]) -> sN[N] pub fn umax<N: u32>(x: uN[N], y: uN[N]) -> uN[N] Returns the maximum of two integers.","title":"std::?max"},{"location":"dslx_std/#stdumin","text":"pub fn umin < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> uN [ N ] Returns the minimum of two unsigned integers.","title":"std::umin"},{"location":"dslx_std/#signed-comparison","text":"pub fn sge < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> bool pub fn sgt < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> bool pub fn sle < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> bool pub fn slt < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> bool Explicit signed comparison helpers for working with unsigned values, can be a bit more convenient and a bit more explicit intent than doing casting of left hand side and right hand side.","title":"Signed comparison"},{"location":"dslx_std/#stdfind_index","text":"pub fn find_index < BITS : u32 , ELEMS : u32 > ( array : uN [ BITS ][ ELEMS ], x : uN [ BITS ]) -> ( bool , u32 ) Returns ( found , index ) given an array and the element to find within the array. Note that when found is false, the index is 0 -- 0 is provided instead of a value like -1 to prevent out-of-bounds accesses from occurring if the index is used in a match expression (which will eagerly evaluate all of its arms), to prevent it from creating an error at simulation time if the value is ultimately discarded from the unselected match arm.","title":"std::find_index"},{"location":"dslx_std/#stdlsb","text":"pub fn lsb < N : u32 > ( x : uN [ N ]) -> u1 Extracts the LSB (Least Significant Bit) from the value x and returns it.","title":"std::lsb"},{"location":"dslx_std/#stdconvert_to_bits","text":"pub fn convert_to_bits<N: u32>(x: bool[N]) -> uN[N] Converts an array of N bools to a bits[N] value. Note well: the boolean value at index 0 of the array becomes the most significant bit in the resulting bit value. Similarly, the last index of the array becomes the least significant bit in the resulting bit value. import std #[test] fn convert_to_bits_test () { let _ = assert_eq ( u3 : 0b001 , convert_to_bits ( bool [ 3 ] : [ false , false , true ])); let _ = assert_eq ( u3 : 0b100 , convert_to_bits ( bool [ 3 ] : [ true , false , false ])); () } There's always a source of confusion in these orderings: Mathematically we often indicate the least significant digit as \"digit 0\" But , in a number as we write the digits from left-to-right on a piece of paper, if you made an array from the written characters, the digit at \"array index 0\" would be the most significant bit. So, it's somewhat ambiguous whether \"index 0\" in the array would become the least significant bit or the most significant bit. This routine uses the \"as it looks on paper\" conversion; e.g. [true, false, false] becomes 0b100 .","title":"std::convert_to_bits"},{"location":"dslx_std/#stdmask_bits","text":"pub fn mask_bits < X : u32 > () -> bits [ X ] Returns a value with X bits set (of type bits[X]).","title":"std::mask_bits"},{"location":"dslx_std/#stdconcat3","text":"pub fn concat3 < X : u32 , Y : u32 , Z : u32 , R : u32 = X + Y + Z > ( x : bits [ X ], y : bits [ Y ], z : bits [ Z ]) -> bits [ R ] Concatenates 3 values of arbitrary bitwidths to a single value.","title":"std::concat3"},{"location":"dslx_std/#stdrrot","text":"pub fn rrot<N: u32>(x: bits[N], y: bits[N]) -> bits[N] Rotate x right by y bits.","title":"std::rrot"},{"location":"dslx_std/#acm_randomx","text":"Port of ACM random number generator to DSLX. DO NOT use acm_random.x for any application where security -- unpredictability of subsequent output and previous output -- is needed. ACMRandom is in NO WAY a cryptographically secure pseudorandom number generator, and using it where recipients of its output may wish to guess earlier/later output values would be very bad.","title":"acm_random.x"},{"location":"dslx_std/#acm_randomrng_deterministic_seed","text":"pub fn rng_deterministic_seed () -> u32 Returns a fixed seed for use in the random number generator.","title":"acm_random::rng_deterministic_seed"},{"location":"dslx_std/#acm_randomrng_new","text":"pub fn rng_new ( seed : u32 ) -> State Create the state for a new random number generator using the given seed.","title":"acm_random::rng_new"},{"location":"dslx_std/#acm_randomrng_next","text":"pub fn rng_next ( s : State ) -> ( State , u32 ) Returns a pseudo-random number in the range [1, 2^31-2] . Note that this is one number short on both ends of the full range of non-negative 32-bit integers, which range from 0 to 2^31-1 .","title":"acm_random::rng_next"},{"location":"dslx_std/#acm_randomrng_next64","text":"pub fn rng_next ( s : State ) -> ( State , u64 ) Returns a pseudo random number in the range [1, (2^31-2)^2] . Note that this does not cover all non-negative values of int64, which range from 0 to 2^63-1 . The top two bits are ALWAYS ZERO .","title":"acm_random::rng_next64"},{"location":"floating_point/","text":"Floating-point routines XLS provides implementations of several floating-point operations and may add more at any time. Here are listed notable details of our implementations or of floating-point operations in general. Unless otherwise specified, not possible or out-of-scope, all operations and types should be IEEE-754 compliant. For example, floating-point exceptions have not been implemented; they're outside our current scope. The numeric results of multiplication, on the other hand, should exactly match those of any other compliant implementation. APFloat Floating-point operations are, in general, defined by the same sequence of steps regardless of their underlying bit widths: fractional parts must be expanded then aligned, then an operation (add, multiply, etc.) must be performed, interpreting the fractions as integral types, followed by rounding, special case handling, and reconstructing an output FP type. This observation leads to the possibility of generic floating-point routines: a fully parameterized add, for example, which can be instantiated with and 8-bit exponent and 23-bit fractional part for binary32 types, and an 11-bit exponent and 52-bit fractional part for binary64 types. Even more interesting, a hypothetical bfloat32 type could immediately be supported by, say, instantiating that adder with, say, 15 exponent bits and 16 fractional ones. As much as possible, XLS implements FP operations in terms of its APFloat (arbitrary-precision floating-point) type. APFloat is a parameterized floating-point structure with a fixed one-bit sign and specifiable exponent and fractional part size. For ease of use, common types, such as float32 , are defined in terms of those APFloat types. For example, the generic \"is X infinite\" operation is defined in apfloat.x as: // Returns whether or not the given APFloat represents an infinite quantity. pub fn is_inf < EXP_SZ : u32 , FRACTION_SZ : u32 > ( x : APFloat < EXP_SZ , FRACTION_SZ > ) -> u1 { ( x . bexp == std :: mask_bits < EXP_SZ > () && x . fraction == bits [ FRACTION_SZ ] : 0 ) } Whereas in float32.x , F32 is defined as: pub type F32 = apfloat :: APFloat < u32 : 8 , u32 : 23 > ; and is_inf is exposed as: pub fn is_inf ( f : F32 ) -> u1 { apfloat :: is_inf < u32 : 8 , u32 : 23 > ( f ) } In this way, users can refer to F32 types and can use them as and with float32::is_inf(f) , giving them simplified access to a generic operation. More complex functionality such as addition and multiplication are defined in standalone modules, e.g, apfloat_add_2.x defining apfloat_add_2::add for the APFloat type fp32_add_2.x instantiating the operation for the float32 type. Supported operations Here are listed the routines so far implemented in XLS. Unless otherwise specified, operations are implemented in terms of APFloats such that they can support any precisions (aside from corner cases, such as a zero-byte fractional part). Operation details Add/sub Floating-point addition, like any FP operation, is much more complicated than integer addition, and has many more steps. Being the first operation described, we'll take extra care to explain floating-point addition: Expand fractions: Floating-point operations are computed with bits beyond that in their normal representations for increased precision. For IEEE 754 numbers, there are three extra, called the guard, rounding and sticky bits. The first two behave normally, but the last, the \"sticky\" bit, is special. During shift operations (below), if a \"1\" value is ever shifted into the sticky bit, it \"sticks\" - the bit will remain \"1\" through any further shift operations. In this step, the fractions are expanded by these three bits. Align fractions: To ensure that fractions are added with appropriate magnitudes, they must be aligned according to their exponents. To do so, the smaller significant needs to be shifted to the right (each right shift is equivalent to increasing the exponent by one). - The extra precision bits are populated in this shift. - As part of this step, the leading 1 bit... and a sign bit Note: The sticky bit is calculated and applied in this step. Sign-adjustment: if the fractions differ in sign, then the fraction with the smaller initial exponent needs to be (two's complement) negated. Add the fractions and capture the carry bit. Note that, if the signs of the fractions differs, then this could result in higher bits being cleared. Normalize the fractions: Shift the result so that the leading '1' is present in the proper space. This means shifting right one place if the result set the carry bit, and to the left some number of places if high bits were cleared. - The sticky bit must be preserved in any of these shifts! Rounding: Here, the extra precision bits are examined to determine if the result fraction's last bit should be rounded up. IEEE 754 supports five rounding modes: - Round towards 0: just chop off the extra precision bits. - Round towards +infinity: round up if any extra precision bits are set. - Round towards -infinity: round down if any extra precision bits are set. - Round to nearest, ties away from zero: Rounds to the nearest value. In cases where the extra precision bits are halfway between values, i.e., 0b100, then the result is rounded up for positive numbers and down for negative ones. - Round to nearest, ties to even: Rounds to the nearest value. In cases where the extra precision bits are halfway between values, then the result is rounded in whichever direction causes the LSB of the result significant to be 0. - This is the most commonly-used rounding mode. - This is [currently] the only supported mode by the DSLX implementation. Special case handling: The results are examined for special cases such as NaNs, infinities, or (optionally) subnormals. Result sign determination The sign of the result will normally be the same as the sign of the operand with the greater exponent, but there are two extra cases to consider. If the operands have the same exponent, then the sign will be that of the greater fraction, and if the result is 0, then we favor positive 0 vs. negative 0 (types are as for a C float implementation): let fraction = ( addend_x as s29 ) + ( addend_y as s29 ); let fraction_is_zero = fraction == s29 : 0 ; let result_sign = match ( fraction_is_zero , fraction < s29 : 0 ) { ( true , _ ) => u1 : 0 , ( false , true ) => ! greater_exp . sign , _ => greater_exp . sign , }; Rounding As complicated as rounding is to describe, its implementation is relatively straightforward (types are as for a C float implementation): let normal_chunk = shifted_fraction [ 0 : 3 ]; let half_way_chunk = shifted_fraction [ 2 : 4 ]; let do_round_up = u1 : 1 if ( normal_chunk > u3 : 0x4 ) | ( half_way_chunk == u2 : 0x3 ) else u1 : 0 ; // We again need an extra bit for carry. let rounded_fraction = ( shifted_fraction as u28 ) + u28 : 0x8 if do_round_up else ( shifted_fraction as u28 ); let rounding_carry = rounded_fraction [ - 1 : ]; Mul TODO(rspringer): 2021-04-06: This. FMA The fma operation (again, fused multiply-add) is a three-operand operation that computes the product of the first two and the sum of that with the third. The IEEE 754-2008 description of the operation states that the operation should be performed \"as if with unbounded range and precision\", limited only by rounding of the final result. In other words, this differs from a sequence of a separate multiply followed by an add in that there is only a single rounding step (instead of the two involved in separate operations). In practice, this means A) that the precision of an FMA is higher than individual ops, and thus that B) an FMA requires significantly more internal precision bits than naively expected. For binary32 inputs, to achieve the standard-specified precision, the initial mul requires the usual 48 ((23 fraction + 1 \"hidden\") * 2) fraction bits. When performing the subsequent add step, though, it is necessary to maintain 72 fraction bits ((23 fraction + 1 \"hidden\") * 3). Fortunately, this sum includes the guard, round, and sticky bits (so we don't need 75). The mathematical derivation of the exact amount will not be given here (as I've not done it), but the same calculated size would apply for other data types (i.e., 54 * 2 = 108 and 54 * 3 = 162 for binary64). Aside from determining the necessary precision bits, the FMA implementation is rather straightforward, especially after reviewing the adder and multiplier. Testing Several different methods are used to test these routines, depending on applicability. These are: Reference comparison: exhaustive testing Reference comparison: space-sampling Formal proving When comparing to a reference, a natural question is the stability of the reference, i.e., is the reference answer the same across all versions or environments? Will the answer given by glibc/libm on AArch64 be the same as one given by a hardware FMA unit on a GPU? Fortunately, all \"correct\" implementations will give the same results for the same inputs.* In addition, POSIX has the same result-precision language. It's worth noting that -ffast-math doesn't currently affect FMA emission/fusion/fission/etc. * - There are operations for which this is not true. Transcendental ops may differ between implementations due to the table maker's dilemma . Exhaustive testing This is the happiest case - where the input space is so small that we can iterate over every possible input, effectively treating the input as a binary iteration counter. Sadly, this is uncommon (except, perhaps for ML math), as binary32 is the precision floor for most problems, and a 64-bit input space is well beyond our current abilities. Still - if your problem can be exhaustively tested (with respect to a trusted reference), it should be exhaustively tested! None of our current ops are tested in this way, although the bf16 cases could/should be. Space-sampling When the problem input space is too large for exhaustive testing, then random samples can be tested instead. This approach can't give complete verification of an implementation, but, given enough samples, it can yield a high degree of confidence. The existing modules are tested in this way. This could be improved by preventing re-testing of any given sample (at the cost of memory and, perhaps, atomic/locking costs) and by identifying interesting \"corner cases\" of the input space and focusing on those. Formal verification This sort of testing utilizes our formal solver infrastructure to prove correctness with the solver's internal FP implementation. This is fully described in the solvers documentation .","title":"Floating Point"},{"location":"floating_point/#floating-point-routines","text":"XLS provides implementations of several floating-point operations and may add more at any time. Here are listed notable details of our implementations or of floating-point operations in general. Unless otherwise specified, not possible or out-of-scope, all operations and types should be IEEE-754 compliant. For example, floating-point exceptions have not been implemented; they're outside our current scope. The numeric results of multiplication, on the other hand, should exactly match those of any other compliant implementation.","title":"Floating-point routines"},{"location":"floating_point/#apfloat","text":"Floating-point operations are, in general, defined by the same sequence of steps regardless of their underlying bit widths: fractional parts must be expanded then aligned, then an operation (add, multiply, etc.) must be performed, interpreting the fractions as integral types, followed by rounding, special case handling, and reconstructing an output FP type. This observation leads to the possibility of generic floating-point routines: a fully parameterized add, for example, which can be instantiated with and 8-bit exponent and 23-bit fractional part for binary32 types, and an 11-bit exponent and 52-bit fractional part for binary64 types. Even more interesting, a hypothetical bfloat32 type could immediately be supported by, say, instantiating that adder with, say, 15 exponent bits and 16 fractional ones. As much as possible, XLS implements FP operations in terms of its APFloat (arbitrary-precision floating-point) type. APFloat is a parameterized floating-point structure with a fixed one-bit sign and specifiable exponent and fractional part size. For ease of use, common types, such as float32 , are defined in terms of those APFloat types. For example, the generic \"is X infinite\" operation is defined in apfloat.x as: // Returns whether or not the given APFloat represents an infinite quantity. pub fn is_inf < EXP_SZ : u32 , FRACTION_SZ : u32 > ( x : APFloat < EXP_SZ , FRACTION_SZ > ) -> u1 { ( x . bexp == std :: mask_bits < EXP_SZ > () && x . fraction == bits [ FRACTION_SZ ] : 0 ) } Whereas in float32.x , F32 is defined as: pub type F32 = apfloat :: APFloat < u32 : 8 , u32 : 23 > ; and is_inf is exposed as: pub fn is_inf ( f : F32 ) -> u1 { apfloat :: is_inf < u32 : 8 , u32 : 23 > ( f ) } In this way, users can refer to F32 types and can use them as and with float32::is_inf(f) , giving them simplified access to a generic operation. More complex functionality such as addition and multiplication are defined in standalone modules, e.g, apfloat_add_2.x defining apfloat_add_2::add for the APFloat type fp32_add_2.x instantiating the operation for the float32 type.","title":"APFloat"},{"location":"floating_point/#supported-operations","text":"Here are listed the routines so far implemented in XLS. Unless otherwise specified, operations are implemented in terms of APFloats such that they can support any precisions (aside from corner cases, such as a zero-byte fractional part).","title":"Supported operations"},{"location":"floating_point/#operation-details","text":"","title":"Operation details"},{"location":"floating_point/#addsub","text":"Floating-point addition, like any FP operation, is much more complicated than integer addition, and has many more steps. Being the first operation described, we'll take extra care to explain floating-point addition: Expand fractions: Floating-point operations are computed with bits beyond that in their normal representations for increased precision. For IEEE 754 numbers, there are three extra, called the guard, rounding and sticky bits. The first two behave normally, but the last, the \"sticky\" bit, is special. During shift operations (below), if a \"1\" value is ever shifted into the sticky bit, it \"sticks\" - the bit will remain \"1\" through any further shift operations. In this step, the fractions are expanded by these three bits. Align fractions: To ensure that fractions are added with appropriate magnitudes, they must be aligned according to their exponents. To do so, the smaller significant needs to be shifted to the right (each right shift is equivalent to increasing the exponent by one). - The extra precision bits are populated in this shift. - As part of this step, the leading 1 bit... and a sign bit Note: The sticky bit is calculated and applied in this step. Sign-adjustment: if the fractions differ in sign, then the fraction with the smaller initial exponent needs to be (two's complement) negated. Add the fractions and capture the carry bit. Note that, if the signs of the fractions differs, then this could result in higher bits being cleared. Normalize the fractions: Shift the result so that the leading '1' is present in the proper space. This means shifting right one place if the result set the carry bit, and to the left some number of places if high bits were cleared. - The sticky bit must be preserved in any of these shifts! Rounding: Here, the extra precision bits are examined to determine if the result fraction's last bit should be rounded up. IEEE 754 supports five rounding modes: - Round towards 0: just chop off the extra precision bits. - Round towards +infinity: round up if any extra precision bits are set. - Round towards -infinity: round down if any extra precision bits are set. - Round to nearest, ties away from zero: Rounds to the nearest value. In cases where the extra precision bits are halfway between values, i.e., 0b100, then the result is rounded up for positive numbers and down for negative ones. - Round to nearest, ties to even: Rounds to the nearest value. In cases where the extra precision bits are halfway between values, then the result is rounded in whichever direction causes the LSB of the result significant to be 0. - This is the most commonly-used rounding mode. - This is [currently] the only supported mode by the DSLX implementation. Special case handling: The results are examined for special cases such as NaNs, infinities, or (optionally) subnormals.","title":"Add/sub"},{"location":"floating_point/#result-sign-determination","text":"The sign of the result will normally be the same as the sign of the operand with the greater exponent, but there are two extra cases to consider. If the operands have the same exponent, then the sign will be that of the greater fraction, and if the result is 0, then we favor positive 0 vs. negative 0 (types are as for a C float implementation): let fraction = ( addend_x as s29 ) + ( addend_y as s29 ); let fraction_is_zero = fraction == s29 : 0 ; let result_sign = match ( fraction_is_zero , fraction < s29 : 0 ) { ( true , _ ) => u1 : 0 , ( false , true ) => ! greater_exp . sign , _ => greater_exp . sign , };","title":"Result sign determination"},{"location":"floating_point/#rounding","text":"As complicated as rounding is to describe, its implementation is relatively straightforward (types are as for a C float implementation): let normal_chunk = shifted_fraction [ 0 : 3 ]; let half_way_chunk = shifted_fraction [ 2 : 4 ]; let do_round_up = u1 : 1 if ( normal_chunk > u3 : 0x4 ) | ( half_way_chunk == u2 : 0x3 ) else u1 : 0 ; // We again need an extra bit for carry. let rounded_fraction = ( shifted_fraction as u28 ) + u28 : 0x8 if do_round_up else ( shifted_fraction as u28 ); let rounding_carry = rounded_fraction [ - 1 : ];","title":"Rounding"},{"location":"floating_point/#mul","text":"TODO(rspringer): 2021-04-06: This.","title":"Mul"},{"location":"floating_point/#fma","text":"The fma operation (again, fused multiply-add) is a three-operand operation that computes the product of the first two and the sum of that with the third. The IEEE 754-2008 description of the operation states that the operation should be performed \"as if with unbounded range and precision\", limited only by rounding of the final result. In other words, this differs from a sequence of a separate multiply followed by an add in that there is only a single rounding step (instead of the two involved in separate operations). In practice, this means A) that the precision of an FMA is higher than individual ops, and thus that B) an FMA requires significantly more internal precision bits than naively expected. For binary32 inputs, to achieve the standard-specified precision, the initial mul requires the usual 48 ((23 fraction + 1 \"hidden\") * 2) fraction bits. When performing the subsequent add step, though, it is necessary to maintain 72 fraction bits ((23 fraction + 1 \"hidden\") * 3). Fortunately, this sum includes the guard, round, and sticky bits (so we don't need 75). The mathematical derivation of the exact amount will not be given here (as I've not done it), but the same calculated size would apply for other data types (i.e., 54 * 2 = 108 and 54 * 3 = 162 for binary64). Aside from determining the necessary precision bits, the FMA implementation is rather straightforward, especially after reviewing the adder and multiplier.","title":"FMA"},{"location":"floating_point/#testing","text":"Several different methods are used to test these routines, depending on applicability. These are: Reference comparison: exhaustive testing Reference comparison: space-sampling Formal proving When comparing to a reference, a natural question is the stability of the reference, i.e., is the reference answer the same across all versions or environments? Will the answer given by glibc/libm on AArch64 be the same as one given by a hardware FMA unit on a GPU? Fortunately, all \"correct\" implementations will give the same results for the same inputs.* In addition, POSIX has the same result-precision language. It's worth noting that -ffast-math doesn't currently affect FMA emission/fusion/fission/etc. * - There are operations for which this is not true. Transcendental ops may differ between implementations due to the table maker's dilemma .","title":"Testing"},{"location":"floating_point/#exhaustive-testing","text":"This is the happiest case - where the input space is so small that we can iterate over every possible input, effectively treating the input as a binary iteration counter. Sadly, this is uncommon (except, perhaps for ML math), as binary32 is the precision floor for most problems, and a 64-bit input space is well beyond our current abilities. Still - if your problem can be exhaustively tested (with respect to a trusted reference), it should be exhaustively tested! None of our current ops are tested in this way, although the bf16 cases could/should be.","title":"Exhaustive testing"},{"location":"floating_point/#space-sampling","text":"When the problem input space is too large for exhaustive testing, then random samples can be tested instead. This approach can't give complete verification of an implementation, but, given enough samples, it can yield a high degree of confidence. The existing modules are tested in this way. This could be improved by preventing re-testing of any given sample (at the cost of memory and, perhaps, atomic/locking costs) and by identifying interesting \"corner cases\" of the input space and focusing on those.","title":"Space-sampling"},{"location":"floating_point/#formal-verification","text":"This sort of testing utilizes our formal solver infrastructure to prove correctness with the solver's internal FP implementation. This is fully described in the solvers documentation .","title":"Formal verification"},{"location":"fpga_characterization/","text":"FPGA Characterization Note that right now the in-tree yosys and nextpnr-ice40 builds plugins aren't registering properly, see issue #188 . As a result, we have to use out-of-tree yosys and nextpnr-ice40 builds for the moment. $ bazel build -c opt //xls/synthesis/yosys:yosys_server_main $ ./bazel-bin/xls/synthesis/yosys/yosys_server_main \\ --yosys_path $(which yosys) \\ --nextpnr_path $(which nextpnr-ice40) \\ --synthesis_target=ice40 \\ --alsologtostderr The above runs a gRPC service, so in another terminal pane, we run the characterization driver: $ bazel run -c opt //xls/synthesis:timing_characterization_client_main $ ./bazel-bin/xls/synthesis/timing_characterization_client_main \\ > ./xls/delay_model/models/ice40.textproto This produces a textual representation of the delay model protobuf. Building In-Tree Binaries Note that these cannot currently be used for the above characterization flow, see issue #188 Build yosys and nextpnr-ice40 : $ bazel build -c opt @at_clifford_yosys//:yosys @nextpnr//:nextpnr-ice40","title":"FPGA characterization (experimental)"},{"location":"fpga_characterization/#fpga-characterization","text":"Note that right now the in-tree yosys and nextpnr-ice40 builds plugins aren't registering properly, see issue #188 . As a result, we have to use out-of-tree yosys and nextpnr-ice40 builds for the moment. $ bazel build -c opt //xls/synthesis/yosys:yosys_server_main $ ./bazel-bin/xls/synthesis/yosys/yosys_server_main \\ --yosys_path $(which yosys) \\ --nextpnr_path $(which nextpnr-ice40) \\ --synthesis_target=ice40 \\ --alsologtostderr The above runs a gRPC service, so in another terminal pane, we run the characterization driver: $ bazel run -c opt //xls/synthesis:timing_characterization_client_main $ ./bazel-bin/xls/synthesis/timing_characterization_client_main \\ > ./xls/delay_model/models/ice40.textproto This produces a textual representation of the delay model protobuf.","title":"FPGA Characterization"},{"location":"fpga_characterization/#building-in-tree-binaries","text":"Note that these cannot currently be used for the above characterization flow, see issue #188 Build yosys and nextpnr-ice40 : $ bazel build -c opt @at_clifford_yosys//:yosys @nextpnr//:nextpnr-ice40","title":"Building In-Tree Binaries"},{"location":"fuzzer/","text":"XLS Fuzzer XLS Fuzzer Crashers Directory Single-file reproducers IR minimization Summaries Debugging a failing sample Debugging a tool crash Result miscomparison: unoptimized IR Result miscomparison: optimized IR Debugging the LLVM JIT Building LLVM tools Isolating the bug Instcombine Evaluating LLVM IR Building LLVM at head Result miscomparison: simulated Verilog To execute the XLS fuzz driver simply run a command line like the following: bazel run -c opt \\ //xls/fuzzer:run_fuzz_multiprocess \\ -- --crash_path=/tmp/crashers-$(date +'%Y-%m-%d') --seed=0 --duration=8h The XLS fuzzer generates a sequence of randomly generated DSLX functions and a set of random inputs to each function often with interesting bit patterns. Given that stimulus, the fuzz driver performs the following actions some of which may be disabled/enabled via flags (run with --help for more details): Runs the DSLX program through the DSLX interpreter with the batch of arguments Converts the DSLX program to IR Optimizes the converted IR Interprets the pre-optimized and optimized IR with the batch of arguments Generates the Verilog from the IR with randomly selected codegen options Simulates the generated Verilog using the batch of arguments Performs a multi-way comparison of the DSLX interpreter results, the pre-optimized IR interpreter results, post-optimized IR interpreter results, and the simulator results If an issue is observed, the fuzz driver attempts to minimize the IR that causes an issue to occur. The above actions are coordinated and run by the SampleRunner class. Many actions are performed by invoking a separate binary which isolates any crashes. When miscompares in results occur or the generated function crashes part of XLS, all artifacts generated by the fuzzer for that sample are written into a uniquely-named subdirectory under the --crash_path given in the command line. The fuzzer also writes a crasher file which is a single file for reproducing the issue. See below for instructions on debugging a failing sample. Crashers Directory The crashers directory includes a subdirectory created for each failing sample. To avoid collisions the subdirectory is named using a hash of the DSLX code. Each crasher subdirectory has the following contents: $ ls /tmp/crashers-2019-06-25/05adbd50 args.txt ir_converter_main.stderr run.sh cl.txt ir_minimizer.options.json sample.ir crasher_2020-04-23_9b05.x ir_minimizer_test.sh sample.ir.results eval_ir_main.stderr options.json sample.x exception.txt opt_main.stderr sample.x.results The directory includes the problematic DSLX sample ( sample.x ) and the input arguments ( args.txt ) as well as all artifacts generated and stderr output emitted by the various utilities invoked to test the sample. Notable files include: options.json : Options used to run the sample. sample.ir : Unoptimized IR generated from the DSLX sample. sample.opt.ir : IR after optimizations. sample.v : Generated Verilog. *.results : The results (numeric values) produced by interpreting or simulating the respective input (DSLX, IR, or Verilog). exception.txt : The exception raised when running the sample. Typically this will indicate either a result miscomparison or a tool return non-zero status (for example, the IR optimizer crashed). crasher_*.x : A single file reproducer which includes the DSLX code, arguments, and options. See below for details. Typically the exact nature of the failure can be identified by reading the file exception.txt and possibly the stderr outputs of the various tools. The fuzzer can optionally produce a minimized IR reproduction of the problem. This will be written to minimized.ir . See below for details. Single-file reproducers When the fuzzer encounters an issue it will create a single-file reproducer: --- Worker 14 observed an exception, noting --- Worker 14 noted crasher #1 for sampleno 42 at /tmp/crashers/095fb405 Copying that file to the directory //xls/fuzzer/crashers will automatically create a bazel test target for it and add it to the regression suite. Tests can also be added as known failures in //xls/fuzzer/build_defs.bzl as they're being triaged / investigated like so: generate_crasher_regression_tests( srcs = glob([\"crashers/*\"]), prefix = \"xls/fuzzer\", # TODO(xls-team): 2019-06-30 Triage and fix these. failing = [ \"crashers/crasher_2019-06-29_129987.x\", \"crashers/crasher_2019-06-29_402110.x\", ], ) Known-failures are marked as manual and excluded from continuous testing. To run the regression suite: bazel test //xls/fuzzer:all To run the regression suite including known-failures, run the regression target directly: bazel test //xls/fuzzer:regression_tests To reproduce from that single-file reproducer there is a command line tool: bazel run //xls/fuzzer:run_crasher -- \\ crasher_2019-06-26_3354.x IR minimization By default the fuzzer attempts to generate a minimal IR reproducer for the problem identified by the DSLX sample. Starting with the unoptimized IR the fuzzer invokes ir_minimizer_main to reduce the size of the input IR. It uses various simplification strategies to minimize the number of nodes in the IR. See the usage description in the tool source code for detailed information. The minimized IR is written to a file minimized.ir in the crasher directory for the sample. Note that minimization is only possible if the problem (crash, result miscomparison, etc.) occurs after conversion from DSLX to XLS IR. Summaries To monitor progress of the fuzzer and to determine op coverage the fuzzer can optionally (with --summary_path ) write summary information to files. The summary files are Protobuf files containing the proto SampleSummaryProto defined in //xls/fuzzer/sample_summary.proto . The summary information about the IR generated from the DSLX sample such as the number and type of each IR op as well as the bit width and number of operands. The summary information also includes a timing breakdown of the various operations performed for each sample (sample generation, IR conversion, etc). This can be used to identify performance bottlenecks in the fuzzer. The summaries can be read with the tool //xls/fuzzer/read_summary_main . See usage description in the code for more details. Debugging a failing sample A generated sample can fail in one of two ways: a tool crash or a result miscomparison. A tool crash occurred if one of the tools invoked by the fuzzer (e.g., opt_main which optimizes the IR) returned a non-zero status. A result miscomparison occurred if there is not perfect correspondence between the results produced by various ways in which the generated function is evaluated: Interpreted DSLX Evaluated unoptimized IR Evaluated optimized IR Simulation of the generated (System)Verilog Generally, the results produced by the interpretation of the DSLX serves are the reference results for comparisons. To identify the underlying cause of the sample failure inspect the exception.txt file in the crasher directory. The file contains the text of the exception raised in SampleRunner which clearly identifies the kind of failure (result miscomparison or tool crash) and details about which evaluation resulted in a miscompare or which tool crashed, respectively. Consult the following sections on how to debug particular kinds of failures. Debugging a tool crash The exception.txt file includes the invocation of the tool for reproducing the failure. Generally, this is a straightforward debugging process. If the failing tool is the IR optimizer binary opt_main the particular pass causing the failure should be in the backtrace. To retrieve the input to this pass, run opt_main with --ir_dump_path to dump the IR between each pass. The last IR file produced (the files are numbered sequentially) is the input to the failing pass. Result miscomparison: unoptimized IR The evaluation of the unoptimized IR is the first point at which result comparison occurs (DSLX interpretation versus unoptimized IR evaluation). A miscomparison here can indicate a bug in one of several places: DSLX interpreter DSLX to IR conversion IR interpreter or IR JIT. The error message in exception.txt indicates whether the JIT or the interpreter was used. To help narrow this down, the IR interpreter can be compared against the JIT with the eval_ir_main tool: eval_ir_main --test_llvm_jit --input_file=args.txt sample.ir This runs both the JIT and the interpreter on the unoptimized IR file ( sample.ir ) using the arguments in args.txt and compares the results. If this is successful, then likely the IR interpreter and the JIT are correct and problem lies earlier in the pipeline (DSLX interpretation or DSLX to IR conversion). Otherwise, there is definitely a bug in either the interpreter or the JIT as their results should always be equal. If a minimized IR file exists ( minimized.ir ) this may be a better starting point for isolating the failure. Result miscomparison: optimized IR This can indicate a bug in IR evaluation (interpreter or JIT) or in the optimizer. In this case, a comparison of the evaluation of the unoptimized IR against the DSLX interpreter has already succeeds so DSLX interpretation or conversion is unlikely to be the underlying cause. As with miscomparison involving the unoptimized IR, eval_ir_main can be used to compare the JIT results against the interpreter results: eval_ir_main --test_llvm_jit --input_file=args.txt sample.opt.ir If the above invocation fails there is a bug in the JIT or the interpreter. Otherwise, there may be a bug in the optimizer. The tool eval_ir_main can help isolate the problematic optimization pass by running with the options --optimize_ir and --eval_after_each_pass . With these flags, the tool runs the optimization pipeline on the given IR and evaluates the IR after each pass is run. The first pass which results in a miscompare against the unoptimized input IR is flagged. Invocation: eval_ir_main --input_file=args.txt \\ --optimize_ir \\ --eval_after_each_pass \\ sample.ir Debugging the LLVM JIT To help isolate bugs in the JIT, LLVM's optimization level can be set using the --llvm_opt_level flag: eval_ir_main --test_llvm_jit \\ --llvm_opt_level=0 \\ --input_file=args.txt sample.opt.ir If the results match (pass) with the optimization level set to zero but fail with the default optimization level of 3, there is likely a bug in the LLVM optimizer or the XLS-generated LLVM program has undefined behavior. Unoptimized and optimized LLVM IR are dumped by the JIT with vlog level of 2 or higher, and the assembly is dumped at level 3 or higher. For example: eval_ir_main -v=3 --logtostderr sample.opt.ir Extract the unoptimized LLVM IR to file to enable working with it in isolation. Building LLVM tools The various LLVM tools such as opt and lli can be built with: bazel build /llvm/llvm-project/llvm:all Build in fastbuild mode to get checks and debug features in LLVM. Isolating the bug To inspect the optimized LLVM IR, run: opt sample.ll -O3 -S To print the IR before and after each pass: opt sample.ll -S -print-after-all -print-before-all -O3 Instcombine Instcombine is an LLVM optimization pass which is a common source of bugs in code generated from XLS because XLS uses unusual bit widths which exercise little used paths in this optimization. To run instcombine alone: opt /tmp/bad.ll -S --instcombine Instcombine is a large monolithic pass and it can be difficult to isolate the exact transformation which caused the problem. Fortunately, this pass includes a \"compiler fuel\" option which can be used to limit the number of transformations performed by the pass. Example usage (fastbuild of LLVM is required): opt -S --instcombine sample.ll --debug-counter=instcombine-visit-skip=0,instcombine-visit-count=42 Evaluating LLVM IR The LLVM tool lli evaluates LLVM IR. The tool expects the IR to include a entry function main . See the uploaded file in this LLVM bug for an example LLVM IR file which includes a main function that calls a (slightly modified) XLS-generated function. The LLVM tool opt optimizes the LLVM IR and can be piped to lli like so: opt sample.ll --O2 | lli Building LLVM at head Although the internal Google mirror of LLVM is updated frequently, prior to filing an LLVM bug it's a good idea to verify the failure against LLVM head. Steps to build: git clone https://github.com/llvm/llvm-project.git cd llvm mkdir build cd build cmake -G Ninja ../llvm/ cmake --build . -- opt Result miscomparison: simulated Verilog This can be a bug in codegen, XLS's Verilog testbench code, or the Verilog simulator itself. Running the generated Verilog with different simulators can help isolate the problem: simulate_module_main --signature_file=module_sig.textproto \\ --args_file=args.txt \\ --verilog_simulator=iverilog \\ sample.v simulate_module_main --signature_file=module_sig.textproto \\ --args_file=args.txt \\ --verilog_simulator=${SIM_2} \\ sample.v The tool outputs the results of the evaluation to stdout so diffing their outputs is required.","title":"Fuzzer"},{"location":"fuzzer/#xls-fuzzer","text":"XLS Fuzzer Crashers Directory Single-file reproducers IR minimization Summaries Debugging a failing sample Debugging a tool crash Result miscomparison: unoptimized IR Result miscomparison: optimized IR Debugging the LLVM JIT Building LLVM tools Isolating the bug Instcombine Evaluating LLVM IR Building LLVM at head Result miscomparison: simulated Verilog To execute the XLS fuzz driver simply run a command line like the following: bazel run -c opt \\ //xls/fuzzer:run_fuzz_multiprocess \\ -- --crash_path=/tmp/crashers-$(date +'%Y-%m-%d') --seed=0 --duration=8h The XLS fuzzer generates a sequence of randomly generated DSLX functions and a set of random inputs to each function often with interesting bit patterns. Given that stimulus, the fuzz driver performs the following actions some of which may be disabled/enabled via flags (run with --help for more details): Runs the DSLX program through the DSLX interpreter with the batch of arguments Converts the DSLX program to IR Optimizes the converted IR Interprets the pre-optimized and optimized IR with the batch of arguments Generates the Verilog from the IR with randomly selected codegen options Simulates the generated Verilog using the batch of arguments Performs a multi-way comparison of the DSLX interpreter results, the pre-optimized IR interpreter results, post-optimized IR interpreter results, and the simulator results If an issue is observed, the fuzz driver attempts to minimize the IR that causes an issue to occur. The above actions are coordinated and run by the SampleRunner class. Many actions are performed by invoking a separate binary which isolates any crashes. When miscompares in results occur or the generated function crashes part of XLS, all artifacts generated by the fuzzer for that sample are written into a uniquely-named subdirectory under the --crash_path given in the command line. The fuzzer also writes a crasher file which is a single file for reproducing the issue. See below for instructions on debugging a failing sample.","title":"XLS Fuzzer"},{"location":"fuzzer/#crashers-directory","text":"The crashers directory includes a subdirectory created for each failing sample. To avoid collisions the subdirectory is named using a hash of the DSLX code. Each crasher subdirectory has the following contents: $ ls /tmp/crashers-2019-06-25/05adbd50 args.txt ir_converter_main.stderr run.sh cl.txt ir_minimizer.options.json sample.ir crasher_2020-04-23_9b05.x ir_minimizer_test.sh sample.ir.results eval_ir_main.stderr options.json sample.x exception.txt opt_main.stderr sample.x.results The directory includes the problematic DSLX sample ( sample.x ) and the input arguments ( args.txt ) as well as all artifacts generated and stderr output emitted by the various utilities invoked to test the sample. Notable files include: options.json : Options used to run the sample. sample.ir : Unoptimized IR generated from the DSLX sample. sample.opt.ir : IR after optimizations. sample.v : Generated Verilog. *.results : The results (numeric values) produced by interpreting or simulating the respective input (DSLX, IR, or Verilog). exception.txt : The exception raised when running the sample. Typically this will indicate either a result miscomparison or a tool return non-zero status (for example, the IR optimizer crashed). crasher_*.x : A single file reproducer which includes the DSLX code, arguments, and options. See below for details. Typically the exact nature of the failure can be identified by reading the file exception.txt and possibly the stderr outputs of the various tools. The fuzzer can optionally produce a minimized IR reproduction of the problem. This will be written to minimized.ir . See below for details.","title":"Crashers Directory"},{"location":"fuzzer/#reproducers","text":"When the fuzzer encounters an issue it will create a single-file reproducer: --- Worker 14 observed an exception, noting --- Worker 14 noted crasher #1 for sampleno 42 at /tmp/crashers/095fb405 Copying that file to the directory //xls/fuzzer/crashers will automatically create a bazel test target for it and add it to the regression suite. Tests can also be added as known failures in //xls/fuzzer/build_defs.bzl as they're being triaged / investigated like so: generate_crasher_regression_tests( srcs = glob([\"crashers/*\"]), prefix = \"xls/fuzzer\", # TODO(xls-team): 2019-06-30 Triage and fix these. failing = [ \"crashers/crasher_2019-06-29_129987.x\", \"crashers/crasher_2019-06-29_402110.x\", ], ) Known-failures are marked as manual and excluded from continuous testing. To run the regression suite: bazel test //xls/fuzzer:all To run the regression suite including known-failures, run the regression target directly: bazel test //xls/fuzzer:regression_tests To reproduce from that single-file reproducer there is a command line tool: bazel run //xls/fuzzer:run_crasher -- \\ crasher_2019-06-26_3354.x","title":"Single-file reproducers"},{"location":"fuzzer/#minimization","text":"By default the fuzzer attempts to generate a minimal IR reproducer for the problem identified by the DSLX sample. Starting with the unoptimized IR the fuzzer invokes ir_minimizer_main to reduce the size of the input IR. It uses various simplification strategies to minimize the number of nodes in the IR. See the usage description in the tool source code for detailed information. The minimized IR is written to a file minimized.ir in the crasher directory for the sample. Note that minimization is only possible if the problem (crash, result miscomparison, etc.) occurs after conversion from DSLX to XLS IR.","title":"IR minimization"},{"location":"fuzzer/#summaries","text":"To monitor progress of the fuzzer and to determine op coverage the fuzzer can optionally (with --summary_path ) write summary information to files. The summary files are Protobuf files containing the proto SampleSummaryProto defined in //xls/fuzzer/sample_summary.proto . The summary information about the IR generated from the DSLX sample such as the number and type of each IR op as well as the bit width and number of operands. The summary information also includes a timing breakdown of the various operations performed for each sample (sample generation, IR conversion, etc). This can be used to identify performance bottlenecks in the fuzzer. The summaries can be read with the tool //xls/fuzzer/read_summary_main . See usage description in the code for more details.","title":"Summaries"},{"location":"fuzzer/#debugging","text":"A generated sample can fail in one of two ways: a tool crash or a result miscomparison. A tool crash occurred if one of the tools invoked by the fuzzer (e.g., opt_main which optimizes the IR) returned a non-zero status. A result miscomparison occurred if there is not perfect correspondence between the results produced by various ways in which the generated function is evaluated: Interpreted DSLX Evaluated unoptimized IR Evaluated optimized IR Simulation of the generated (System)Verilog Generally, the results produced by the interpretation of the DSLX serves are the reference results for comparisons. To identify the underlying cause of the sample failure inspect the exception.txt file in the crasher directory. The file contains the text of the exception raised in SampleRunner which clearly identifies the kind of failure (result miscomparison or tool crash) and details about which evaluation resulted in a miscompare or which tool crashed, respectively. Consult the following sections on how to debug particular kinds of failures.","title":"Debugging a failing sample"},{"location":"fuzzer/#debugging-a-tool-crash","text":"The exception.txt file includes the invocation of the tool for reproducing the failure. Generally, this is a straightforward debugging process. If the failing tool is the IR optimizer binary opt_main the particular pass causing the failure should be in the backtrace. To retrieve the input to this pass, run opt_main with --ir_dump_path to dump the IR between each pass. The last IR file produced (the files are numbered sequentially) is the input to the failing pass.","title":"Debugging a tool crash"},{"location":"fuzzer/#result-miscomparison-unoptimized-ir","text":"The evaluation of the unoptimized IR is the first point at which result comparison occurs (DSLX interpretation versus unoptimized IR evaluation). A miscomparison here can indicate a bug in one of several places: DSLX interpreter DSLX to IR conversion IR interpreter or IR JIT. The error message in exception.txt indicates whether the JIT or the interpreter was used. To help narrow this down, the IR interpreter can be compared against the JIT with the eval_ir_main tool: eval_ir_main --test_llvm_jit --input_file=args.txt sample.ir This runs both the JIT and the interpreter on the unoptimized IR file ( sample.ir ) using the arguments in args.txt and compares the results. If this is successful, then likely the IR interpreter and the JIT are correct and problem lies earlier in the pipeline (DSLX interpretation or DSLX to IR conversion). Otherwise, there is definitely a bug in either the interpreter or the JIT as their results should always be equal. If a minimized IR file exists ( minimized.ir ) this may be a better starting point for isolating the failure.","title":"Result miscomparison: unoptimized IR"},{"location":"fuzzer/#result-miscomparison-optimized-ir","text":"This can indicate a bug in IR evaluation (interpreter or JIT) or in the optimizer. In this case, a comparison of the evaluation of the unoptimized IR against the DSLX interpreter has already succeeds so DSLX interpretation or conversion is unlikely to be the underlying cause. As with miscomparison involving the unoptimized IR, eval_ir_main can be used to compare the JIT results against the interpreter results: eval_ir_main --test_llvm_jit --input_file=args.txt sample.opt.ir If the above invocation fails there is a bug in the JIT or the interpreter. Otherwise, there may be a bug in the optimizer. The tool eval_ir_main can help isolate the problematic optimization pass by running with the options --optimize_ir and --eval_after_each_pass . With these flags, the tool runs the optimization pipeline on the given IR and evaluates the IR after each pass is run. The first pass which results in a miscompare against the unoptimized input IR is flagged. Invocation: eval_ir_main --input_file=args.txt \\ --optimize_ir \\ --eval_after_each_pass \\ sample.ir","title":"Result miscomparison: optimized IR"},{"location":"fuzzer/#debugging-the-llvm-jit","text":"To help isolate bugs in the JIT, LLVM's optimization level can be set using the --llvm_opt_level flag: eval_ir_main --test_llvm_jit \\ --llvm_opt_level=0 \\ --input_file=args.txt sample.opt.ir If the results match (pass) with the optimization level set to zero but fail with the default optimization level of 3, there is likely a bug in the LLVM optimizer or the XLS-generated LLVM program has undefined behavior. Unoptimized and optimized LLVM IR are dumped by the JIT with vlog level of 2 or higher, and the assembly is dumped at level 3 or higher. For example: eval_ir_main -v=3 --logtostderr sample.opt.ir Extract the unoptimized LLVM IR to file to enable working with it in isolation.","title":"Debugging the LLVM JIT"},{"location":"fuzzer/#building-llvm-tools","text":"The various LLVM tools such as opt and lli can be built with: bazel build /llvm/llvm-project/llvm:all Build in fastbuild mode to get checks and debug features in LLVM.","title":"Building LLVM tools"},{"location":"fuzzer/#isolating-the-bug","text":"To inspect the optimized LLVM IR, run: opt sample.ll -O3 -S To print the IR before and after each pass: opt sample.ll -S -print-after-all -print-before-all -O3","title":"Isolating the bug"},{"location":"fuzzer/#instcombine","text":"Instcombine is an LLVM optimization pass which is a common source of bugs in code generated from XLS because XLS uses unusual bit widths which exercise little used paths in this optimization. To run instcombine alone: opt /tmp/bad.ll -S --instcombine Instcombine is a large monolithic pass and it can be difficult to isolate the exact transformation which caused the problem. Fortunately, this pass includes a \"compiler fuel\" option which can be used to limit the number of transformations performed by the pass. Example usage (fastbuild of LLVM is required): opt -S --instcombine sample.ll --debug-counter=instcombine-visit-skip=0,instcombine-visit-count=42","title":"Instcombine"},{"location":"fuzzer/#evaluating-llvm-ir","text":"The LLVM tool lli evaluates LLVM IR. The tool expects the IR to include a entry function main . See the uploaded file in this LLVM bug for an example LLVM IR file which includes a main function that calls a (slightly modified) XLS-generated function. The LLVM tool opt optimizes the LLVM IR and can be piped to lli like so: opt sample.ll --O2 | lli","title":"Evaluating LLVM IR"},{"location":"fuzzer/#building-llvm-at-head","text":"Although the internal Google mirror of LLVM is updated frequently, prior to filing an LLVM bug it's a good idea to verify the failure against LLVM head. Steps to build: git clone https://github.com/llvm/llvm-project.git cd llvm mkdir build cd build cmake -G Ninja ../llvm/ cmake --build . -- opt","title":"Building LLVM at head"},{"location":"fuzzer/#result-miscomparison-simulated-verilog","text":"This can be a bug in codegen, XLS's Verilog testbench code, or the Verilog simulator itself. Running the generated Verilog with different simulators can help isolate the problem: simulate_module_main --signature_file=module_sig.textproto \\ --args_file=args.txt \\ --verilog_simulator=iverilog \\ sample.v simulate_module_main --signature_file=module_sig.textproto \\ --args_file=args.txt \\ --verilog_simulator=${SIM_2} \\ sample.v The tool outputs the results of the evaluation to stdout so diffing their outputs is required.","title":"Result miscomparison: simulated Verilog"},{"location":"ideas_and_projects/","text":"Ideas and Projects This document lists a few sample ideas, projects, and research ideas, to help get started on contributing to XLS. Programming Languages XLS New Frontends One of XLS' primary core focus areas was defining a compiler intermediate representation that is powerful enough to express all required concepts, but minimal enough to make it easy to target from other, new, or modified programming languages. We are focusing on a functional domain-specific language, but others are possible. There is work to target the IR from C++. There are many other research systems out there with their own respective DSLs and other input mechanisms. It would be interesting to connect those to allow comparisons. An embedded DSL in Python could be developed, which is straightforward as the IR's builder interfaces are already exported to Python. Core XLS ML for Delay Estimation We currently estimate the delay of ops and op combos via benchmarks and the theory of logical efforts. In principle we are trying to guess what the commercial toolchains will do. This is a problem that seems to be just made for ML, especially for new technology nodes or FPGA devices. Delay Estimation for a variety of Devices We are focusing on only a small number of FPGAs and very specific ASIC Flows. For the lager community out there we should add many more models, improve automation of deriving a delay model, and/or try ML based approaches. Delay Estimation for Implicit Broadcasts HLS often creates implicit broadcasts (wire load / fan out / wiring congestion) in unrolled loops, deep pipelines, large memory blocks, etc. that lead to frequency bottlenecks. Modeling these broadcasts in the delay model can help mitigate or even completely solve such problems. Z3 We use Z3 for our logical equivalence checking, eg., to check that the optimized and unoptimized IR have the same semantics. With a solver like this in place, there are many more opportunities to apply it or make it more practical, for example. (Automatic) Partitioning of the input IR to reduce per-phase problem space Add / compare with other formal verification tools. There are other alternatives, e.g. Boolector Use cases we haven't thought of yet. Design Verification Flows Provide mechanisms for \"constrained random\" approaches to hardware verification -- either libraries to emulate capabilities of constraint-based vector generation similar to UVM, or more automated approaches provided by QuickCheck/Hypothesis. Implement new blocks and functions in XLS Extend XLS standard library XLS has a small standard library which includes some basic utility functions and some limited floating point support. Extending this library and developing more complex functionality would improve the usability of XLS. Ideas: FP Libraries Implement libraries for important FP operations, modes, widths, with all the relevant parameters. BFloat libraries Fixed point libraries, similar to above Exploit FPGA hard macros and BRAMs XLS implementations of common hardware libraries - arbiters - counters - encoders - fifos Three-stage RISC-V Piccolo is a 3-stage RISC-V core. Implementation appears of reasonable size. IF: ISA / Instruction Fetch / Decode Unit DM: Data Memory stage WB: Write Back stage ALU add/sub unit, mul unit, shifter unit PLIC / Platform Level Interrupts architecture Cache Hierarchy with various sizes, associativity, and replacement policies MMU and L1-Cache CSRs Register File Support for Systolic Arrays as a \"Parallel Pattern\" It is clear, especially in the domain of machine learning and signal processing, that HLS will benefit from full support of 2D elements, such as systolic arrays or specialized convolution engines. The idea here is to abstract these properly. For example, one could add constructs to the IR, or we can extend the analysis capabilities and add passes to construct them as needed. We also have to make sure that the mechanisms are fully supported by all optimization and code generation passes. This is a fascinating and wide-open research area and intersects with the current work on concurrent blocks. Interoperability with other languages, FFI Users may want to reuse building blocks written in other languages that are pre-optimized. Support proper FFI. Instantiate external blocks by specifying their properties in a way the scheduler can understand. Import type definitions from system verilog descriptions e.g. as encoded in protobufs XLS Tools Visualization XLS includes some minimal visualization and exploration tools. There is large potential for tool builders to improve and add information, insights, suggestions, etc. Source Correlation It is always important to maintain a high level of productivity and utility for a toolset like XLS provides. Source correlation, annotations, and many other techniques are always improvable the enhance the debugging, visualization, and also design verification experiences. Source Analysis A linter/style checker for the DSL would be very helpful for users to write high-quality HLS code. XLS Integration with other tools Add Verilator support to XLS Verilator is a strong open-source (System)Verilog simulator. Currently XLS only supports Icarus Verilog, a Verilog-only simulator. Verilator support in XLS will provide much higher simulation performance and SystemVerilog support to open source users. Unlike other simulators which can run testbench code, Verilator only operates on synthesizable Verilog and emits C++ code for compilation and execution. This unique flow needs to be integrating into XLS's simulation framework.","title":"Ideas and Projects"},{"location":"ideas_and_projects/#ideas-and-projects","text":"This document lists a few sample ideas, projects, and research ideas, to help get started on contributing to XLS.","title":"Ideas and Projects"},{"location":"ideas_and_projects/#programming-languages","text":"","title":"Programming Languages"},{"location":"ideas_and_projects/#xls-new-frontends","text":"One of XLS' primary core focus areas was defining a compiler intermediate representation that is powerful enough to express all required concepts, but minimal enough to make it easy to target from other, new, or modified programming languages. We are focusing on a functional domain-specific language, but others are possible. There is work to target the IR from C++. There are many other research systems out there with their own respective DSLs and other input mechanisms. It would be interesting to connect those to allow comparisons. An embedded DSL in Python could be developed, which is straightforward as the IR's builder interfaces are already exported to Python.","title":"XLS New Frontends"},{"location":"ideas_and_projects/#core-xls","text":"","title":"Core XLS"},{"location":"ideas_and_projects/#ml-for-delay-estimation","text":"We currently estimate the delay of ops and op combos via benchmarks and the theory of logical efforts. In principle we are trying to guess what the commercial toolchains will do. This is a problem that seems to be just made for ML, especially for new technology nodes or FPGA devices.","title":"ML for Delay Estimation"},{"location":"ideas_and_projects/#delay-estimation-for-a-variety-of-devices","text":"We are focusing on only a small number of FPGAs and very specific ASIC Flows. For the lager community out there we should add many more models, improve automation of deriving a delay model, and/or try ML based approaches.","title":"Delay Estimation for a variety of Devices"},{"location":"ideas_and_projects/#delay-estimation-for-implicit-broadcasts","text":"HLS often creates implicit broadcasts (wire load / fan out / wiring congestion) in unrolled loops, deep pipelines, large memory blocks, etc. that lead to frequency bottlenecks. Modeling these broadcasts in the delay model can help mitigate or even completely solve such problems.","title":"Delay Estimation for Implicit Broadcasts"},{"location":"ideas_and_projects/#z3","text":"We use Z3 for our logical equivalence checking, eg., to check that the optimized and unoptimized IR have the same semantics. With a solver like this in place, there are many more opportunities to apply it or make it more practical, for example. (Automatic) Partitioning of the input IR to reduce per-phase problem space Add / compare with other formal verification tools. There are other alternatives, e.g. Boolector Use cases we haven't thought of yet.","title":"Z3"},{"location":"ideas_and_projects/#design-verification-flows","text":"Provide mechanisms for \"constrained random\" approaches to hardware verification -- either libraries to emulate capabilities of constraint-based vector generation similar to UVM, or more automated approaches provided by QuickCheck/Hypothesis.","title":"Design Verification Flows"},{"location":"ideas_and_projects/#implement-new-blocks-and-functions-in-xls","text":"","title":"Implement new blocks and functions in XLS"},{"location":"ideas_and_projects/#extend-xls-standard-library","text":"XLS has a small standard library which includes some basic utility functions and some limited floating point support. Extending this library and developing more complex functionality would improve the usability of XLS. Ideas: FP Libraries Implement libraries for important FP operations, modes, widths, with all the relevant parameters. BFloat libraries Fixed point libraries, similar to above Exploit FPGA hard macros and BRAMs XLS implementations of common hardware libraries - arbiters - counters - encoders - fifos","title":"Extend XLS standard library"},{"location":"ideas_and_projects/#three-stage-risc-v","text":"Piccolo is a 3-stage RISC-V core. Implementation appears of reasonable size. IF: ISA / Instruction Fetch / Decode Unit DM: Data Memory stage WB: Write Back stage ALU add/sub unit, mul unit, shifter unit PLIC / Platform Level Interrupts architecture Cache Hierarchy with various sizes, associativity, and replacement policies MMU and L1-Cache CSRs Register File","title":"Three-stage RISC-V"},{"location":"ideas_and_projects/#support-for-systolic-arrays-as-a-parallel-pattern","text":"It is clear, especially in the domain of machine learning and signal processing, that HLS will benefit from full support of 2D elements, such as systolic arrays or specialized convolution engines. The idea here is to abstract these properly. For example, one could add constructs to the IR, or we can extend the analysis capabilities and add passes to construct them as needed. We also have to make sure that the mechanisms are fully supported by all optimization and code generation passes. This is a fascinating and wide-open research area and intersects with the current work on concurrent blocks.","title":"Support for Systolic Arrays as a \"Parallel Pattern\""},{"location":"ideas_and_projects/#interoperability-with-other-languages-ffi","text":"Users may want to reuse building blocks written in other languages that are pre-optimized. Support proper FFI. Instantiate external blocks by specifying their properties in a way the scheduler can understand. Import type definitions from system verilog descriptions e.g. as encoded in protobufs","title":"Interoperability with other languages, FFI"},{"location":"ideas_and_projects/#xls-tools","text":"","title":"XLS Tools"},{"location":"ideas_and_projects/#visualization","text":"XLS includes some minimal visualization and exploration tools. There is large potential for tool builders to improve and add information, insights, suggestions, etc.","title":"Visualization"},{"location":"ideas_and_projects/#source-correlation","text":"It is always important to maintain a high level of productivity and utility for a toolset like XLS provides. Source correlation, annotations, and many other techniques are always improvable the enhance the debugging, visualization, and also design verification experiences.","title":"Source Correlation"},{"location":"ideas_and_projects/#source-analysis","text":"A linter/style checker for the DSL would be very helpful for users to write high-quality HLS code.","title":"Source Analysis"},{"location":"ideas_and_projects/#xls-integration-with-other-tools","text":"","title":"XLS Integration with other tools"},{"location":"ideas_and_projects/#add-verilator-support-to-xls","text":"Verilator is a strong open-source (System)Verilog simulator. Currently XLS only supports Icarus Verilog, a Verilog-only simulator. Verilator support in XLS will provide much higher simulation performance and SystemVerilog support to open source users. Unlike other simulators which can run testbench code, Verilator only operates on synthesizable Verilog and emits C++ code for compilation and execution. This unique flow needs to be integrating into XLS's simulation framework.","title":"Add Verilator support to XLS"},{"location":"interpreters/","text":"Interpreters XLS provides a several interpreters to assist in design validation across our functional stack, from input DSLX down to the netlist level. Interpreters DSLX Execution comparison IR Netlists DSLX The DSLX interpreter ( //xls/dslx:interpreter_main ) operates on DSLX .x files that contain both the design and unit tests to execute (present as #[test] annotated functions). The adler32 example demonstrates this: the design is encapsulated in the main , adler32_seq , and mod functions, and the samples are present in the test adler32_one_char (note that unit-style tests/interpretations of adler32_seq and mod could also be present). Interpreter targets are automatically generated for dslx_test() targets, so no special declarations are necessary to wrap DSLX code. To invoke these samples, execute the following: bazel build -c opt //xls/examples:adler32_dslx_test ./bazel-bin/xls/examples/adler32_dslx_test To execute directly via the interpreter, you can instead run: $ bazel build -c opt //xls/dslx/interpreter_main $ ./bazel-bin/xls/dslx/interpreter_main \\ ./xls/examples/adler32.x These two methods are equivalent. Execution comparison The DSL interpreter provides a flag, --compare , to implicitly compare its run results to those of the IR-converted DSL functions. This helps \"spot check\" consistency between IR and DSL execution (in addition to other methods used in more generally in XLS, like the fuzzer). The user may compare DSL execution to IR interpreter execution, IR JIT execution, or not perform IR comparison at all. $ ./bazel-bin/xls/dslx/interpreter_main \\ ./xls/examples/adler32.x --compare = jit $ ./bazel-bin/xls/dslx/interpreter_main \\ ./xls/examples/adler32.x --compare = interpreter $ ./bazel-bin/xls/dslx/interpreter_main \\ ./xls/examples/adler32.x --compare = none IR XLS provides two means of evaluating IR - interpretation and native host compilation (the JIT ). Both are invoked in nearly the same way, via the eval_ir_main tool. eval_ir_main supports a wide number of use cases, but the most common end-user case will be to run a sample through a design. To evaluate a sample (1.0 + 2.5) on the floating-point adder , one would run the following: bazel build -c opt //xls/tools:eval_ir_main ./bazel-bin/xls/tools/eval_ir_main \\ --input '(bits[1]: 0x0, bits[8]:0x7F, bits[23]:0x0); (bits[1]: 0x0, bits[8]:0x80, bits[23]:0x200000)' \\ ./xls/modules/fp/fp32_add_2.x By default, this runs via the JIT. To use the interpreter, add the --use_llvm_jit=false flag to the invocation. eval_ir_main supports a broad set of options and modes of execution. Refer to its [very thorough] --help documentation for full details. Netlists Finally, compiled netlists can also be interpreted against input samples via the aptly-named netlist_interpreter_main tool. This tool currently only supports single sample evaluation (as illustrated in the IR section above): bazel build -c opt //xls/tools:netlist_interpreter_main ./bazel-bin/xls/tools/netlist_interpreter_main \\ --netlist <path to netlist> --module <module to evaluate> --cell_library[_proto] <path to the module's cell library [proto]> --inputs <input sample, as above> As XLS does not currently provide an sample/example netlist (TODO(rspringer)), concrete values can't [yet] be provided here. The --cell_library flag merits extra discussion, though. During netlist compilation, a cell library is provided to indicate the individual logic cells available for the design, and these cells are referenced in the output netlist. The interpreter needs a description of these cells' behaviors/functions, so the cell library must be provided here, as well. Many cell libraries are very large (> 1GB), and can thus incur significant processing overhead at startup, so we also accept pre-processed cell libraries, as CellLibraryProto messages, that contain much-abridged cell descriptions. The function_extractor_main tool can automatically perform this extraction for Liberty -formatted cell library descriptions.","title":"Interpreters"},{"location":"interpreters/#interpreters","text":"XLS provides a several interpreters to assist in design validation across our functional stack, from input DSLX down to the netlist level. Interpreters DSLX Execution comparison IR Netlists","title":"Interpreters"},{"location":"interpreters/#dslx","text":"The DSLX interpreter ( //xls/dslx:interpreter_main ) operates on DSLX .x files that contain both the design and unit tests to execute (present as #[test] annotated functions). The adler32 example demonstrates this: the design is encapsulated in the main , adler32_seq , and mod functions, and the samples are present in the test adler32_one_char (note that unit-style tests/interpretations of adler32_seq and mod could also be present). Interpreter targets are automatically generated for dslx_test() targets, so no special declarations are necessary to wrap DSLX code. To invoke these samples, execute the following: bazel build -c opt //xls/examples:adler32_dslx_test ./bazel-bin/xls/examples/adler32_dslx_test To execute directly via the interpreter, you can instead run: $ bazel build -c opt //xls/dslx/interpreter_main $ ./bazel-bin/xls/dslx/interpreter_main \\ ./xls/examples/adler32.x These two methods are equivalent.","title":"DSLX"},{"location":"interpreters/#execution-comparison","text":"The DSL interpreter provides a flag, --compare , to implicitly compare its run results to those of the IR-converted DSL functions. This helps \"spot check\" consistency between IR and DSL execution (in addition to other methods used in more generally in XLS, like the fuzzer). The user may compare DSL execution to IR interpreter execution, IR JIT execution, or not perform IR comparison at all. $ ./bazel-bin/xls/dslx/interpreter_main \\ ./xls/examples/adler32.x --compare = jit $ ./bazel-bin/xls/dslx/interpreter_main \\ ./xls/examples/adler32.x --compare = interpreter $ ./bazel-bin/xls/dslx/interpreter_main \\ ./xls/examples/adler32.x --compare = none","title":"Execution comparison"},{"location":"interpreters/#ir","text":"XLS provides two means of evaluating IR - interpretation and native host compilation (the JIT ). Both are invoked in nearly the same way, via the eval_ir_main tool. eval_ir_main supports a wide number of use cases, but the most common end-user case will be to run a sample through a design. To evaluate a sample (1.0 + 2.5) on the floating-point adder , one would run the following: bazel build -c opt //xls/tools:eval_ir_main ./bazel-bin/xls/tools/eval_ir_main \\ --input '(bits[1]: 0x0, bits[8]:0x7F, bits[23]:0x0); (bits[1]: 0x0, bits[8]:0x80, bits[23]:0x200000)' \\ ./xls/modules/fp/fp32_add_2.x By default, this runs via the JIT. To use the interpreter, add the --use_llvm_jit=false flag to the invocation. eval_ir_main supports a broad set of options and modes of execution. Refer to its [very thorough] --help documentation for full details.","title":"IR"},{"location":"interpreters/#netlists","text":"Finally, compiled netlists can also be interpreted against input samples via the aptly-named netlist_interpreter_main tool. This tool currently only supports single sample evaluation (as illustrated in the IR section above): bazel build -c opt //xls/tools:netlist_interpreter_main ./bazel-bin/xls/tools/netlist_interpreter_main \\ --netlist <path to netlist> --module <module to evaluate> --cell_library[_proto] <path to the module's cell library [proto]> --inputs <input sample, as above> As XLS does not currently provide an sample/example netlist (TODO(rspringer)), concrete values can't [yet] be provided here. The --cell_library flag merits extra discussion, though. During netlist compilation, a cell library is provided to indicate the individual logic cells available for the design, and these cells are referenced in the output netlist. The interpreter needs a description of these cells' behaviors/functions, so the cell library must be provided here, as well. Many cell libraries are very large (> 1GB), and can thus incur significant processing overhead at startup, so we also accept pre-processed cell libraries, as CellLibraryProto messages, that contain much-abridged cell descriptions. The function_extractor_main tool can automatically perform this extraction for Liberty -formatted cell library descriptions.","title":"Netlists"},{"location":"ir_jit/","text":"IR JIT Compiler IR JIT Compiler Usage Specialized matching Direct usage Design Arg passing ArrayIndex main() generator Usage Design Output type determination XLS provides a JIT compiler for evaluating functions written in the [XLS] compiler intermediate representation (IR) at native machine speed. Usage Given a DSLX file and build target, one can build and run it through the JIT by: Declaring a cc_xls_ir_jit_wrapper target matching the DSLX build target. Creating a JIT object and calling its Run() method. Using the 2-way floating-point adder as an example: # include \"xls/modules/fpadd_2x24_jit_wrapper.h\" absl :: StatusOr < Value > foo ( Value a , Value b ) { ... // Only create this once and re-use it; it's created here just as // an illustration. XLS_ASSIGN_OR_RETURN ( Fpadd2x32 adder , Fpadd2x32 :: Create ()); return adder . Run ( a , b ); } The advantages of JIT compilation (or any compilation, for that matter) only come into play when repeatedly using the compiled object, so programs should be structured to create a JIT wrapper once and to reuse it many times, e.g., to test a module across many - or even exhaustively, across all possible - inputs. Specialized matching In many cases, the types used by DSLX designs map to native types, such as the C/C++ float . In that case, a simplified wrapper call is available: #include \"xls/modules/fpadd_2x24_jit_wrapper.h\" absl::StatusOr<float> foo(float a, float b) { ... // Only create this once and re-use it; it's created here just as // an illustration. XLS_ASSIGN_OR_RETURN(Fpadd2x32 adder, Fpadd2x32::Create()); return adder.Run(a, b); } When available, these simplified wrappers should be used for higher performance (~30% in our measured cases). Currently, floats and integral types >= 64 bits in this way. For non-native integral types, the generated wrapper will accept the next larger native type e.g., uint64_t for bits[47] . Proper operation with next-larger types depends on the input value being present in the least-significant bits of the containing type. These are higher performance because they avoid unnecessary marshaling of these types into Views (e.g., a float outside the JIT -> View -> float inside the JIT). Direct usage The JIT is also available as a library with a straightforward interface: absl::StatusOr<Value> RunOnJit( Function* function, absl::Span<const Value> args) { XLS_ASSIGN_OR_RETURN(auto jit, FunctionJit::Create(function)); return jit->Run(args); } The IR JIT is the default backend for the eval_ir_main tool, which loads IR from disk and runs with args present on either the command line or in a specified file. Design Internally, the JIT converts XLS IR to LLVM IR and uses LLVM's ORC infrastructure to convert that into native machine code. The details of compiling an LLVM IR program with ORC are mostly generic and are available online - here are discussed details specific to our usage in XLS. XLS IR is converted to LLVM IR by recursively visiting every node in a function using the DfsVisitor functions. Most nodes have relatively straightforward implementations, e.g., in Concat, we create an empty value with the combined width of the operands, and each is shifted and blitted into that value to produce the result. Some operations, though, merit more discussion. Arg passing When LLVM JIT-compiles a program, the resulting value is simply a function pointer to the requested entry point (that can be called like any function pointer). Calling such a function pointer with concrete-typed arguments, though, is difficult: one must either make heavy [ab]use of C++ templates or \"hide\" the argument types behind an opaque pointer. The latter approach is taken here. When a compiled function is invoked (via FunctionJit::Run() ), the typed input args are \"packed\" into an opaque byte buffer which is passed into the new function. Inside there, any references to an argument (via DfsVisitor::HandleParam() ) calculate the offset of that param in the opaque buffer and load from there appropriately (this should happen at most once per arg; LLVM and/or XLS should optimize away redundant loads). A special case is for function invocations (inside the JITted function): for these, the arguments already exist inside \"LLVM-space\", so there's no need for unpacking args, so LlvmFunction::getArg() can be used as usual. Results must be handled in a similar way - they could be of any type and will need to be packed inside XLS types before returning, so there's a corresponding argument unpacking phase at function exit. For both packing and unpacking, LLVM's DataLayout must be used to determining where input and output values will be placed, as LLVM will use those conventions when, e.g., loading values from a struct. ArrayIndex IRBuilder provides three means of extracting values from an aggregate type: CreateGEP : these use the getelementptr instruction, which requires a pointer-typed value (not the same thing as an array!). This requires holding a value in a specially-created allocation (via CreateAlloca() or in an input buffer). CreateExtractElement : returns the value at a given index in a vector -typed value. CreateExtractValue : returns the value at a given constant index in an aggregate value. Unfortunately, #2 doesn't apply, as arrays aren't LLVM vectors, and #3 doesn't apply, as an array index isn't necessarily a constant value. Uniformly managing arrays as allocas doesn't scale well (consider the case of arrays of arrays of tuples...), so for ArrayIndex nodes, we lazily create allocas for only the array of interest and load the requested index from there. main() generator The IR JIT finds more than its share of LLVM bugs, in large part due to XLS' use of fuzzing, which often generates bit widths not often found in software, e.g. a 231-bit wide integer, which won't be emitted by Clang (as it's not a native C type). To ensure that any mismatches between the JIT and the IR interpreter are correctly triaged, it's important to compare results between the JIT and LLVM-provided tools, e.g., lli or llc, the LLVM IR interpreter and compiler, respectively. Both lli and llc execute self-contained LLVM IR programs, i.e., those with an int main(int argc, char** argv) entry point - the JIT does not produce these (as they're not part of a hardware description). To avoid the need to manually edit the JIT-produced IR (which experience has shown to be very error prone), we can generate a main driver function for our samples. Usage To generate and execute a main for LLVM IR produced by the JIT, run the following: $ bazel build //xls/tools:llvm_main_generator \\ //xls/tools:run_llvm_main $ ./bazel-bin/xls/tools/llvm_main_generator \\ -entry_function <Mangled IR function name> \\ -input <Path to file containing JIT-generated LLVM IR> \\ -output <Path to write output> $ ./bazel-bin/xls/tools/run_llvm_main \\ <Output path from above> \\ <Input values as string-formatted XLS Values> For a concrete example: $ bazel-bin/xls/tools/llvm_main_generator \\ -entry_function \"sample::__sample__main\" \\ -input ~/fuzz/mismatch/sample.opt.ll \\ -output ./foo.ll $ bazel-bin/xls/tools/run_llvm_main \\ ./foo.ll \\ bits[56]:0x800_0000_0000 \\ bits[66]:0x4000_0000 \\ bits[7]:0x1 \\ bits[2]:0x3 \\ bits[12]:0x0 \\ bits[74]:0x3c5_482a_0984_e061_1a90 bits[74]:0x3ff_ffc5_482a_0984_e061 The final line is the result of running the sample. If the value mismatch occurs both in the IR JIT evaluation as well as lli evaluation, then there's likely a bug in LLVM. A main-annotated IR sample is suitable for attaching to an LLVM bug report (on http://bugs.llvm.org) as a reproducer, along with the input to generate the mismatch. Design The main generator, at a high level, uses the LLVM tools (namely IRBuilder) to create a main() function to: Examine its command-line parameters to determine their overall size and to them into the input buffer (as in Arg Passing above). Invoke the entry function (which remains unchanged from the source emitted by the JIT). Unpack the output buffer after the entry function completes and print its contents. For arg packing/unpacking, the functions provided by JitRuntime are used, but in a stateless context wrapped in an extern \"C\" space, to simplify invocation from within LLVM IR. Output type determination Inferring the computation's output type merits special discussion. While it's trivial for the main generator to detect the output type, it's very difficult to do so inside the executing main() - in the former, the type is real data, wherein the latter, it's metadata. To make this possible (without doing down an RTTI rabbit hole, or even simply requiring the user to specify it on the command line), we do the following: Determine the llvm::Type of the output. Convert that into an xls::Type , and capture that type as a String. Hardcode that string as a constant in the emitted main() function. At runtime, pass that type string into UnpackAndPrintBuffer() (one of the extern \"C\" function wrappers). Inside UnpackAndPrintBuffer() , parse that string (via xls::Parser::ParseType() ), and use the resulting xls::Type to determine the contents of the computation's output buffer.","title":"Overview"},{"location":"ir_jit/#ir-jit-compiler","text":"IR JIT Compiler Usage Specialized matching Direct usage Design Arg passing ArrayIndex main() generator Usage Design Output type determination XLS provides a JIT compiler for evaluating functions written in the [XLS] compiler intermediate representation (IR) at native machine speed.","title":"IR JIT Compiler"},{"location":"ir_jit/#usage","text":"Given a DSLX file and build target, one can build and run it through the JIT by: Declaring a cc_xls_ir_jit_wrapper target matching the DSLX build target. Creating a JIT object and calling its Run() method. Using the 2-way floating-point adder as an example: # include \"xls/modules/fpadd_2x24_jit_wrapper.h\" absl :: StatusOr < Value > foo ( Value a , Value b ) { ... // Only create this once and re-use it; it's created here just as // an illustration. XLS_ASSIGN_OR_RETURN ( Fpadd2x32 adder , Fpadd2x32 :: Create ()); return adder . Run ( a , b ); } The advantages of JIT compilation (or any compilation, for that matter) only come into play when repeatedly using the compiled object, so programs should be structured to create a JIT wrapper once and to reuse it many times, e.g., to test a module across many - or even exhaustively, across all possible - inputs.","title":"Usage"},{"location":"ir_jit/#specialized-matching","text":"In many cases, the types used by DSLX designs map to native types, such as the C/C++ float . In that case, a simplified wrapper call is available: #include \"xls/modules/fpadd_2x24_jit_wrapper.h\" absl::StatusOr<float> foo(float a, float b) { ... // Only create this once and re-use it; it's created here just as // an illustration. XLS_ASSIGN_OR_RETURN(Fpadd2x32 adder, Fpadd2x32::Create()); return adder.Run(a, b); } When available, these simplified wrappers should be used for higher performance (~30% in our measured cases). Currently, floats and integral types >= 64 bits in this way. For non-native integral types, the generated wrapper will accept the next larger native type e.g., uint64_t for bits[47] . Proper operation with next-larger types depends on the input value being present in the least-significant bits of the containing type. These are higher performance because they avoid unnecessary marshaling of these types into Views (e.g., a float outside the JIT -> View -> float inside the JIT).","title":"Specialized matching"},{"location":"ir_jit/#direct-usage","text":"The JIT is also available as a library with a straightforward interface: absl::StatusOr<Value> RunOnJit( Function* function, absl::Span<const Value> args) { XLS_ASSIGN_OR_RETURN(auto jit, FunctionJit::Create(function)); return jit->Run(args); } The IR JIT is the default backend for the eval_ir_main tool, which loads IR from disk and runs with args present on either the command line or in a specified file.","title":"Direct usage"},{"location":"ir_jit/#design","text":"Internally, the JIT converts XLS IR to LLVM IR and uses LLVM's ORC infrastructure to convert that into native machine code. The details of compiling an LLVM IR program with ORC are mostly generic and are available online - here are discussed details specific to our usage in XLS. XLS IR is converted to LLVM IR by recursively visiting every node in a function using the DfsVisitor functions. Most nodes have relatively straightforward implementations, e.g., in Concat, we create an empty value with the combined width of the operands, and each is shifted and blitted into that value to produce the result. Some operations, though, merit more discussion.","title":"Design"},{"location":"ir_jit/#arg-passing","text":"When LLVM JIT-compiles a program, the resulting value is simply a function pointer to the requested entry point (that can be called like any function pointer). Calling such a function pointer with concrete-typed arguments, though, is difficult: one must either make heavy [ab]use of C++ templates or \"hide\" the argument types behind an opaque pointer. The latter approach is taken here. When a compiled function is invoked (via FunctionJit::Run() ), the typed input args are \"packed\" into an opaque byte buffer which is passed into the new function. Inside there, any references to an argument (via DfsVisitor::HandleParam() ) calculate the offset of that param in the opaque buffer and load from there appropriately (this should happen at most once per arg; LLVM and/or XLS should optimize away redundant loads). A special case is for function invocations (inside the JITted function): for these, the arguments already exist inside \"LLVM-space\", so there's no need for unpacking args, so LlvmFunction::getArg() can be used as usual. Results must be handled in a similar way - they could be of any type and will need to be packed inside XLS types before returning, so there's a corresponding argument unpacking phase at function exit. For both packing and unpacking, LLVM's DataLayout must be used to determining where input and output values will be placed, as LLVM will use those conventions when, e.g., loading values from a struct.","title":"Arg passing"},{"location":"ir_jit/#arrayindex","text":"IRBuilder provides three means of extracting values from an aggregate type: CreateGEP : these use the getelementptr instruction, which requires a pointer-typed value (not the same thing as an array!). This requires holding a value in a specially-created allocation (via CreateAlloca() or in an input buffer). CreateExtractElement : returns the value at a given index in a vector -typed value. CreateExtractValue : returns the value at a given constant index in an aggregate value. Unfortunately, #2 doesn't apply, as arrays aren't LLVM vectors, and #3 doesn't apply, as an array index isn't necessarily a constant value. Uniformly managing arrays as allocas doesn't scale well (consider the case of arrays of arrays of tuples...), so for ArrayIndex nodes, we lazily create allocas for only the array of interest and load the requested index from there.","title":"ArrayIndex"},{"location":"ir_jit/#main-generator","text":"The IR JIT finds more than its share of LLVM bugs, in large part due to XLS' use of fuzzing, which often generates bit widths not often found in software, e.g. a 231-bit wide integer, which won't be emitted by Clang (as it's not a native C type). To ensure that any mismatches between the JIT and the IR interpreter are correctly triaged, it's important to compare results between the JIT and LLVM-provided tools, e.g., lli or llc, the LLVM IR interpreter and compiler, respectively. Both lli and llc execute self-contained LLVM IR programs, i.e., those with an int main(int argc, char** argv) entry point - the JIT does not produce these (as they're not part of a hardware description). To avoid the need to manually edit the JIT-produced IR (which experience has shown to be very error prone), we can generate a main driver function for our samples.","title":"main() generator"},{"location":"ir_jit/#usage_1","text":"To generate and execute a main for LLVM IR produced by the JIT, run the following: $ bazel build //xls/tools:llvm_main_generator \\ //xls/tools:run_llvm_main $ ./bazel-bin/xls/tools/llvm_main_generator \\ -entry_function <Mangled IR function name> \\ -input <Path to file containing JIT-generated LLVM IR> \\ -output <Path to write output> $ ./bazel-bin/xls/tools/run_llvm_main \\ <Output path from above> \\ <Input values as string-formatted XLS Values> For a concrete example: $ bazel-bin/xls/tools/llvm_main_generator \\ -entry_function \"sample::__sample__main\" \\ -input ~/fuzz/mismatch/sample.opt.ll \\ -output ./foo.ll $ bazel-bin/xls/tools/run_llvm_main \\ ./foo.ll \\ bits[56]:0x800_0000_0000 \\ bits[66]:0x4000_0000 \\ bits[7]:0x1 \\ bits[2]:0x3 \\ bits[12]:0x0 \\ bits[74]:0x3c5_482a_0984_e061_1a90 bits[74]:0x3ff_ffc5_482a_0984_e061 The final line is the result of running the sample. If the value mismatch occurs both in the IR JIT evaluation as well as lli evaluation, then there's likely a bug in LLVM. A main-annotated IR sample is suitable for attaching to an LLVM bug report (on http://bugs.llvm.org) as a reproducer, along with the input to generate the mismatch.","title":"Usage"},{"location":"ir_jit/#design_1","text":"The main generator, at a high level, uses the LLVM tools (namely IRBuilder) to create a main() function to: Examine its command-line parameters to determine their overall size and to them into the input buffer (as in Arg Passing above). Invoke the entry function (which remains unchanged from the source emitted by the JIT). Unpack the output buffer after the entry function completes and print its contents. For arg packing/unpacking, the functions provided by JitRuntime are used, but in a stateless context wrapped in an extern \"C\" space, to simplify invocation from within LLVM IR.","title":"Design"},{"location":"ir_jit/#output-type-determination","text":"Inferring the computation's output type merits special discussion. While it's trivial for the main generator to detect the output type, it's very difficult to do so inside the executing main() - in the former, the type is real data, wherein the latter, it's metadata. To make this possible (without doing down an RTTI rabbit hole, or even simply requiring the user to specify it on the command line), we do the following: Determine the llvm::Type of the output. Convert that into an xls::Type , and capture that type as a String. Hardcode that string as a constant in the emitted main() function. At runtime, pass that type string into UnpackAndPrintBuffer() (one of the extern \"C\" function wrappers). Inside UnpackAndPrintBuffer() , parse that string (via xls::Parser::ParseType() ), and use the resulting xls::Type to determine the contents of the computation's output buffer.","title":"Output type determination"},{"location":"ir_overview/","text":"XLS: IR Overview XLS: IR Overview Before providing a detailed specification of the IR, in this section we briefly outline the ideas and philosophy behind the IR design and explain how to build, modify, and navigate the IR. The XLS IR is a dataflow-oriented IR that has the static-single-assignment (SSA) property, but is specialized for generating circuitry. It started out as a purely functional IR but over time more and more side-effecting operations had to be introduced. Specifically: XLS has a single IR representation which is used from the front-end down to the RTL-level. A single representation throughout the compiler enables maximal resuse of analysis and transformation components. Often compilers have different specialized IRs (or \"dialects\") for different levels of abstraction which can add complexity and inhibit reusability. However, in XLS this tradeoff between specialization and reusability is unnecessary because we start with a dataflow representation in the front end and can smoothly lower the IR down to the RTL-level which is itself dataflow. XLS IR is not control-flow graph (CFG) based, as many other compiler infrastructures. The insight is that the CFG abstraction was developed to model serial execution on a CPU. In hardware, however, everything happens at all times and in parallel. A sea-of-nodes (SoN) representation much more closely resembles this reality, which is why we have chosen it. It further turns out that many optimization passes are rather trivial to implement in the SoN representation, in particular as it requires no explicit SSA updates. The SSA property is automatically maintained by the IR being functional. TODO: High-level structure, package -> func,proc,block -> sea of nodes TODO: How to navigate TODO: Talk about basic types","title":"Overview"},{"location":"ir_overview/#xls-ir-overview","text":"XLS: IR Overview Before providing a detailed specification of the IR, in this section we briefly outline the ideas and philosophy behind the IR design and explain how to build, modify, and navigate the IR. The XLS IR is a dataflow-oriented IR that has the static-single-assignment (SSA) property, but is specialized for generating circuitry. It started out as a purely functional IR but over time more and more side-effecting operations had to be introduced. Specifically: XLS has a single IR representation which is used from the front-end down to the RTL-level. A single representation throughout the compiler enables maximal resuse of analysis and transformation components. Often compilers have different specialized IRs (or \"dialects\") for different levels of abstraction which can add complexity and inhibit reusability. However, in XLS this tradeoff between specialization and reusability is unnecessary because we start with a dataflow representation in the front end and can smoothly lower the IR down to the RTL-level which is itself dataflow. XLS IR is not control-flow graph (CFG) based, as many other compiler infrastructures. The insight is that the CFG abstraction was developed to model serial execution on a CPU. In hardware, however, everything happens at all times and in parallel. A sea-of-nodes (SoN) representation much more closely resembles this reality, which is why we have chosen it. It further turns out that many optimization passes are rather trivial to implement in the SoN representation, in particular as it requires no explicit SSA updates. The SSA property is automatically maintained by the IR being functional. TODO: High-level structure, package -> func,proc,block -> sea of nodes TODO: How to navigate TODO: Talk about basic types","title":"XLS: IR Overview"},{"location":"ir_semantics/","text":"XLS: IR semantics XLS: IR semantics Data types Bits Array Tuple Token Functions, procs, and blocks Function Proc Block Port Register Instantiation Operations Unary bitwise operations Variadic bitwise operations Arithmetic unary operations Arithmetic binary operations Comparison operations Shift operations Extension operations zero_ext sign_ext Channel operations receive send Array operations array array_index array_update Tuple operations tuple tuple_index Bit-vector operations bit_slice bit_slice_update dynamic_bit_slice concat reverse decode encode one_hot Control-oriented operations param sel one_hot_sel priority_sel invoke map dynamic_counted_for counted_for Sequencing operations after_all Other side-effecting operations assert cover gate RTL-level operations input_port output_port register_read register_write instantiation_input instantiation_output The XLS IR is a pure dataflow-oriented IR that has the static-single-assignment property, but is specialized for generating circuitry. The aim is to create effective circuit designs through a \"lifted\" understanding of the high-level operations and their semantics, instead of trying to reverse all relevant properties via dependence analysis, which often cannot take advantage of high level knowledge that the designer holds in their mind at design time. This document describes the semantics of the XLS intermediate representation (IR) including data types, operations, and textual representation. Data types Bits A vector of bits with a fixed width. Type syntax: bits[N] where N is the number of bits. Value syntax: A literal decimal number. Example: 42 . A binary number prefixed with 0b . Example: 0b10101 A hexadecimal number: 0x . Example: 0xdeadbeef The representation may optionally include the bit width in which case the type is prefixed before the literal: bits[N]:$literal . Example: bits[8]:0xab . Array A one-dimensional array of elements of the same type with a fixed number of elements. An array can contain bits, arrays, or tuples as elements. Empty (zero-element) arrays are not supported. Type syntax: $type[N] : an array containing N elements of type $type . Examples: Two-element array of 8-bit bits type: bits[8][2] Three-element array of tuple type: (bits[32], bits[2])[3] Value syntax: [$value_1, ... , $value_N] where $value_n is the value of the n -th element. Examples: Array of bits elements with explicit bit count: [bits[8]:10, bits[8]:30] Three-element array consisting of two-element arrays of bits elements: [[1, 2], [3, 4], [5, 6]]] Tuple An ordered set of fixed size containing elements with potentially different types. tuples can contain bits, arrays, or tuples as elements. May be empty. Type syntax: ($type_{0}, ..., $type_{N-1}) where N is the number of elements and where $type_n is the type of the n -th element. Value syntax: ($value_{0}, ..., $value_{N-1}) where $value_n is the value of the n -th element. Examples: Tuple containing two bits elements: (0b100, 0b101) A nested tuple containing various element types: ((1, 2), 42, [5, 6]) Token A type used to enforce ordering between channel operations. The token type has no value and all tokens are identical. A token is purely symbolic / semantic and has no correlate in hardware. Type syntax: token Functions, procs, and blocks The XLS IR has three function-level abstractions each which hold a data-flow graph of XLS IR operations: functions , procs , and blocks . Names of function, procs and blocks must be unique among their respective abstractions (functions, procs, and blocks). For example, a block cannot share a name with another block but can share a name with a function. Function A function is a stateless abstraction with a single-output which is computed from zero or more input parameters. May invoke other functions. Proc A Proc is a stateful abstraction with an arbitrarily-typed recurrent state. Procs can communicate with other procs via channels which (abstractly) are infinite-depth FIFOs with flow control. Channel communication is handled via send and receive IR operations. Procs may invoke functions. TODO(meheff): 2021/11/04 Expand to include more details. Block A Block is an RTL-level abstraction used for code generation. It corresponds to a single Verilog module. Procs and functions are converted to blocks as part of the code generation process. Blocks may \u201cinvoke\u201d other blocks via instantiation. A block includes explicit representations of RTL constructs: ports, registers, and instantiations. The constructs are scoped within the block. Port A port is a representation of an input or output to the block. These correspond to ports on Verilog modules. Ports can be arbitrarily-typed. In the block, each port is represented with a input_port or output_port operation. Register A register is a representation of a hardware register (flop). Registers can be arbitrarily-typed. Each register must have a single register_write and a single register_read operation for writing and reading the register respectively. Instantiation An instantiation is a block-scoped construct that represents a module instantiation at the Verilog level. The instantiated object can be another block, a FIFO (not yet supported), or a externally defined Verilog module (not yet supported). The instantiation is integrated into the instantiating block with instantiation_input and instantiation_output operations. There is a one-to-one mapping between the instantiation input/output and the ports of the instantiated objects. Operations Operations share a common syntax and have both positional and keyword arguments \u00e0 la Python. Positional arguments are ordered and must appear first in the argument list. Positional arguments are exclusively the identifiers of the operands. Keyword arguments are unordered and must appear after the positional arguments. Keyword arguments can include arbitrary value types. result = operation(pos_arg_0, ..., pos_arg_N, keyword_0=value0, ..., keyword_M=valueM, ...) Common keyword arguments Keyword Type Required Default Description pos SourceLocation no The source location associated with this operation. The syntax is a triplet of comma-separated integer values: Fileno,Lineno,Colno Unary bitwise operations Performs a bit-wise operation on a single bits-typed operand. Syntax result = identity(operand) result = not(operand) Types Value Type operand bits[N] result bits[N] Operations Operation Opcode Semantics identity Op::kIdentity result = operand not Op::kNot result = ~operand Variadic bitwise operations Performs a bit-wise operation on one-or-more identically-typed bits operands. If only a single argument is provided the operation is a no-op. Syntax result = and(operand_{0}, ..., operand_{N-1}) result = or(operand_{0}, ..., operand_{N-1}) result = xor(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} bits[N] result bits[N] Operations Operation Opcode Semantics and Op::kAnd result = lhs & rhs & ... or Op::kOr result = lhs \\| rhs \\| ... xor Op::kXor result = lhs ^ rhs ^ ... Arithmetic unary operations Performs an arithmetic operation on a single bits-typed operand. Syntax result = neg(operand) Types Value Type operand bits[N] result bits[N] Operations Operation Opcode Semantics neg Op::kNeg result = -operand Arithmetic binary operations Performs an arithmetic operation on a pair of bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'. Syntax result = add(lhs, rhs) result = smul(lhs, rhs) result = umul(lhs, rhs) result = sdiv(lhs, rhs) result = smod(lhs, rhs) result = sub(lhs, rhs) result = udiv(lhs, rhs) result = umod(lhs, rhs) result = smulp(lhs, rhs) result = umulp(lhs, rhs) Types Currently signed and unsigned multiply, as wells as their partial product variants, support arbitrary width operands and result. For all other arithmetic operations the operands and the result are the same width. The expectation is that all arithmetic operations will eventually support arbitrary widths. Operations Operation Opcode Semantics add Op::kAdd result = lhs + rhs sdiv Op::kSDiv result = $signed(lhs) / $signed(rhs) * ** smod Op::kSMod result = $signed(lhs) % $signed(rhs) * *** smul Op::kSMul result = $signed(lhs) * $signed(rhs) smulp Op::kSMulp result[0] + result[1] = $signed(lhs) * $signed(rhs) **** sub Op::kSub result = lhs - rhs udiv Op::kUDiv result = lhs / rhs * ** umod Op::kUMod result = lhs % rhs * umul Op::kUMul result = lhs * rhs umulp Op::kUMulp result[0] + result[1] = lhs * rhs **** * Synthesizing division or modulus can lead to failing synthesis and/or problems with timing closure. It is usually best not to rely on this Verilog operator in practice, but instead explicitly instantiate a divider of choice. ** Division rounds toward zero. For unsigned division this is the same as truncation. If the divisor is zero, unsigned division produces a maximal positive value. For signed division, if the divisor is zero the result is the maximal positive value if the dividend is non-negative or the maximal negative value if the dividend is negative. *** The sign of the result of modulus matches the sign of the left operand. If the right operand is zero the result is zero. **** The partial product multiply variants return a two-element tuple with both elements having the same type. The outputs are not fully constrained; the operations are free to return any values that sum to the product lhs * rhs . Comparison operations Performs a comparison on a pair of identically-typed bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'. Produces a result of bits[1] type. Syntax result = eq(lhs, rhs) result = ne(lhs, rhs) result = sge(lhs, rhs) result = sgt(lhs, rhs) result = sle(lhs, rhs) result = slt(lhs, rhs) result = uge(lhs, rhs) result = ugt(lhs, rhs) result = ule(lhs, rhs) result = ult(lhs, rhs) Types Value Type lhs bits[N] rhs bits[N] result bits[1] Operations Operation Opcode Semantics eq Op::kEq result = lhs == rhs ne Op::kNe result = lhs != rhs sge Op::kSGe result = lhs >= rhs sgt Op::kSGt result = lhs > rhs sle Op::kSLe result = lhs <= rhs slt Op::kSLt result = lhs < rhs uge Op::kUGe result = lhs >= rhs ugt Op::kUGt result = lhs > rhs ule Op::kULe result = lhs <= rhs ult Op::kULt result = lhs < rhs Shift operations Performs an shift operation on an input operand where the shift amount is specified by a second operand. Syntax result = shll(operand, amount) result = shra(operand, amount) result = shrl(operand, amount) Types The shifted operand and the result of the shift are the same width. Widths of the shift amount may be arbitrary. Operations Operation Opcode Semantics shll Op::kShll result = lhs << rhs * shra Op::kShra result = lhs >>> rhs (arithmetic shift right) ** shrl Op::kShrl result = lhs >> rhs * * Logically shifting greater than or equal to the number of bits in the lhs produces a result of zero. ** Arithmetic right shifting greater than or equal to the number of bits in the lhs produces a result equal to all of the bits set to the sign of the lhs . Extension operations Extends a bit value to a new (larger) target bit-length. Syntax result = zero_ext(x, new_bit_count=42) result = sign_ext(x, new_bit_count=42) Types Value Type arg bits[N] new_bit_count int64_t result bits[new_bit_count] Note: new_bit_count should be >= N or an error may be raised. zero_ext Zero-extends a value: turns its bit-length into the new target bit-length by filling zeroes in the most significant bits. sign_ext Sign-extends a value: turns its bit-length into the new target bit-length by filling in the most significant bits (MSbs) with the following policy: ones in the MSbs if the MSb of the original value was set, or zeros in the MSbs if the MSb of the original value was unset. Channel operations These operations send or receive data over channels. Channels are monomorphic, and each channel supports a fixed set of data types which are sent or received in a single transaction. receive Receives a data value from a specified channel. The type of the data value is determined by the channel. An optional predicate value conditionally enables the receive operation. An optional blocking attribute determines whether the receive operation is blocking. A blocking receive waits (or blocks) until valid data is present at the channel. Compared to a blocking receive, a non-blocking receive has an additional entry in its return tuple of type bits[1] denoting whether the data read is valid. result = receive(tkn, predicate=<pred>, blocking=<bool>, channel_id=<ch>) Types Value Type tkn token pred bits[1] result (token, T) if blocking == true else (token, T, bits[1]) Keyword arguments Keyword Type Required Default Description predicate bits[1] no A value is received iff predicate is true blocking bool no true Whether the receive is blocking channel_id int64_t yes The ID of the channel to receive data from If the predicate is false the data values in the result are zero-filled. send Sends data to a specified channel. The type of the data values is determined by the channel. An optional predicate value conditionally enables the send operation. result = send(tkn, data, predicate=<pred>, channel_id=<ch>) Types Value Type tkn token data T pred bits[1] result token The type of data must match the type supported by the channel. Keyword arguments Keyword Type Required Default Description predicate bits[1] no A value is sent iff predicate is true channel_id int64_t yes The ID of the channel to send data to. Array operations array Constructs an array of its operands. result = array(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} T result T[N] Array can take an arbitrary number of operands including zero (which produces an empty array). The n-th operand becomes the n-th element of the array. array_index Returns a single element from an array. Syntax result = array_index(array, indices=[idx_{0}, ... , idx_{N-1}]) Types Value Type array Array of at least N dimensions idx_{i} Arbitrary bits type result T Returns the element of array indexed by the indices idx_{0} ... idx_{N-1} . The array must have at least as many dimensions as number of index elements N . Each element idx_{i} indexes a dimension of array . The first element idx_{0} indexes the outer most dimension, the second element idx_{1} indexes the second outer most dimension, etc. The result type T is the type of array with the N outer most dimensions removed. Any out-of-bounds indices idx_{i} are clamped to the maximum in bounds index for the respective dimension. The table below shows examples of the result type T and the result expression assuming input array operand A . Indices Array type result type T Result expression {1, 2} bits[3][4][5] bits[3] A[1][2] {10, 2} bits[3][4][5] bits[3] A[4][2] (first index is out-of-bounds and clamped at the maximum index) {1} bits[3][4][5] bits[3][4] A[1] {} bits[3][4][5] bits[3][4][5] A {} bits[32] bits[32] A array_update Returns a modified copy of an array. Syntax result = array_update(array, value, indices=[idx_{0}, ... , idx_{N-1}]) Types Value Type array Array of at least N dimensions value T idx_{i} Arbitrary bits type result Same type as array Returns a copy of the input array with the element at the given indices replaced with the given value. If any index is out of bounds, the result is identical to the input array . The indexing semantics is identical to array_index with the exception of out-of-bounds behavior. Tuple operations tuple Constructs a tuple of its operands. result = tuple(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} T_{i} result (T_{0}, ... , T_{N-1}) Tuple can take and arbitrary number of operands including zero (which produces an empty tuple). tuple_index Returns a single element from a tuple-typed operand. Syntax result = tuple_index(operand, index=<index>) Types Value Type operand (T_{0}, ... , T_{N-1}) result T_{<index>} Keyword arguments Keyword Type Required Default Description index int64_t yes Index of tuple element to produce Bit-vector operations bit_slice Slices a contiguous range of bits from a bits-typed operand. Syntax result = bit_slice(operand, start=<start>, width=<width>) Types Value Type operand bits[N] result bits[<width>] Keyword arguments Keyword Type Required Default Description start int64_t yes The starting bit of the slice. start is is zero-indexed where zero is the least-significant bit of the operand. width int64_t yes The width of the slice. The bit-width of operand must be greater than or equal to <start> plus <width> . bit_slice_update Replaces a contiguous range of bits in a bits-typed operand at a variable start index with a given value. Syntax result = bit_slice_update(operand, start, update_value) Types Value Type operand bits[N] start bits[I] update_value bits[M] result bits[N] Evaluates to operand with the contiguous M bits starting at index start replaced with update_value . Out-of-bound bits (which occur if start + M > N ) are ignored. Examples: operand start update_value result bits[16]:0xabcd 0 bits[8]:0xff bits[16]:0xabff bits[16]:0xabcd 4 bits[8]:0xff bits[16]:0xaffd bits[16]:0xabcd 12 bits[8]:0xff bits[16]:0xfbcd bits[16]:0xabcd 16 bits[8]:0xff bits[16]:0xabcd dynamic_bit_slice Slices a contiguous range of bits from a bits-typed operand, with variable starting index but fixed width. Out-of-bounds slicing is supported by treating all out-of-bounds bits as having value 0. Syntax result = dynamic_bit_slice(operand, start, width=<width>) Types Value Type operand bits[N] start bits[M] result bits[<width>] start can be of arbitrary bit width. It will be interpreted as an unsigned integer. Keyword arguments Keyword Type Required Default Description width int64_t yes The width of the slice. concat Concatenates and arbitrary number of bits-typed operands. result = concat(operand{0}, ..., operand{n-1}) Types Value Type operand_{i} bits[N_{i}] result bits[Sum(N_{i})] This is equivalent to the verilog concat operator: result = {arg0, ..., argN} reverse Reverses the order of bits of its operand. result = reverse(operand) Types Value Type operand bits[N] result bits[N] decode Implements a binary decoder. result = decode(operand, width=<width>) Types Value Type operand bits[N] result bits[M] The result width M must be less than or equal to 2** N where N is the operand width. Keyword arguments Keyword Type Required Default Description width int64_t yes Width of the result decode converts the binary-encoded operand value into a one-hot result. For an operand value of n interpreted as an unsigned number the n -th result bit and only the n -th result bit is set. The width of the decode operation may be less than the maximum value expressible by the input (2** N - 1). If the encoded operand value is larger than the number of bits of the result the result is zero. encode Implements a binary encoder. result = encode(operand, width=<width>) Types Value Type operand bits[N] result bits[M] The result width M must be equal to \\( \\(\\lceil \\log_{2} N \\rceil\\) \\) . encode converts the one-hot operand value into a binary-encoded value of the \"hot\" bit of the input. If the n -th bit and only the n -th bit of the operand is set the result is equal the value n as an unsigned number. If multiple bits of the input are set the result is equal to the logical or of the results produced by the input bits individually. For example, if bit 3 and bit 5 of an encode input are set the result is equal to 3 | 5 = 7. If no bits of the input are set the result is zero. one_hot Produces a bits value with exactly one bit set. The index of the set bit depends upon the input value. Syntax result = one_hot(input, lsb_prio=true) result = one_hot(input, lsb_prio=false) Types Value Type input bits[N] result bits[N+1] Keyword arguments Keyword Type Required Default Description lsb_prio bool yes Whether the least significant bit (LSb) has priority. For lsb_prio=true : result bit i for 0 <= i < N is set in result iff bit i is set in the input and all lower bits j for j < i are not set in the input. For lsb_prio=false : result bit i for N-1 >= i >= 0 is set in result iff bit i is set in the input and all higher (more significant) bits j for j > i are not set in the input. For both lsb_prio=true and lsb_prio=false , result bit N (the most significant bit in the output) is only set if no bits in the input are set. Examples: one_hot(0b0011, lsb_prio=true) => 0b00001 -- note that an extra MSb has been appended to the output to potentially represent the \"all zeros\" case. one_hot(0b0111, lsb_prio=false) => 0b00100 . one_hot(0b00, lsb_prio=false) => 0b100 . one_hot(0b00, lsb_prio=true) => 0b100 -- note the output for one_hot is the same for the all-zeros case regardless of whether lsb_prio is true or false. This operation is useful for constructing match or switch operation semantics where a condition is matched against an ordered set of cases and the first match is chosen. It is also useful for one-hot canonicalizing, e.g. as a prelude to counting leading/trailing zeros. Control-oriented operations For context note that, in XLS, operations are evaluated eagerly in a very general sense: all \"branches\" of computation may be evaluated in full before the result is selected via an operation such as one_hot_sel or sel . This model is amenable to pipeline-like hardware execution, where operations tend to be fixed in some spatial area and operations execute a single function, while interconnect is used for reconfiguration purposes. Towards this eager-evaluation-capable model, operations used within a function are generally not Turing-complete: operations such as counted_for require a finite bound so that they could be implemented using a finite amount of pipeline area. Operations such as dynamic_counted_for are an exception, where that operation will only be possible to use in a time-multiplexed code generation mode, such as the XLS sequential emitter, where arbitrary iteration to some dynamic bound is likely to be possible. param A parameter to the current IR function, which can be used as an operand for operations within the function. Syntax Parameters have a special syntactic form distinct from other nodes, where they are listed directly in the function signature with their type. fn f(x: bits[32]) -> bits[32] { ret identity.2 = identity(x, id=2) } Types Value Type name str type type sel Selects between operands based on a selector value. Syntax result = sel(selector, cases=[case_{0}, ... , case_{N-1}], default=<default>) Types Value Type selector bits[M] case_{i} T default T result T one_hot_sel Selects between operands based on a one-hot selector, OR -ing all selected cases if more than one case is selected. See one_hot for an example of the one-hot selector invariant. Note that when the selector is not one-hot, this operation is still well defined. Note that when one_hot operations are used to precondition the selector operand to one_hot_sel , the XLS optimizer will try to determine when they are unnecessary and subsequently eliminate them. Syntax result = one_hot_sel(selector, cases=[case_{0}, ... , case_{N-1}]) Types Value Type selector bits[N] case_{i} T result T The result is the logical OR of all cases case_{i} for which the corresponding bit i is set in the selector. When selector is one-hot this performs a select operation. priority_sel Selects between operands based on a selector, choosing the highest-priority case if more than one case is selected. Each bit in the selector corresponds to a case, with the least significant bit corresponding to the first case and having the highest priority. If there are no bits in the selector set, no case is selected and the default value of 0 is chosen. See one_hot for an example of the one-hot selector invariant. Note that when the selector is not one-hot, this operation is still well defined. Note that when one_hot operations are used to precondition the selector operand to priority_sel , the XLS optimizer will try to determine when they are unnecessary and subsequently eliminate them. Syntax result = priority_sel(selector, cases=[case_{0}, ... , case_{N-1}]) Types Value Type selector bits[N] case_{i} T result T The result is the first case case_{i} for which the corresponding bit i is set in the selector. If the selector is known to be one-hot, then the priority_sel() operation is equivalent to a one_hot_sel() . invoke Invokes a function. The return value for the invoked function is the result value. Syntax result = invoke(operand_{0}, ... , operand_{N-1}, to_apply=<to_apply>) Types Value Type init T result T Keyword arguments Keyword Type Required Default Description to_apply string yes Name of the function to use as the loop body map Applies a function to the elements of an array and returns the result as an array. Syntax result = map(operand, to_apply=<to_apply>) Types Value Type operand array[T] result array[U] Keyword arguments Keyword Type Required Default Description to_apply string yes Name of the function to apply to each element of the operand dynamic_counted_for Invokes a dynamic-trip count loop. Syntax result = counted_for(init, trip_count, stride, body=<body>, invariant_args=<inv_args>) Types Value Type init T trip_count bits[N], treated as unsigned stride bits[M], treated as signed , result T Keyword arguments Keyword Type Required Default Description invariant_args array of yes Names of the invariant operands as the loop body body string yes Name of the function to use as the loop body dynamic_counted_for invokes the function body trip_count times, passing loop-carried data that starts with value init . The induction variable is incremented by stride after each iteration. The first argument passed to body is the induction variable -- presently, the induction variable always starts at zero and increments by stride after every trip. The second argument passed to body is the loop-carry data. The return type of body must be the same as the type of the init loop carry data. The value returned from the last trip is the result of the counted_for expression. All subsequent arguments passed to body are passed from invariant_args ; e.g. if there are two members in invariant_args those values are passed as the third and fourth arguments. Therefore body should have a signature that matches the following: body(i, loop_carry_data, [invariant_arg0, invariant_arg1, ...]) Note that we currently inspect the body function to see what type of induction variable ( i above) it accepts in order to pass an i value of that type. trip_count must have fewer bits than i and stride should have fewer than or equal number of bits to i . Code generation support for dynamic_counted_for is limited because the pipeline generator cannot handle an unknown trip count. counted_for Invokes a fixed-trip count loop. Syntax result = counted_for(init, trip_count=<trip_count>, stride=<stride>, body=<body>, invariant_args=<inv_args>) Types Value Type init T result T Keyword arguments Keyword Type Required Default Description trip_count int64_t yes Trip count of the loop (number of times that the loop body will be executed) stride int64_t no 1 Stride of the induction variable invariant_args array of yes Names of the invariant operands as the loop body body string yes Name of the function to use as the loop body counted_for invokes the function body trip_count times, passing loop-carried data that starts with value init . The first argument passed to body is the induction variable -- presently, the induction variable always starts at zero and increments by stride after every trip. The second argument passed to body is the loop-carry data. The return type of body must be the same as the type of the init loop carry data. The value returned from the last trip is the result of the counted_for expression. All subsequent arguments passed to body are passed from invariant_args ; e.g. if there are two members in invariant_args those values are passed as the third and fourth arguments. Therefore body should have a signature that matches the following: body(i, loop_carry_data[, invariant_arg0, invariant_arg1, ...]) Note that we currently inspect the body function to see what type of induction variable ( i above) it accepts in order to pass an i value of that type. Sequencing operations Some operations in XLS IR are sensitive to sequence order, similar to channel operations , but are not themselves channel-related. Tokens are used to determine the possible sequencing of these effects, and after_all can be used to join together tokens as a sequencing merge point for concurrent threads of execution described by different tokens. after_all Used to construct partial orderings among channel operations. result = after_all(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} token result token after_all can consume an arbitrary number of token operands including zero. Other side-effecting operations Aside from channels operations such as send and receive several other operations have side-effects. Care must be taken when adding, removing, or transforming these operations, e.g., in the optimizer. assert Raises an error at software run-time (DSLX/IR interpretation, JIT execution, RTL simulation) if the given condition evaluates to false. The operation takes a literal string attribute which is included in the error message. This is a software-only operation and has no representation in the generated hardware. Tokens are used to connect the operation to the graph and order with respect to other side-effecting operations. result = assert(tkn, condition, message=<string>) result = assert(tkn, condition, message=<string>, label=<string>) Types Value Type tkn token condition bits[1] result token Keyword arguments Keyword Type Required Default Description message string yes Message to include in raised error label optional string yes Label to associate with the assert statement in the generated (System)Verilog cover Records the number of times the given condition evaluates to true. Just like assert , this is a software-only construct and is not emitted in a final hardware design. Tokens are used to sequence this operation in the graph. result = cover(tkn, condition, label=<string>) Types Value Type tkn token condition bits[1] result token Keyword arguments Keyword Type Required Default Description label string yes Name associated with the counter. gate Gates an arbitrarily-typed value based on a condition. The result of the operation is the data operand if the condition is true, otherwise the result is a zero value of the type of the data operand (i.e., the value is gated off). A helpful mnemonic is to think of this as analogous to an AND gate: if the condition is true , the value passes through, otherwise it's zeroed. This operation can reduce switching and may be used in power optimizations. This is intended for use in operand gating for power reduction, and the compiler may ultimately use it to perform register-level load-enable gating. The operation is considered side-effecting to prevent removal of the operation when the gated result (condition is false) is not observable. In this case the gate operation is still desirable because of the operations' effects on power consumption. result = gate(condition, data) Types Value Type condition bits[1] data T result T RTL-level operations These IR operations correspond to RTL-level constructs in the emitted Verilog. These operations are added and used in the code generation process and may only appear in blocks (not procs or functions). input_port Corresponds to an input port on a Verilog module. Syntax result = input_port() Types Value Type result T An input_port operation can be an arbitrary type. output_port Corresponds to an output port on a Verilog module. The value sent to the output port is the data operand. Syntax result = output_port(data) Types Value Type data T result T register_read Reads a value from a register. The register is defined on the block. Syntax result = register_read(register=<register_name>) Types Value Type result T The type T of the result of the operation is the type of the register. Keyword arguments Keyword Type Required Default Description register string yes Name of the register to read register_write Writes a value to a register. The write to the register may be conditioned upon an optional load-enable and/or reset signal. The register is defined on the block. Syntax result = register_write(data, load_enable=<load_enable>, reset=<reset>, register=<register_name>) Types Value Type data T load_enable bits[1] (optional) reset bits[1] (optional) result () (empty tuple) Keyword arguments Keyword Type Required Default Description register string yes Name of the register to write The type T of the data operand must be the same as the the type of the register. instantiation_input Corresponds to a single input port of an instantiation. An instantiation is a block-scoped construct that represents a module instantiation at the Verilog level. Each instantation_input operation corresponds to a particular port of the instantiated object, so generally a single instantiation can have multiple associated instantiation_input operations (one for each input port). Syntax result = instantiation_input(data, instantiation=<instantiation>, port_name=<port_name>) Types Value Type data T result () Keyword arguments Keyword Type Required Default Description instantiation string yes Name of the instantiation. port_name string yes Name of the associated port of the instantiation. The type T of the data operand must be the same as the the type of the associated input port of the instantiated object. instantiation_output Corresponds to a single output port of an instantiation. An instantiation is a block-scoped construct that represents a module instantiation at the Verilog level. Each instantation_output operation corresponds to a output particular port of the instantiated object, so generally a single instantiation can have multiple associated instantiation_output operations (one for each output port). Syntax result = instantiation_output(instantiation=<instantiation>, port_name=<port_name>) Types Value Type result T Keyword arguments Keyword Type Required Default Description instantiation string yes Name of the instantiation. port_name string yes Name of the associated port of the instantiation. The type T of the result is type of the associated output port of the instantiated object.","title":"Semantics"},{"location":"ir_semantics/#xls-ir-semantics","text":"XLS: IR semantics Data types Bits Array Tuple Token Functions, procs, and blocks Function Proc Block Port Register Instantiation Operations Unary bitwise operations Variadic bitwise operations Arithmetic unary operations Arithmetic binary operations Comparison operations Shift operations Extension operations zero_ext sign_ext Channel operations receive send Array operations array array_index array_update Tuple operations tuple tuple_index Bit-vector operations bit_slice bit_slice_update dynamic_bit_slice concat reverse decode encode one_hot Control-oriented operations param sel one_hot_sel priority_sel invoke map dynamic_counted_for counted_for Sequencing operations after_all Other side-effecting operations assert cover gate RTL-level operations input_port output_port register_read register_write instantiation_input instantiation_output The XLS IR is a pure dataflow-oriented IR that has the static-single-assignment property, but is specialized for generating circuitry. The aim is to create effective circuit designs through a \"lifted\" understanding of the high-level operations and their semantics, instead of trying to reverse all relevant properties via dependence analysis, which often cannot take advantage of high level knowledge that the designer holds in their mind at design time. This document describes the semantics of the XLS intermediate representation (IR) including data types, operations, and textual representation.","title":"XLS: IR semantics"},{"location":"ir_semantics/#data-types","text":"","title":"Data types"},{"location":"ir_semantics/#bits","text":"A vector of bits with a fixed width. Type syntax: bits[N] where N is the number of bits. Value syntax: A literal decimal number. Example: 42 . A binary number prefixed with 0b . Example: 0b10101 A hexadecimal number: 0x . Example: 0xdeadbeef The representation may optionally include the bit width in which case the type is prefixed before the literal: bits[N]:$literal . Example: bits[8]:0xab .","title":"Bits"},{"location":"ir_semantics/#array","text":"A one-dimensional array of elements of the same type with a fixed number of elements. An array can contain bits, arrays, or tuples as elements. Empty (zero-element) arrays are not supported. Type syntax: $type[N] : an array containing N elements of type $type . Examples: Two-element array of 8-bit bits type: bits[8][2] Three-element array of tuple type: (bits[32], bits[2])[3] Value syntax: [$value_1, ... , $value_N] where $value_n is the value of the n -th element. Examples: Array of bits elements with explicit bit count: [bits[8]:10, bits[8]:30] Three-element array consisting of two-element arrays of bits elements: [[1, 2], [3, 4], [5, 6]]]","title":"Array"},{"location":"ir_semantics/#tuple","text":"An ordered set of fixed size containing elements with potentially different types. tuples can contain bits, arrays, or tuples as elements. May be empty. Type syntax: ($type_{0}, ..., $type_{N-1}) where N is the number of elements and where $type_n is the type of the n -th element. Value syntax: ($value_{0}, ..., $value_{N-1}) where $value_n is the value of the n -th element. Examples: Tuple containing two bits elements: (0b100, 0b101) A nested tuple containing various element types: ((1, 2), 42, [5, 6])","title":"Tuple"},{"location":"ir_semantics/#token","text":"A type used to enforce ordering between channel operations. The token type has no value and all tokens are identical. A token is purely symbolic / semantic and has no correlate in hardware. Type syntax: token","title":"Token"},{"location":"ir_semantics/#functions-procs-and-blocks","text":"The XLS IR has three function-level abstractions each which hold a data-flow graph of XLS IR operations: functions , procs , and blocks . Names of function, procs and blocks must be unique among their respective abstractions (functions, procs, and blocks). For example, a block cannot share a name with another block but can share a name with a function.","title":"Functions, procs, and blocks"},{"location":"ir_semantics/#function","text":"A function is a stateless abstraction with a single-output which is computed from zero or more input parameters. May invoke other functions.","title":"Function"},{"location":"ir_semantics/#proc","text":"A Proc is a stateful abstraction with an arbitrarily-typed recurrent state. Procs can communicate with other procs via channels which (abstractly) are infinite-depth FIFOs with flow control. Channel communication is handled via send and receive IR operations. Procs may invoke functions. TODO(meheff): 2021/11/04 Expand to include more details.","title":"Proc"},{"location":"ir_semantics/#block","text":"A Block is an RTL-level abstraction used for code generation. It corresponds to a single Verilog module. Procs and functions are converted to blocks as part of the code generation process. Blocks may \u201cinvoke\u201d other blocks via instantiation. A block includes explicit representations of RTL constructs: ports, registers, and instantiations. The constructs are scoped within the block.","title":"Block"},{"location":"ir_semantics/#port","text":"A port is a representation of an input or output to the block. These correspond to ports on Verilog modules. Ports can be arbitrarily-typed. In the block, each port is represented with a input_port or output_port operation.","title":"Port"},{"location":"ir_semantics/#register","text":"A register is a representation of a hardware register (flop). Registers can be arbitrarily-typed. Each register must have a single register_write and a single register_read operation for writing and reading the register respectively.","title":"Register"},{"location":"ir_semantics/#instantiation","text":"An instantiation is a block-scoped construct that represents a module instantiation at the Verilog level. The instantiated object can be another block, a FIFO (not yet supported), or a externally defined Verilog module (not yet supported). The instantiation is integrated into the instantiating block with instantiation_input and instantiation_output operations. There is a one-to-one mapping between the instantiation input/output and the ports of the instantiated objects.","title":"Instantiation"},{"location":"ir_semantics/#operations","text":"Operations share a common syntax and have both positional and keyword arguments \u00e0 la Python. Positional arguments are ordered and must appear first in the argument list. Positional arguments are exclusively the identifiers of the operands. Keyword arguments are unordered and must appear after the positional arguments. Keyword arguments can include arbitrary value types. result = operation(pos_arg_0, ..., pos_arg_N, keyword_0=value0, ..., keyword_M=valueM, ...) Common keyword arguments Keyword Type Required Default Description pos SourceLocation no The source location associated with this operation. The syntax is a triplet of comma-separated integer values: Fileno,Lineno,Colno","title":"Operations"},{"location":"ir_semantics/#unary-bitwise-operations","text":"Performs a bit-wise operation on a single bits-typed operand. Syntax result = identity(operand) result = not(operand) Types Value Type operand bits[N] result bits[N] Operations Operation Opcode Semantics identity Op::kIdentity result = operand not Op::kNot result = ~operand","title":"Unary bitwise operations"},{"location":"ir_semantics/#variadic-bitwise-operations","text":"Performs a bit-wise operation on one-or-more identically-typed bits operands. If only a single argument is provided the operation is a no-op. Syntax result = and(operand_{0}, ..., operand_{N-1}) result = or(operand_{0}, ..., operand_{N-1}) result = xor(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} bits[N] result bits[N] Operations Operation Opcode Semantics and Op::kAnd result = lhs & rhs & ... or Op::kOr result = lhs \\| rhs \\| ... xor Op::kXor result = lhs ^ rhs ^ ...","title":"Variadic bitwise operations"},{"location":"ir_semantics/#arithmetic-unary-operations","text":"Performs an arithmetic operation on a single bits-typed operand. Syntax result = neg(operand) Types Value Type operand bits[N] result bits[N] Operations Operation Opcode Semantics neg Op::kNeg result = -operand","title":"Arithmetic unary operations"},{"location":"ir_semantics/#arithmetic-binary-operations","text":"Performs an arithmetic operation on a pair of bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'. Syntax result = add(lhs, rhs) result = smul(lhs, rhs) result = umul(lhs, rhs) result = sdiv(lhs, rhs) result = smod(lhs, rhs) result = sub(lhs, rhs) result = udiv(lhs, rhs) result = umod(lhs, rhs) result = smulp(lhs, rhs) result = umulp(lhs, rhs) Types Currently signed and unsigned multiply, as wells as their partial product variants, support arbitrary width operands and result. For all other arithmetic operations the operands and the result are the same width. The expectation is that all arithmetic operations will eventually support arbitrary widths. Operations Operation Opcode Semantics add Op::kAdd result = lhs + rhs sdiv Op::kSDiv result = $signed(lhs) / $signed(rhs) * ** smod Op::kSMod result = $signed(lhs) % $signed(rhs) * *** smul Op::kSMul result = $signed(lhs) * $signed(rhs) smulp Op::kSMulp result[0] + result[1] = $signed(lhs) * $signed(rhs) **** sub Op::kSub result = lhs - rhs udiv Op::kUDiv result = lhs / rhs * ** umod Op::kUMod result = lhs % rhs * umul Op::kUMul result = lhs * rhs umulp Op::kUMulp result[0] + result[1] = lhs * rhs **** * Synthesizing division or modulus can lead to failing synthesis and/or problems with timing closure. It is usually best not to rely on this Verilog operator in practice, but instead explicitly instantiate a divider of choice. ** Division rounds toward zero. For unsigned division this is the same as truncation. If the divisor is zero, unsigned division produces a maximal positive value. For signed division, if the divisor is zero the result is the maximal positive value if the dividend is non-negative or the maximal negative value if the dividend is negative. *** The sign of the result of modulus matches the sign of the left operand. If the right operand is zero the result is zero. **** The partial product multiply variants return a two-element tuple with both elements having the same type. The outputs are not fully constrained; the operations are free to return any values that sum to the product lhs * rhs .","title":"Arithmetic binary operations"},{"location":"ir_semantics/#comparison-operations","text":"Performs a comparison on a pair of identically-typed bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'. Produces a result of bits[1] type. Syntax result = eq(lhs, rhs) result = ne(lhs, rhs) result = sge(lhs, rhs) result = sgt(lhs, rhs) result = sle(lhs, rhs) result = slt(lhs, rhs) result = uge(lhs, rhs) result = ugt(lhs, rhs) result = ule(lhs, rhs) result = ult(lhs, rhs) Types Value Type lhs bits[N] rhs bits[N] result bits[1] Operations Operation Opcode Semantics eq Op::kEq result = lhs == rhs ne Op::kNe result = lhs != rhs sge Op::kSGe result = lhs >= rhs sgt Op::kSGt result = lhs > rhs sle Op::kSLe result = lhs <= rhs slt Op::kSLt result = lhs < rhs uge Op::kUGe result = lhs >= rhs ugt Op::kUGt result = lhs > rhs ule Op::kULe result = lhs <= rhs ult Op::kULt result = lhs < rhs","title":"Comparison operations"},{"location":"ir_semantics/#shift-operations","text":"Performs an shift operation on an input operand where the shift amount is specified by a second operand. Syntax result = shll(operand, amount) result = shra(operand, amount) result = shrl(operand, amount) Types The shifted operand and the result of the shift are the same width. Widths of the shift amount may be arbitrary. Operations Operation Opcode Semantics shll Op::kShll result = lhs << rhs * shra Op::kShra result = lhs >>> rhs (arithmetic shift right) ** shrl Op::kShrl result = lhs >> rhs * * Logically shifting greater than or equal to the number of bits in the lhs produces a result of zero. ** Arithmetic right shifting greater than or equal to the number of bits in the lhs produces a result equal to all of the bits set to the sign of the lhs .","title":"Shift operations"},{"location":"ir_semantics/#extension-operations","text":"Extends a bit value to a new (larger) target bit-length. Syntax result = zero_ext(x, new_bit_count=42) result = sign_ext(x, new_bit_count=42) Types Value Type arg bits[N] new_bit_count int64_t result bits[new_bit_count] Note: new_bit_count should be >= N or an error may be raised.","title":"Extension operations"},{"location":"ir_semantics/#zero_ext","text":"Zero-extends a value: turns its bit-length into the new target bit-length by filling zeroes in the most significant bits.","title":"zero_ext"},{"location":"ir_semantics/#sign_ext","text":"Sign-extends a value: turns its bit-length into the new target bit-length by filling in the most significant bits (MSbs) with the following policy: ones in the MSbs if the MSb of the original value was set, or zeros in the MSbs if the MSb of the original value was unset.","title":"sign_ext"},{"location":"ir_semantics/#channel-operations","text":"These operations send or receive data over channels. Channels are monomorphic, and each channel supports a fixed set of data types which are sent or received in a single transaction.","title":"Channel operations"},{"location":"ir_semantics/#receive","text":"Receives a data value from a specified channel. The type of the data value is determined by the channel. An optional predicate value conditionally enables the receive operation. An optional blocking attribute determines whether the receive operation is blocking. A blocking receive waits (or blocks) until valid data is present at the channel. Compared to a blocking receive, a non-blocking receive has an additional entry in its return tuple of type bits[1] denoting whether the data read is valid. result = receive(tkn, predicate=<pred>, blocking=<bool>, channel_id=<ch>) Types Value Type tkn token pred bits[1] result (token, T) if blocking == true else (token, T, bits[1]) Keyword arguments Keyword Type Required Default Description predicate bits[1] no A value is received iff predicate is true blocking bool no true Whether the receive is blocking channel_id int64_t yes The ID of the channel to receive data from If the predicate is false the data values in the result are zero-filled.","title":"receive"},{"location":"ir_semantics/#send","text":"Sends data to a specified channel. The type of the data values is determined by the channel. An optional predicate value conditionally enables the send operation. result = send(tkn, data, predicate=<pred>, channel_id=<ch>) Types Value Type tkn token data T pred bits[1] result token The type of data must match the type supported by the channel. Keyword arguments Keyword Type Required Default Description predicate bits[1] no A value is sent iff predicate is true channel_id int64_t yes The ID of the channel to send data to.","title":"send"},{"location":"ir_semantics/#array-operations","text":"","title":"Array operations"},{"location":"ir_semantics/#array_1","text":"Constructs an array of its operands. result = array(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} T result T[N] Array can take an arbitrary number of operands including zero (which produces an empty array). The n-th operand becomes the n-th element of the array.","title":"array"},{"location":"ir_semantics/#array_index","text":"Returns a single element from an array. Syntax result = array_index(array, indices=[idx_{0}, ... , idx_{N-1}]) Types Value Type array Array of at least N dimensions idx_{i} Arbitrary bits type result T Returns the element of array indexed by the indices idx_{0} ... idx_{N-1} . The array must have at least as many dimensions as number of index elements N . Each element idx_{i} indexes a dimension of array . The first element idx_{0} indexes the outer most dimension, the second element idx_{1} indexes the second outer most dimension, etc. The result type T is the type of array with the N outer most dimensions removed. Any out-of-bounds indices idx_{i} are clamped to the maximum in bounds index for the respective dimension. The table below shows examples of the result type T and the result expression assuming input array operand A . Indices Array type result type T Result expression {1, 2} bits[3][4][5] bits[3] A[1][2] {10, 2} bits[3][4][5] bits[3] A[4][2] (first index is out-of-bounds and clamped at the maximum index) {1} bits[3][4][5] bits[3][4] A[1] {} bits[3][4][5] bits[3][4][5] A {} bits[32] bits[32] A","title":"array_index"},{"location":"ir_semantics/#array_update","text":"Returns a modified copy of an array. Syntax result = array_update(array, value, indices=[idx_{0}, ... , idx_{N-1}]) Types Value Type array Array of at least N dimensions value T idx_{i} Arbitrary bits type result Same type as array Returns a copy of the input array with the element at the given indices replaced with the given value. If any index is out of bounds, the result is identical to the input array . The indexing semantics is identical to array_index with the exception of out-of-bounds behavior.","title":"array_update"},{"location":"ir_semantics/#tuple-operations","text":"","title":"Tuple operations"},{"location":"ir_semantics/#tuple_1","text":"Constructs a tuple of its operands. result = tuple(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} T_{i} result (T_{0}, ... , T_{N-1}) Tuple can take and arbitrary number of operands including zero (which produces an empty tuple).","title":"tuple"},{"location":"ir_semantics/#tuple_index","text":"Returns a single element from a tuple-typed operand. Syntax result = tuple_index(operand, index=<index>) Types Value Type operand (T_{0}, ... , T_{N-1}) result T_{<index>} Keyword arguments Keyword Type Required Default Description index int64_t yes Index of tuple element to produce","title":"tuple_index"},{"location":"ir_semantics/#bit-vector-operations","text":"","title":"Bit-vector operations"},{"location":"ir_semantics/#bit_slice","text":"Slices a contiguous range of bits from a bits-typed operand. Syntax result = bit_slice(operand, start=<start>, width=<width>) Types Value Type operand bits[N] result bits[<width>] Keyword arguments Keyword Type Required Default Description start int64_t yes The starting bit of the slice. start is is zero-indexed where zero is the least-significant bit of the operand. width int64_t yes The width of the slice. The bit-width of operand must be greater than or equal to <start> plus <width> .","title":"bit_slice"},{"location":"ir_semantics/#bit_slice_update","text":"Replaces a contiguous range of bits in a bits-typed operand at a variable start index with a given value. Syntax result = bit_slice_update(operand, start, update_value) Types Value Type operand bits[N] start bits[I] update_value bits[M] result bits[N] Evaluates to operand with the contiguous M bits starting at index start replaced with update_value . Out-of-bound bits (which occur if start + M > N ) are ignored. Examples: operand start update_value result bits[16]:0xabcd 0 bits[8]:0xff bits[16]:0xabff bits[16]:0xabcd 4 bits[8]:0xff bits[16]:0xaffd bits[16]:0xabcd 12 bits[8]:0xff bits[16]:0xfbcd bits[16]:0xabcd 16 bits[8]:0xff bits[16]:0xabcd","title":"bit_slice_update"},{"location":"ir_semantics/#dynamic_bit_slice","text":"Slices a contiguous range of bits from a bits-typed operand, with variable starting index but fixed width. Out-of-bounds slicing is supported by treating all out-of-bounds bits as having value 0. Syntax result = dynamic_bit_slice(operand, start, width=<width>) Types Value Type operand bits[N] start bits[M] result bits[<width>] start can be of arbitrary bit width. It will be interpreted as an unsigned integer. Keyword arguments Keyword Type Required Default Description width int64_t yes The width of the slice.","title":"dynamic_bit_slice"},{"location":"ir_semantics/#concat","text":"Concatenates and arbitrary number of bits-typed operands. result = concat(operand{0}, ..., operand{n-1}) Types Value Type operand_{i} bits[N_{i}] result bits[Sum(N_{i})] This is equivalent to the verilog concat operator: result = {arg0, ..., argN}","title":"concat"},{"location":"ir_semantics/#reverse","text":"Reverses the order of bits of its operand. result = reverse(operand) Types Value Type operand bits[N] result bits[N]","title":"reverse"},{"location":"ir_semantics/#decode","text":"Implements a binary decoder. result = decode(operand, width=<width>) Types Value Type operand bits[N] result bits[M] The result width M must be less than or equal to 2** N where N is the operand width. Keyword arguments Keyword Type Required Default Description width int64_t yes Width of the result decode converts the binary-encoded operand value into a one-hot result. For an operand value of n interpreted as an unsigned number the n -th result bit and only the n -th result bit is set. The width of the decode operation may be less than the maximum value expressible by the input (2** N - 1). If the encoded operand value is larger than the number of bits of the result the result is zero.","title":"decode"},{"location":"ir_semantics/#encode","text":"Implements a binary encoder. result = encode(operand, width=<width>) Types Value Type operand bits[N] result bits[M] The result width M must be equal to \\( \\(\\lceil \\log_{2} N \\rceil\\) \\) . encode converts the one-hot operand value into a binary-encoded value of the \"hot\" bit of the input. If the n -th bit and only the n -th bit of the operand is set the result is equal the value n as an unsigned number. If multiple bits of the input are set the result is equal to the logical or of the results produced by the input bits individually. For example, if bit 3 and bit 5 of an encode input are set the result is equal to 3 | 5 = 7. If no bits of the input are set the result is zero.","title":"encode"},{"location":"ir_semantics/#one_hot","text":"Produces a bits value with exactly one bit set. The index of the set bit depends upon the input value. Syntax result = one_hot(input, lsb_prio=true) result = one_hot(input, lsb_prio=false) Types Value Type input bits[N] result bits[N+1] Keyword arguments Keyword Type Required Default Description lsb_prio bool yes Whether the least significant bit (LSb) has priority. For lsb_prio=true : result bit i for 0 <= i < N is set in result iff bit i is set in the input and all lower bits j for j < i are not set in the input. For lsb_prio=false : result bit i for N-1 >= i >= 0 is set in result iff bit i is set in the input and all higher (more significant) bits j for j > i are not set in the input. For both lsb_prio=true and lsb_prio=false , result bit N (the most significant bit in the output) is only set if no bits in the input are set. Examples: one_hot(0b0011, lsb_prio=true) => 0b00001 -- note that an extra MSb has been appended to the output to potentially represent the \"all zeros\" case. one_hot(0b0111, lsb_prio=false) => 0b00100 . one_hot(0b00, lsb_prio=false) => 0b100 . one_hot(0b00, lsb_prio=true) => 0b100 -- note the output for one_hot is the same for the all-zeros case regardless of whether lsb_prio is true or false. This operation is useful for constructing match or switch operation semantics where a condition is matched against an ordered set of cases and the first match is chosen. It is also useful for one-hot canonicalizing, e.g. as a prelude to counting leading/trailing zeros.","title":"one_hot"},{"location":"ir_semantics/#control-oriented-operations","text":"For context note that, in XLS, operations are evaluated eagerly in a very general sense: all \"branches\" of computation may be evaluated in full before the result is selected via an operation such as one_hot_sel or sel . This model is amenable to pipeline-like hardware execution, where operations tend to be fixed in some spatial area and operations execute a single function, while interconnect is used for reconfiguration purposes. Towards this eager-evaluation-capable model, operations used within a function are generally not Turing-complete: operations such as counted_for require a finite bound so that they could be implemented using a finite amount of pipeline area. Operations such as dynamic_counted_for are an exception, where that operation will only be possible to use in a time-multiplexed code generation mode, such as the XLS sequential emitter, where arbitrary iteration to some dynamic bound is likely to be possible.","title":"Control-oriented operations"},{"location":"ir_semantics/#param","text":"A parameter to the current IR function, which can be used as an operand for operations within the function. Syntax Parameters have a special syntactic form distinct from other nodes, where they are listed directly in the function signature with their type. fn f(x: bits[32]) -> bits[32] { ret identity.2 = identity(x, id=2) } Types Value Type name str type type","title":"param"},{"location":"ir_semantics/#sel","text":"Selects between operands based on a selector value. Syntax result = sel(selector, cases=[case_{0}, ... , case_{N-1}], default=<default>) Types Value Type selector bits[M] case_{i} T default T result T","title":"sel"},{"location":"ir_semantics/#one_hot_sel","text":"Selects between operands based on a one-hot selector, OR -ing all selected cases if more than one case is selected. See one_hot for an example of the one-hot selector invariant. Note that when the selector is not one-hot, this operation is still well defined. Note that when one_hot operations are used to precondition the selector operand to one_hot_sel , the XLS optimizer will try to determine when they are unnecessary and subsequently eliminate them. Syntax result = one_hot_sel(selector, cases=[case_{0}, ... , case_{N-1}]) Types Value Type selector bits[N] case_{i} T result T The result is the logical OR of all cases case_{i} for which the corresponding bit i is set in the selector. When selector is one-hot this performs a select operation.","title":"one_hot_sel"},{"location":"ir_semantics/#priority_sel","text":"Selects between operands based on a selector, choosing the highest-priority case if more than one case is selected. Each bit in the selector corresponds to a case, with the least significant bit corresponding to the first case and having the highest priority. If there are no bits in the selector set, no case is selected and the default value of 0 is chosen. See one_hot for an example of the one-hot selector invariant. Note that when the selector is not one-hot, this operation is still well defined. Note that when one_hot operations are used to precondition the selector operand to priority_sel , the XLS optimizer will try to determine when they are unnecessary and subsequently eliminate them. Syntax result = priority_sel(selector, cases=[case_{0}, ... , case_{N-1}]) Types Value Type selector bits[N] case_{i} T result T The result is the first case case_{i} for which the corresponding bit i is set in the selector. If the selector is known to be one-hot, then the priority_sel() operation is equivalent to a one_hot_sel() .","title":"priority_sel"},{"location":"ir_semantics/#invoke","text":"Invokes a function. The return value for the invoked function is the result value. Syntax result = invoke(operand_{0}, ... , operand_{N-1}, to_apply=<to_apply>) Types Value Type init T result T Keyword arguments Keyword Type Required Default Description to_apply string yes Name of the function to use as the loop body","title":"invoke"},{"location":"ir_semantics/#map","text":"Applies a function to the elements of an array and returns the result as an array. Syntax result = map(operand, to_apply=<to_apply>) Types Value Type operand array[T] result array[U] Keyword arguments Keyword Type Required Default Description to_apply string yes Name of the function to apply to each element of the operand","title":"map"},{"location":"ir_semantics/#dynamic_counted_for","text":"Invokes a dynamic-trip count loop. Syntax result = counted_for(init, trip_count, stride, body=<body>, invariant_args=<inv_args>) Types Value Type init T trip_count bits[N], treated as unsigned stride bits[M], treated as signed , result T Keyword arguments Keyword Type Required Default Description invariant_args array of yes Names of the invariant operands as the loop body body string yes Name of the function to use as the loop body dynamic_counted_for invokes the function body trip_count times, passing loop-carried data that starts with value init . The induction variable is incremented by stride after each iteration. The first argument passed to body is the induction variable -- presently, the induction variable always starts at zero and increments by stride after every trip. The second argument passed to body is the loop-carry data. The return type of body must be the same as the type of the init loop carry data. The value returned from the last trip is the result of the counted_for expression. All subsequent arguments passed to body are passed from invariant_args ; e.g. if there are two members in invariant_args those values are passed as the third and fourth arguments. Therefore body should have a signature that matches the following: body(i, loop_carry_data, [invariant_arg0, invariant_arg1, ...]) Note that we currently inspect the body function to see what type of induction variable ( i above) it accepts in order to pass an i value of that type. trip_count must have fewer bits than i and stride should have fewer than or equal number of bits to i . Code generation support for dynamic_counted_for is limited because the pipeline generator cannot handle an unknown trip count.","title":"dynamic_counted_for"},{"location":"ir_semantics/#counted_for","text":"Invokes a fixed-trip count loop. Syntax result = counted_for(init, trip_count=<trip_count>, stride=<stride>, body=<body>, invariant_args=<inv_args>) Types Value Type init T result T Keyword arguments Keyword Type Required Default Description trip_count int64_t yes Trip count of the loop (number of times that the loop body will be executed) stride int64_t no 1 Stride of the induction variable invariant_args array of yes Names of the invariant operands as the loop body body string yes Name of the function to use as the loop body counted_for invokes the function body trip_count times, passing loop-carried data that starts with value init . The first argument passed to body is the induction variable -- presently, the induction variable always starts at zero and increments by stride after every trip. The second argument passed to body is the loop-carry data. The return type of body must be the same as the type of the init loop carry data. The value returned from the last trip is the result of the counted_for expression. All subsequent arguments passed to body are passed from invariant_args ; e.g. if there are two members in invariant_args those values are passed as the third and fourth arguments. Therefore body should have a signature that matches the following: body(i, loop_carry_data[, invariant_arg0, invariant_arg1, ...]) Note that we currently inspect the body function to see what type of induction variable ( i above) it accepts in order to pass an i value of that type.","title":"counted_for"},{"location":"ir_semantics/#sequencing-operations","text":"Some operations in XLS IR are sensitive to sequence order, similar to channel operations , but are not themselves channel-related. Tokens are used to determine the possible sequencing of these effects, and after_all can be used to join together tokens as a sequencing merge point for concurrent threads of execution described by different tokens.","title":"Sequencing operations"},{"location":"ir_semantics/#after_all","text":"Used to construct partial orderings among channel operations. result = after_all(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} token result token after_all can consume an arbitrary number of token operands including zero.","title":"after_all"},{"location":"ir_semantics/#other-side-effecting-operations","text":"Aside from channels operations such as send and receive several other operations have side-effects. Care must be taken when adding, removing, or transforming these operations, e.g., in the optimizer.","title":"Other side-effecting operations"},{"location":"ir_semantics/#assert","text":"Raises an error at software run-time (DSLX/IR interpretation, JIT execution, RTL simulation) if the given condition evaluates to false. The operation takes a literal string attribute which is included in the error message. This is a software-only operation and has no representation in the generated hardware. Tokens are used to connect the operation to the graph and order with respect to other side-effecting operations. result = assert(tkn, condition, message=<string>) result = assert(tkn, condition, message=<string>, label=<string>) Types Value Type tkn token condition bits[1] result token Keyword arguments Keyword Type Required Default Description message string yes Message to include in raised error label optional string yes Label to associate with the assert statement in the generated (System)Verilog","title":"assert"},{"location":"ir_semantics/#cover","text":"Records the number of times the given condition evaluates to true. Just like assert , this is a software-only construct and is not emitted in a final hardware design. Tokens are used to sequence this operation in the graph. result = cover(tkn, condition, label=<string>) Types Value Type tkn token condition bits[1] result token Keyword arguments Keyword Type Required Default Description label string yes Name associated with the counter.","title":"cover"},{"location":"ir_semantics/#gate","text":"Gates an arbitrarily-typed value based on a condition. The result of the operation is the data operand if the condition is true, otherwise the result is a zero value of the type of the data operand (i.e., the value is gated off). A helpful mnemonic is to think of this as analogous to an AND gate: if the condition is true , the value passes through, otherwise it's zeroed. This operation can reduce switching and may be used in power optimizations. This is intended for use in operand gating for power reduction, and the compiler may ultimately use it to perform register-level load-enable gating. The operation is considered side-effecting to prevent removal of the operation when the gated result (condition is false) is not observable. In this case the gate operation is still desirable because of the operations' effects on power consumption. result = gate(condition, data) Types Value Type condition bits[1] data T result T","title":"gate"},{"location":"ir_semantics/#rtl-level-operations","text":"These IR operations correspond to RTL-level constructs in the emitted Verilog. These operations are added and used in the code generation process and may only appear in blocks (not procs or functions).","title":"RTL-level operations"},{"location":"ir_semantics/#input_port","text":"Corresponds to an input port on a Verilog module. Syntax result = input_port() Types Value Type result T An input_port operation can be an arbitrary type.","title":"input_port"},{"location":"ir_semantics/#output_port","text":"Corresponds to an output port on a Verilog module. The value sent to the output port is the data operand. Syntax result = output_port(data) Types Value Type data T result T","title":"output_port"},{"location":"ir_semantics/#register_read","text":"Reads a value from a register. The register is defined on the block. Syntax result = register_read(register=<register_name>) Types Value Type result T The type T of the result of the operation is the type of the register. Keyword arguments Keyword Type Required Default Description register string yes Name of the register to read","title":"register_read"},{"location":"ir_semantics/#register_write","text":"Writes a value to a register. The write to the register may be conditioned upon an optional load-enable and/or reset signal. The register is defined on the block. Syntax result = register_write(data, load_enable=<load_enable>, reset=<reset>, register=<register_name>) Types Value Type data T load_enable bits[1] (optional) reset bits[1] (optional) result () (empty tuple) Keyword arguments Keyword Type Required Default Description register string yes Name of the register to write The type T of the data operand must be the same as the the type of the register.","title":"register_write"},{"location":"ir_semantics/#instantiation_input","text":"Corresponds to a single input port of an instantiation. An instantiation is a block-scoped construct that represents a module instantiation at the Verilog level. Each instantation_input operation corresponds to a particular port of the instantiated object, so generally a single instantiation can have multiple associated instantiation_input operations (one for each input port). Syntax result = instantiation_input(data, instantiation=<instantiation>, port_name=<port_name>) Types Value Type data T result () Keyword arguments Keyword Type Required Default Description instantiation string yes Name of the instantiation. port_name string yes Name of the associated port of the instantiation. The type T of the data operand must be the same as the the type of the associated input port of the instantiated object.","title":"instantiation_input"},{"location":"ir_semantics/#instantiation_output","text":"Corresponds to a single output port of an instantiation. An instantiation is a block-scoped construct that represents a module instantiation at the Verilog level. Each instantation_output operation corresponds to a output particular port of the instantiated object, so generally a single instantiation can have multiple associated instantiation_output operations (one for each output port). Syntax result = instantiation_output(instantiation=<instantiation>, port_name=<port_name>) Types Value Type result T Keyword arguments Keyword Type Required Default Description instantiation string yes Name of the instantiation. port_name string yes Name of the associated port of the instantiation. The type T of the result is type of the associated output port of the instantiated object.","title":"instantiation_output"},{"location":"ir_visualization/","text":"IR Visualization The XLS IR visualization web app presents the IR in text and graphical form side-by-side and enables interactive exploration of the IR. Running the web app To build and launch the IR visualization web app run: bazel run -c opt //xls/visualization/ir_viz:app -- --delay_model = unit Then visit http://localhost:5000 in a browser. Screenshot The screenshot below shows a zoomed-in portion of the IR graph for the fp_adder benchmark. The highlighted path in blue is the timing critical path the through the graph. Usage Text IR The left hand side of the UI shows the IR in text form in an editable text box. The IR may be entered or loaded in several ways: Upload from a file on the local file system via the Upload button. Enter directly by typing in the text box or cut and pasting. Load a pre-compiled benchmark via the Benchmarks button. The IR is from the benchmark after optimizations. The text IR is parsed as you type. The result of the parse ( OK or an error) appears in an alert at the bottom of the text box. On successful parsing all identifiers in the IR will be shown in bold. IR graph The right-hand side of the UI shows the IR in graphical form. Clicking on the View Graph button renders the text IR on the left hand side as a graph. The View Graph button is enabled only if the IR is parsed successfully. The graph view may be manipulated as follows: Zoom The mouse scroll wheel zooms the view of the IR graph. Pan Clicking and holding the left mouse button down in the graph panel (while not on a graph element) and moving the mouse pans the graph. Moving nodes Nodes in the graph are moved by clicking and holding on the node and moving the mouse. Focusing on nodes Clicking on a node in the graph while holding down the control key scrolls the respective definition of the node in the text IR into view in the text box. Similarly, control clicking on an identifier in the text IR zooms and centers the graph view on the respective node. Node colors Every node in the graph is assigned a color on a spectrum from white ( #FFFFFF ) to red ( #FF0000 ) depending on the modeled latency of the operation. The nodes with the longest latency in the graph are assigned red. Nodes with zero latency are assigned white. Hovering on IR elements Hovering on nodes and edges in the graph highlights the corresponding element in the text IR and vice versa. In the text IR, the definition and all uses of the IR value are highlighted when a node is highlighted. When a graph edge is highlighted, the definition and corresponding use are highlighted in the text IR. Information about a highlighted node (identifier in text IR) is displayed in a box above the IR graph. This information includes: The definition of the IR value in text form. Estimate of the delay in picoseconds of the corresponding operation. The delay estimation methodology is described here . Any known bits of the value as determined by the query engine (https://github.com/google/xls/tree/main/xls/passes/query_engine.h). Selecting nodes Nodes in the graph may be in a selected or deselected state. Clicking on a node in the graph or identifier in the text IR toggles the selection state. A selected node (identifier in IR text) is shown with a blue border. Nodes and edges which are neighbors of selected nodes (the selection frontier) are shown in orange. Clicking on an empty area of the graph deselects all nodes. Showing only selected nodes The toggle Show only selected nodes controls whether to show the entire graph or only the selected node and those elements in the selection frontier. Showing only selected nodes can be used to display only a subgraph of interest. For large graphs which are slow to render in their entirety, this mechanism can be used to interactively explore parts of the graph. When showing only selected nodes, the graph maybe be expanded by selecting additional nodes to add to the graph. The graph is re-rendered to include the newly selected node. Similarly, nodes may be removed from the graph by deselecting nodes. Selecting the critical path The button Critical Path selects exactly those nodes which are on the critical path as determined by XLS's timing model. This may be used with the Show only selected nodes toggle to show a graph containing only critical path elements and neighbors. In the screenshot above, the selected critical path is shown in blue.","title":"Visualizer"},{"location":"ir_visualization/#ir-visualization","text":"The XLS IR visualization web app presents the IR in text and graphical form side-by-side and enables interactive exploration of the IR.","title":"IR Visualization"},{"location":"ir_visualization/#running-the-web-app","text":"To build and launch the IR visualization web app run: bazel run -c opt //xls/visualization/ir_viz:app -- --delay_model = unit Then visit http://localhost:5000 in a browser.","title":"Running the web app"},{"location":"ir_visualization/#screenshot","text":"The screenshot below shows a zoomed-in portion of the IR graph for the fp_adder benchmark. The highlighted path in blue is the timing critical path the through the graph.","title":"Screenshot"},{"location":"ir_visualization/#usage","text":"","title":"Usage"},{"location":"ir_visualization/#text-ir","text":"The left hand side of the UI shows the IR in text form in an editable text box. The IR may be entered or loaded in several ways: Upload from a file on the local file system via the Upload button. Enter directly by typing in the text box or cut and pasting. Load a pre-compiled benchmark via the Benchmarks button. The IR is from the benchmark after optimizations. The text IR is parsed as you type. The result of the parse ( OK or an error) appears in an alert at the bottom of the text box. On successful parsing all identifiers in the IR will be shown in bold.","title":"Text IR"},{"location":"ir_visualization/#ir-graph","text":"The right-hand side of the UI shows the IR in graphical form. Clicking on the View Graph button renders the text IR on the left hand side as a graph. The View Graph button is enabled only if the IR is parsed successfully. The graph view may be manipulated as follows: Zoom The mouse scroll wheel zooms the view of the IR graph. Pan Clicking and holding the left mouse button down in the graph panel (while not on a graph element) and moving the mouse pans the graph. Moving nodes Nodes in the graph are moved by clicking and holding on the node and moving the mouse. Focusing on nodes Clicking on a node in the graph while holding down the control key scrolls the respective definition of the node in the text IR into view in the text box. Similarly, control clicking on an identifier in the text IR zooms and centers the graph view on the respective node.","title":"IR graph"},{"location":"ir_visualization/#node-colors","text":"Every node in the graph is assigned a color on a spectrum from white ( #FFFFFF ) to red ( #FF0000 ) depending on the modeled latency of the operation. The nodes with the longest latency in the graph are assigned red. Nodes with zero latency are assigned white.","title":"Node colors"},{"location":"ir_visualization/#hovering-on-ir-elements","text":"Hovering on nodes and edges in the graph highlights the corresponding element in the text IR and vice versa. In the text IR, the definition and all uses of the IR value are highlighted when a node is highlighted. When a graph edge is highlighted, the definition and corresponding use are highlighted in the text IR. Information about a highlighted node (identifier in text IR) is displayed in a box above the IR graph. This information includes: The definition of the IR value in text form. Estimate of the delay in picoseconds of the corresponding operation. The delay estimation methodology is described here . Any known bits of the value as determined by the query engine (https://github.com/google/xls/tree/main/xls/passes/query_engine.h).","title":"Hovering on IR elements"},{"location":"ir_visualization/#selecting-nodes","text":"Nodes in the graph may be in a selected or deselected state. Clicking on a node in the graph or identifier in the text IR toggles the selection state. A selected node (identifier in IR text) is shown with a blue border. Nodes and edges which are neighbors of selected nodes (the selection frontier) are shown in orange. Clicking on an empty area of the graph deselects all nodes.","title":"Selecting nodes"},{"location":"ir_visualization/#showing-only-selected-nodes","text":"The toggle Show only selected nodes controls whether to show the entire graph or only the selected node and those elements in the selection frontier. Showing only selected nodes can be used to display only a subgraph of interest. For large graphs which are slow to render in their entirety, this mechanism can be used to interactively explore parts of the graph. When showing only selected nodes, the graph maybe be expanded by selecting additional nodes to add to the graph. The graph is re-rendered to include the newly selected node. Similarly, nodes may be removed from the graph by deselecting nodes.","title":"Showing only selected nodes"},{"location":"ir_visualization/#selecting-the-critical-path","text":"The button Critical Path selects exactly those nodes which are on the critical path as determined by XLS's timing model. This may be used with the Show only selected nodes toggle to show a graph containing only critical path elements and neighbors. In the screenshot above, the selected critical path is shown in blue.","title":"Selecting the critical path"},{"location":"optimizations/","text":"XLS Optimizations XLS Optimizations Traditional compiler optimizations Dead Code Elimination (DCE) Common Subexpression Elimination (CSE) Constant Folding Assert Cleanup IO Simplifications Reassociation Narrowing Optimizations Select operations Arithmetic and shift operations Comparison operations Known-literals and ArrayIndex Strength Reductions Arithmetic Comparison Strength Reductions \"Simple\" Boolean Simplification Bit-slice optimizations Slicing sign-extended values Concat optimizations Hoisting a reverse above a concat Hoisting a bitwise operation above a concat Merging consecutive bit-slices Select optimizations Converting chains of Selects to into a single OneHotSelect Specializing select arms Consecutive selects with identical selectors Sparsifying selects with range analysis Binary Decision Diagram based optimizations BDD common subexpression elimination OneHot MSB elimination Traditional compiler optimizations Many optimizations from traditional compilers targeting CPUs also apply to the optimization of hardware. Common objectives of traditional compiler optimizations include exposing parallelism, reducing latency, and eliminating instructions. Often these translate directly into the primary objectives of hardware optimization of reducing delay and area. Dead Code Elimination (DCE) Dead Code Elimination (DCE for short) is usually one of the easiest and most straightforward optimization passes in compilers. The same is true for XLS (the implementation is in xls/passes/dce_pass.* ). Understanding the pass is also a good way to familiarize yourself with basics of the compiler IR, how to implement a pass, how to iterate over the nodes in the IR, how to query for node properties and so on. In general, DCE removes nodes from the IR that cannot be reached. Nodes can become unreachable by construction, for example, when a developer writes side-effect-free computations in DSLX that are disconnected from the function return ops. Certain optimization passes may also result in dead nodes. Let's look at the structure of the pass. The header file is straightforward, The DeadCodeEliminationPass is a function-level pass and hence derived from FunctionBasePass . Every function-level pass must implement the function RunOnFunctionBaseInternal and return a status indicating whether or not the pass made a change to the IR: class DeadCodeEliminationPass : public FunctionBasePass { public : DeadCodeEliminationPass () : FunctionBasePass ( \"dce\" , \"Dead Code Elimination\" ) {} ~ DeadCodeEliminationPass () override {} protected : // Iterate all nodes, mark and eliminate the unvisited nodes. absl :: StatusOr < bool > RunOnFunctionBaseInternal ( FunctionBase * f , const PassOptions & options , PassResults * results ) const override ; }; Now let's look at the implementation (in file xls/passes/dce_pass.cc ). After the function declaration: absl :: StatusOr < bool > DeadCodeEliminationPass :: RunOnFunctionBaseInternal ( FunctionBase * f , const PassOptions & options , PassResults * results ) const { There is a little lambda function testing whether a node is deletable or not: auto is_deletable = []( Node * n ) { return ! n -> function_base () -> HasImplicitUse ( n ) && ! OpIsSideEffecting ( n -> op ()); }; This function tests for two special classes of nodes. A node with implicit uses (defined in xls/ir/function.h ) is a function's return value. In case of procs, the return is not reached and may appear as dead. It must not be removed as XLS expects each function to have a return value. There are a number of side-effecting nodes, such as send/receive operations, asserts, covers, input / output ports, register read / writes, or parameters (and a few more). Because of their side effects, they must not be eliminated by DCE. Next the pass iterates over all nodes in the function and adds deletable nodes with no users to a worklist. Those are leaf nodes, they are the initial candidates for deletion: std :: deque < Node *> worklist ; for ( Node * n : f -> nodes ()) { if ( n -> users (). empty () && is_deletable ( n )) { worklist . push_back ( n ); } } Now on to the heart of the DCE algorithm. The algorithm iterates over nodes in the worklist until it is empty, popping elements from the front of the list and potentially adding new elements to the list. For example, assume there was a leaf node A with no further users. Further assume that its operand(s) only have node A as user, then the operand will be added to the worklist and visited in the next iteration over the worklist. There is a minor subtlety here - the code has to ensure that operands are only visited once, hence the use of a flat_hash_set<Node*> to check whether an operand has been visited already. After all operands have been visited and potentially added to the worklist, the original leaf node A is being removed and a corresponding logging statement (level 3) is generated. int64_t removed_count = 0 ; absl :: flat_hash_set < Node *> unique_operands ; while ( ! worklist . empty ()) { Node * node = worklist . front (); worklist . pop_front (); // A node may appear more than once as an operand of 'node'. Keep track of // which operands have been handled in a set. unique_operands . clear (); for ( Node * operand : node -> operands ()) { if ( unique_operands . insert ( operand ). second ) { if ( operand -> users (). size () == 1 && is_deletable ( operand )) { worklist . push_back ( operand ); } } } XLS_VLOG ( 3 ) << \"DCE removing \" << node -> ToString (); XLS_RETURN_IF_ERROR ( f -> RemoveNode ( node )); removed_count ++ ; } Finally, a pass has to indicate whether or not it made any changes to the IR. For this pass, this amounts to returning whether or not a single IR node has been DCE'ed: XLS_VLOG ( 2 ) << \"Removed \" << removed_count << \" dead nodes\" ; return removed_count > 0 ; } Common Subexpression Elimination (CSE) Common subexpression elimination is another example of a classic compiler optimization that equally applies to high-level synthesis. The heuristics on which specific expressions to commonize may differ, given that commonizing expressions can increase fan-out and connectivity of the IR, complicating place and route. Currently, XLS is greedy and does not apply heuristics. It commonizes any and all common expressions it can find. The CSE implementation can be found in files xls/passes/cse_pass.* . What does CSE actually do? The principles are quite simple and similar in classic control-flow based compilers. Yet, as mentioned, the heuristics may be moderately different. In CFG-based IRs, CSE would look for common expressions and substitute in temporary variables. For example, for code like this with the common expression a+b : x = a + b + c ; if ( cond ) { ... } else { y = b + a ; } The compiler would first determine that addition is commutative and that hence the expressions a+b and b+a can be canonicalized, eg., by ordering the operands alphabetically. Then the compiler would introduce a temporary variable for the expression and forward-substitute it into all occurances. For the example, the resulting code would be something like this: t1 = a + b x = t1 + c ; if ( cond ) { ... } else { y = t1 ; } In effect, an arithmetic operation has been traded against a register (or cache) access. Even in classic compilers, the CSE heuristics may consider factors such as the length and number of live ranges (and corresponding register pressure) to determine whether or not it may be better to just recompute the expression. In XLS's \"sea of nodes\" IR, this transformation is quite simple. Given a graph that contains multiple common subexpressions, for example: A B \\ / C1 A B \\ \\ / ... C2 / op(C2) XLS would find that C1 and C2 compute identical expressions and would simply replace the use of C2 with C1 , as in this graph (which will result in C2 being dead-code eliminated): A B \\ / C1 A B \\ \\ / ... C2 \\ op(C1) Now let's see how this is implemented in XLS. CSE is a function-level transformation and accordingly the pass is derived from FunctionBasePass . In the header file: class CsePass : public FunctionBasePass { public : CsePass () : FunctionBasePass ( \"cse\" , \"Common subexpression elimination\" ) {} ~ CsePass () override {} protected : absl :: StatusOr < bool > RunOnFunctionBaseInternal ( FunctionBase * f , const PassOptions & options , PassResults * results ) const override ; }; Several other optimizations passes expose new CSE opportunities. To make it easy to call CSE from these other passes, we declare a standalone function to call it. It accepts as input the function and returns a map containing the potential replacements of one node with another: absl :: StatusOr < bool > RunCse ( FunctionBase * f , absl :: flat_hash_map < Node * , Node *>* replacements ); CSE conceptually has to check for each op and its operands whether or not a similar op exists somewhere else in the IR. To make this more efficient, XLS first computes a 64-bit hash for each node. It combines the nodes' opcode with all operands' IDs into a vector and computes the hash function over this vector. auto hasher = absl :: Hash < std :: vector < int64_t >> (); auto node_hash = [ & ]( Node * n ) { std :: vector < int64_t > values_to_hash = { static_cast < int64_t > ( n -> op ())}; std :: vector < Node *> span_backing_store ; for ( Node * operand : GetOperandsForCse ( n , & span_backing_store )) { values_to_hash . push_back ( operand -> id ()); } // If this is slow because of many literals, the Literal values could be // combined into the hash. As is, all literals get the same hash value. return hasher ( values_to_hash ); }; Note that this procedure uses the function GetOperandsForCse to collect the operands. What does this function do? For nodes to be considered as equivalent, the operands must be in the same order. Commutative operands are agnostic to operand order. So in order to expand the opportunities for CSE, XLS sorts commutative operands by their ID. As an optimization, to avoid having to construct and return a full vector for each node and operands, the function gets a parameter to a vector of nodes to use as storage and returns a absl::Span<Node * const> over this backing store. This may look a bit confusing in the code but is really just a performance optimization: absl :: Span < Node * const > GetOperandsForCse ( Node * node , std :: vector < Node *>* span_backing_store ) { XLS_CHECK ( span_backing_store -> empty ()); if ( ! OpIsCommutative ( node -> op ())) { return node -> operands (); } span_backing_store -> insert ( span_backing_store -> begin (), node -> operands (). begin (), node -> operands (). end ()); SortByNodeId ( span_backing_store ); return * span_backing_store ; } Now on to the meat of the optimization pass. As always, we have to maintain whether or not the pass modified the IR: bool changed = false ; We store each first occurance of an expression in a map which is indexed by the expression's hash value. Since potentially there is no redundancy in the IR, we can pre-allocate this map to the size of function's IR. Note that non-common expressions may result in the same hash value. Because of that, node_buckets is a map from the hash value to a vector of nodes with the same hash value: absl :: flat_hash_map < int64_t , std :: vector < Node *>> node_buckets ; node_buckets . reserve ( f -> node_count ()); Now we iterate over the nodes in the IR, ignoring nodes that have side effects: for ( Node * node : TopoSort ( f )) { if ( OpIsSideEffecting ( node -> op ())) { continue ; } First thing to check is whether or not the op represents an expression that we have potentially already seen. If this is the first occurrance of the expression, which is efficient to check via the hash value, we store the op in node_buckets and continue with the next node. int64_t hash = node_hash ( node ); if ( ! node_buckets . contains ( hash )) { node_buckets [ hash ]. push_back ( node ); continue ; } Now it is getting more interesting. We may have found a node that is common with a previously seen node. We collect the nodes operands (again, this looks a bit complicated because of the performance optimization): std :: vector < Node *> node_span_backing_store ; absl :: Span < Node * const > node_operands_for_cse = GetOperandsForCse ( node , & node_span_backing_store ); Then we iterate over all previously seen nodes with the same hash value which are stored in node_buckets . Again, the may be multiple expressions with the same hash value, hence we have to iterate over all candidates with that hash value. For each candidate, we collect the operands and then check whether the node is definitely identical to a previously seen node. In this case the node's uses can be replaced: for ( Node * candidate : node_buckets . at ( hash )) { std :: vector < Node *> candidate_span_backing_store ; if ( node_operands_for_cse == GetOperandsForCse ( candidate , & candidate_span_backing_store ) && node -> IsDefinitelyEqualTo ( candidate )) { [...] If it was a match we replace the nodes, fill in the resulting replacement map, and mark the IR as modified. We also note whether a true match was found (via replaced ): XLS_VLOG ( 3 ) << absl :: StreamFormat ( \"Replacing %s with equivalent node %s\" , node -> GetName (), candidate -> GetName ()); XLS_RETURN_IF_ERROR ( node -> ReplaceUsesWith ( candidate )); if ( replacements != nullptr ) { ( * replacements )[ node ] = candidate ; } changed = true ; replaced = true ; break ; } } If, however, it turns out that while the hash value was identical but the ops were not identical, we have to update node_buckets and insert the new candidate. if ( ! replaced ) { node_buckets [ hash ]. push_back ( node ); } As a final step, we return whether or not the IR was modified: return changed ; Constant Folding Constant folding is another classic compiler optimization that equally applies to high-level synthesis. What does it do? Given an arithmetic expression that as inputs only as constant values, the compiler computes this expression at compile time and replaces it with the result. There are two complexities: The IR has to be updated after the transformation. More importantly, the evaluation of the expression must match the semantics of the target architecture! Both problems are solved elegantly in XLS. The IR update itself is trivial to do with the sea-of-nodes IR. For expression evaluation, XLS simply re-uses the interpreter, which implements the correct semantics. Let's look again at the implementation, which is in file xls/passes/constant_folding_pass.* . We define the pass as usual and maintain whether or not the pass modified the IR in a boolean variable changed : absl :: StatusOr < bool > ConstantFoldingPass :: RunOnFunctionBaseInternal ( FunctionBase * f , const PassOptions & options , PassResults * results ) const { bool changed = false ; We now iterate over all nodes in the IR and check whether the node only has literals as operands as well as whether it is safe to replace the node. for ( Node * node : TopoSort ( f )) { [...] if ( ! node -> Is < Literal > () && ! TypeHasToken ( node -> GetType ()) && ! OpIsSideEffecting ( node -> op ()) && std :: all_of ( node -> operands (). begin (), node -> operands (). end (), []( Node * o ) { return o -> Is < Literal > (); })) { XLS_VLOG ( 2 ) << \"Folding: \" << * node ; Here now comes the fun part. If the condition is true, XLS simply collects the operands in a vector and calls the interpreter to compute the result. Again, the interpreter has to implement the proper semantics, which leads to this exceedingly simple implementation. std :: vector < Value > operand_values ; for ( Node * operand : node -> operands ()) { operand_values . push_back ( operand -> As < Literal > () -> value ()); } XLS_ASSIGN_OR_RETURN ( Value result , InterpretNode ( node , operand_values )); XLS_RETURN_IF_ERROR ( node -> ReplaceUsesWithNew < Literal > ( result ). status ()); changed = true ; Assert Cleanup Asserts whose condition is known to be true (represented by a 1-bit value of 1) are removed as they will never trigger. This pass is implemented in file xls/passes/useless_assert_removel_pass.* and is rather trivial. Again, it shows in a simple way how to navigate the IR: https://github.com/google/xls/blob/main/xls/passes/useless_assert_removal_pass.cc#L27-L43 IO Simplifications Conditional sends and receives that have a condition known to be false are replaced with their input token (in the case of sends) or a tuple containing their input token and a literal representing the zero value of the appropriate channel (in the case of receives). Conditional sends and receives that have a condition known to be true are replaced with unconditional sends and receives. These two transforms are implemented in file xls/passes/useless_io_removal_pass.* . Reassociation Reassociation in XLS uses the associative and commutative property of arithmetic operations (such as adds and multiplies) to rearrange expressions of identical operations to minimize delay and area. Delay is reduced by transforming chains of operations into balanced trees which reduces the critical-path delay. For example, given the following expression: This can be reassociated into the following balanced tree: The transformation has reduced the critical path through the expression from three adds down to two adds. Reassociation can also create opportunities for constant folding. If an expression contains multiple literal values (constants) the expressions can be reassociated to gather literals into the same subexpression which can then be folded. Generally this requires the operation to be commutative as well as associative. For example, given the following expression: This can be reassociated into: The right-most add of the two literals can be folded reducing the number of adds in the expression to two. Narrowing Optimizations The XLS compiler performs bitwise flow analysis , and so can deduce that certain bits in the output of operations are always-zero or always-one. As a result, after running this bit-level tracking flow analysis, some of the bits on the output of an operation may be \"known\". With known bits in the output of an operation, we can often narrow the operation to only produce the unknown bits (those bits that are not known static constants), which reduces the \"cost\" of the operation (amount of delay through the operation) by reducing its bitwidth. Select operations An example of this is select narrowing, as shown in the following -- beforehand we have three operands, but from our bitwise analysis we know that there are two constant bits in the MSb as well as two constant bits in the LSb being propagated from all of our input operands. Recognizing that property, we squeeze the one hot select operation -- in the \"after\" diagram below observe we've narrowed the operation by slicing the known-constant bits out of the one hot select operation, making it cheaper in terms of delay, and propagated the slices up to the input operands -- these slices being presented at the output of the operands may in turn let them narrow their operation and become cheaper, and this can continue transitively): Arithmetic and shift operations Most arithmetic ops support mixed bit widths where the operand widths may not be the same width as each other or the result. This provides opportunities for narrowing. Specifically (for multiplies, adds and subtracts): If the operands are wider than the result, the operand can be truncated to the result width. If the operation result is wider than the full-precision width of the operation, the operation result can be narrowed to the full-precision width then sign- or zero-extended (depending upon the sign of the operation) to the original width to produce the desired result. The full-precision width of an add or subtract is one more than the width of the widest operand, and the full-precision width of a multiply is the sum of the operand widths. If the most-significant bit of an operand are zeros (for unsigned operations) or the same as the sign-bit value (for signed operations), the operand can be narrowed to remove these known bits. As a special case, adds can be narrowed if the least-significant bits of an is all zeros. The add operation is narrowed to exclude this range of least-significant bits. The least-significant bits of the result are simply the least-significant bits of the non-zero operand: Similarly, if the most-significant bits of the shift-amount of a shift operation are zero the shift amount can be narrowed. Comparison operations Leading and trailing bits can be stripped from the operands of comparison operations if these bits do not affect the result of the comparison. For unsigned comparisons, leading and trailing bits which are identical between the two operands can be stripped which narrows the comparison and reduces the cost of the operation. Signed comparison are more complicated to handle because the sign bit (most-significant bit) affects the interpretation of the value of the remaining bits. Stripping leading bits must preserve the sign bit of the operand. Known-literals and ArrayIndex The narrowing pass also converts any node that is determined by range analysis to have a range containing only one value into a literal. As a further extension of this idea, it also converts ArrayIndex nodes that are determined to have a small number of possible indices into select chains. This latter optimization is sometimes harmful, so we currently hide it behind the --convert_array_index_to_select=<n> flag in opt_main and benchmark_main , where <n> controls the number of possible array indices above which the optimization does not fire. The right number to put there is highly contextual, since this optimization relies heavily on later passes to clean up its output. Strength Reductions Arithmetic Comparison Strength Reductions When arithmetic comparisons occur with respect to constant values, comparisons which are be arithmetic in the general case may be strength reduced to more boolean-analyzeable patterns; for example, comparison with mask constants: u4:0bwxyz > u4:0b0011 Can be strength reduced -- for the left hand side to be greater one of the wx bits must be set, so we can simply replace this with an or-reduction of wx . Similarly, a trailing-bit and-reduce is possible for less-than comparisons. NOTE These examples highlight a set of optimizations that are not applicable (profitable) on traditional CPUs: the ability to use bit slice values below what we'd traditionally think of as a fundamental comparison \"instruction\", which would nominally take a single cycle. \"Simple\" Boolean Simplification NOTE This optimization is a prelude to a more general optimization we expect to come in the near future that is based on Binary Decision Diagrams. It is documented largely as a historical note of an early / simple optimization approach. With the addition of the one_hot_select and its corresponding optimizations, a larger amount of boolean logic appears in XLS's optimized graphs (e.g. or-ing together bits in the selector to eliminate duplicate one_hot_select operands). Un-simplified boolean operations compound their delay on the critical path; with the process independent constant \\(\\tau\\) a single bit or might be \\(6\\tau\\) , which is just to say that having lots of dependent or operations can meaningfully add up in the delay estimation for the critical path. If we can simplify several layers of boolean operations into one operation (say, perhaps with inputs optionally inverted) we could save a meaningful number of tau versus a series of dependent boolean operations. For a simple approach to boolean simplification: The number of parameters to the boolean function is limited to three inputs The truth table is computed for the aggregate boolean function by flowing input bit vectors through all the boolean operation nodes. The result bit vectors on the output frontier are matched the resulting truth table from the flow against one of our standard operations (perhaps with the input operands inverted). The algorithm starts by giving x and y their vectors (columns) from the truth table, enumerating all possible bit combinations for those operands. For example, consider two operands and a (bitwise) boolean function, the following is the truth table: X Y | X+~Y ----+------ 0 0 | 1 0 1 | 0 1 0 | 1 1 1 | 1 Each column in this table is a representation of the possibilities for a node in the graph to take on, as a vector. After giving the vector [0, 0, 1, 1] to the first input node (which is arbitrarily called X) and the vector [0, 1, 0, 1] to the second input node (which is arbitrarily called Y), and flowing those bit vectors through a network of boolean operations, if you wind up with a vector [1, 0, 1, 1] at the end, it is sound to replace that whole network with the expression X+~Y . Similarly, if the algorithm arrived at the vector [1, 1, 1, 1] at the end of the network, you could replace the result with a literal 1 , because it has been proven for all input operand possibilities the result is always 1 in every bit. Effectively, this method works by brute force enumerating all the possibilities for input bits and operating on all of those possibilities at the same time. In the end, the algorithm arrives at a composite boolean function that can be pattern matched against XLS's set of \"simple boolean functions\". In the following example there are two nodes on the \"input frontier\" for the boolean operations ( sub and add , which we \"rename\" to x and y for the purposes of analysis). As shown in the picture, the algorithm starts flowing the bit vector, which represents all possible input values for x and y . You can see that the not which produces \\(\\bar{x}\\) (marked with a red star) simply inverts all the entries in the vector and corresponds to the \\(\\bar{x}\\) column in the truth table. Similarly the and operation joins the two vector with the binary & operation, and finally we end up with the blue-starred bit vector on the \"output frontier\", feeding the dependent one_hot_select (marked as ohs in the diagram). When we resolve that final result bit vector with the blue star against our table of known function substitutions, we see that the final result can be replaced with a node that is simply or(x, y) , saving two unnecessary levels of logic, and reducing the critical path delay in this example from something like \\(13\\tau\\) to something like \\(6\\tau\\) . This basic procedure is then extended to permit three variables on the input frontier to the boolean expression nodes, and the \"known function\" table is extended to include all of our supported logical operators (i.e. nand , nor , xor , and , or ) with bit vectors for all combinations of inputs being present, and when present, either asserted, or their inversions (e.g. we can find \\(nand(\\bar{X}, Y)\\) even though X is inverted). Bit-slice optimizations Bit-slice operations narrow values by selecting a contiguous subset of bits from their operand. Bit-slices are zero-cost operations as no computation is performed. However, optimization of bit-slices can be beneficial as bit-slices can interfere with optimizations and hoisting bit-slices can narrow other operations reducing their computation cost. Slicing sign-extended values A bit-slice of a sign-extended value is a widening operation followed by a narrowing operation and can be optimized. The details of the transformation depends upon relative position of the slice and the sign bit of the original value. Let sssssssXXXXXXXX be the sign-extended value where s is the sign bit and X represents the bits of the original value. There are three possible cases: Slice is entirely within the sign-extend operand. Transformation: replace the bit-slice of the sign-extended value with a bit-slice of the original value. Slice spans the sign bit of the sign-extend operand. Transformation: slice the most significant bits from the original value and sign-extend the result. Slice is entirely within the sign-extended bits. Transformation: slice the sign bit from the original value and sign-extend it. To avoid introducing additional sign-extension operations cases (2) and (3) should only be performed if the bit-slice is the only user of the sign-extension. Concat optimizations Concat (short for concatenation) operations join their operands into a single word. Like BitSlice s, Concat operations have no cost since since they simply create a new label to refer to a set of bits, performing no actual computation. However, Concat optimizations can still provide benefit by reducing the number of IR nodes (increases human readability) or by refactoring the IR in a way that allows other optimizations to be applied. Several Concat optimizations involve hoisting an operation on one or more Concat s to above the Concat such that the operation is applied on the Concat operands directly. This may provide opportunities for optimization by bringing operations which actually perform logic closer to other operations performing logic. Hoisting a reverse above a concat A Reverse operation reverses the order of the bits of the input operand. If a Concat is reversed and the Concat has no other consumers except for reduction operations (which are not sensitive to bit order), we hoist the Reverse above the Concat . In the modified IR, the Concat input operands are Reverse 'd and then concatenated in reverse order, e.g. : Reverse(Concat(a, b, c)) => Concat(Reverse(c), Reverse(b), Reverse(a)) Hoisting a bitwise operation above a concat If the output of multiple Concat operations are combined with a bitwise operation, the bitwise operation is hoisted above the Concat s. In the modified IR, we have a single Concat whose operands are bitwise'd BitSlice s of the original Concat s, e.g. : Or(Concat(A, B), Concat(C, D)), where A,B,C, and D are 1-bit values => Concat(Or(Concat(A, B)[1], Concat(C, D)[1]), Or(Concat(A, B)[0], Concat(C, D)[0])) In the case that an added BitSlice exactly aligns with an original Concat operand, other optimizations (bit slice simplification, constant folding, dead code elimination) will replace the BitSlice with the operand, e.g. for the above example: => Concat(Or(A, C), Or(B, D)) Merging consecutive bit-slices If consecutive Concat operands are consecutive BitSlice s, we create a new, merged BitSlice spanning the range of the consecutive BitSlice s. Then, we create a new Concat that includes this BitSlice as an operand, e.g. Concat(A[3:2], A[1:0], B) => Concat(A[3:0], B) This optimization is sometimes helpful and sometimes harmful, e.g. in the case that there are other consumers of the original BitSlice s, we may only end up adding more IR nodes since the original BitSlice s will not be removed by DCE once they are replaced in the Concat . Some adjustments might be able to help with this issue. An initial attempt at this limited the application of this optimization to cases where the given Concat is the only consumer of the consecutive BitSlice s. This limited the more harmful applications of this optimization, but also reduced instances in which the optimization was beneficially applied (e.g. the same consecutive BitSlice s could be merged in multiple Concat s). Select optimizations XLS supports two types of select operations: Select (opcode Op::kSel ) is a traditional multiplexer. An n -bit binary-encoded selector chooses among 2** n inputs. OneHotSelect (opcode Op::kOneHotSel ) has one bit in the selector for each input. The output of the operation is equal to the logical-or reduction of the inputs corresponding to the set bits of the selector. Generally, a OneHotSelect is lower latency and area than a Select as a Select is effectively a decode operation followed by a OneHotSelect . Converting chains of Select s to into a single OneHotSelect A linear chain of binary Select operations may be produced by the front end to select amongst a number of different values. This is equivalent to nested ternary operators in C++. A chain of Select s has high latency, but latency may be reduced by converting the Select chain into a single OneHotSelect . This may only be performed if the single-bit selectors of the Select instructions are one-hot (at most one selector is set at one time). In this case, the single-bit Select selectors are concatenated together to produce the selector for the one hot. This effectively turns a serial operation into a lower latency parallel one. If the selectors of the original Select instructions can be all zero (but still at most one selector is asserted) the transformation is slightly modified. An additional selector which is the logical NOR of all of the original Select selector bits is appended to the OneHotSelect selector and the respective case is the value selected when all selector bits are zero ( case_0 in the diagram). Specializing select arms Within any given arm of a Select multiplexer, we can assume that the selector has the specific value required to select that arm. This assumption is safe because in the event that the selector has a different value, the arm is dead. This makes it possible to try specialize the arms based on the selector value. In the above example, a LHS of Selector + x could be simplified to 0 + x . The current optimization simply substitutes any usages of the selector in a Select arm with its known value in that arm. Future improvements could use range based analysis or other techniques to narrow down the possible values of any variables within the selector, and use that information to optimize the select arms. Consecutive selects with identical selectors Two consecutive two-way selects which use the same selector can be compacted into a single select statement. The selector only has two states so only two of the three different cases may be selected. The third is dead. Visually, the transformation looks like: The specific cases which remain in the new select instruction depends on whether the upper select feeds the true or false input of the lower select. Sparsifying selects with range analysis If range analysis determines that the selector of a select has fewer possible values than the number of cases, we can do a form of dead code elimination to remove the impossible cases. Currently, we do this in the following way: // Suppose bar has interval set {[1, 4], [6, 7], [10, 13]} foo = sel(bar, cases=[a1, ..., a16]) // Sparsification will lead to the following code: foo = sel((bar >= 1) && (bar <= 4), cases=[ sel((bar >= 6) && (bar <= 7), cases=[ sel(bar - 10, cases=[a11, ..., a14], default=0), sel(bar - 6, cases=[a7, a8], default=0) ]), sel(bar - 1, cases=[a2, ..., a5], default=0) ]) This adds a little bit of code for the comparisons and subtractions but is generally worth it since eliminating a case branch can be a big win. Binary Decision Diagram based optimizations A binary decision diagram (BDD) is a data structure that can represent arbitrary boolean expressions. Properties of the BDD enable easy determination of relationships between different expressions (equality, implication, etc.) which makes them useful for optimization and analysis BDD common subexpression elimination Determining whether two expression are equivalent is trivial using BDDs. This can be used to identify operations in the graph which produce identical results. BDD CSE is an optimization pass which commons these equivalent operations. OneHot MSB elimination A OneHot instruction returns a bitmask with exactly one bit equal to 1. If the input is not all-0 bits, this is the first 1 bit encountered in the input going from least to most significant bit or vice-versa depending on the priority specified. If the input is all-0 bits, the most significant bit of the OneHot output is set to 1. The semantics of OneHot are described in detail here . If the MSB of a OneHot does not affect the functionality of a program, we replace the MSB with a 0-bit, e.g. OneHot(A) such that the MSB has no effect \u21d2 Concat(0, OneHot(A)[all bits except MSB])) This can open up opportunities for further optimization. To determine if a OneHot \u2019s MSB has any effect on a function, we iterate over the OneHot \u2019s post-dominators. We use the BDD to test if setting the OneHot \u2019s MSB to 0 or to 1 (other bits are 0 in both cases) implies the same value for the post-dominator node. If so, we know the value of the MSB cannot possibly affect the function output, so the MSB can safely be replaced with a 0 bit. Note: This approach assumes that IR nodes do not have any side effects. When IR nodes with side effects are introduced (i.e. channels) the analysis for this optimization will have to be adjusted slightly to account for this.","title":"Optimizations"},{"location":"optimizations/#xls-optimizations","text":"XLS Optimizations Traditional compiler optimizations Dead Code Elimination (DCE) Common Subexpression Elimination (CSE) Constant Folding Assert Cleanup IO Simplifications Reassociation Narrowing Optimizations Select operations Arithmetic and shift operations Comparison operations Known-literals and ArrayIndex Strength Reductions Arithmetic Comparison Strength Reductions \"Simple\" Boolean Simplification Bit-slice optimizations Slicing sign-extended values Concat optimizations Hoisting a reverse above a concat Hoisting a bitwise operation above a concat Merging consecutive bit-slices Select optimizations Converting chains of Selects to into a single OneHotSelect Specializing select arms Consecutive selects with identical selectors Sparsifying selects with range analysis Binary Decision Diagram based optimizations BDD common subexpression elimination OneHot MSB elimination","title":"XLS Optimizations"},{"location":"optimizations/#traditional-compiler-optimizations","text":"Many optimizations from traditional compilers targeting CPUs also apply to the optimization of hardware. Common objectives of traditional compiler optimizations include exposing parallelism, reducing latency, and eliminating instructions. Often these translate directly into the primary objectives of hardware optimization of reducing delay and area.","title":"Traditional compiler optimizations"},{"location":"optimizations/#dead-code-elimination-dce","text":"Dead Code Elimination (DCE for short) is usually one of the easiest and most straightforward optimization passes in compilers. The same is true for XLS (the implementation is in xls/passes/dce_pass.* ). Understanding the pass is also a good way to familiarize yourself with basics of the compiler IR, how to implement a pass, how to iterate over the nodes in the IR, how to query for node properties and so on. In general, DCE removes nodes from the IR that cannot be reached. Nodes can become unreachable by construction, for example, when a developer writes side-effect-free computations in DSLX that are disconnected from the function return ops. Certain optimization passes may also result in dead nodes. Let's look at the structure of the pass. The header file is straightforward, The DeadCodeEliminationPass is a function-level pass and hence derived from FunctionBasePass . Every function-level pass must implement the function RunOnFunctionBaseInternal and return a status indicating whether or not the pass made a change to the IR: class DeadCodeEliminationPass : public FunctionBasePass { public : DeadCodeEliminationPass () : FunctionBasePass ( \"dce\" , \"Dead Code Elimination\" ) {} ~ DeadCodeEliminationPass () override {} protected : // Iterate all nodes, mark and eliminate the unvisited nodes. absl :: StatusOr < bool > RunOnFunctionBaseInternal ( FunctionBase * f , const PassOptions & options , PassResults * results ) const override ; }; Now let's look at the implementation (in file xls/passes/dce_pass.cc ). After the function declaration: absl :: StatusOr < bool > DeadCodeEliminationPass :: RunOnFunctionBaseInternal ( FunctionBase * f , const PassOptions & options , PassResults * results ) const { There is a little lambda function testing whether a node is deletable or not: auto is_deletable = []( Node * n ) { return ! n -> function_base () -> HasImplicitUse ( n ) && ! OpIsSideEffecting ( n -> op ()); }; This function tests for two special classes of nodes. A node with implicit uses (defined in xls/ir/function.h ) is a function's return value. In case of procs, the return is not reached and may appear as dead. It must not be removed as XLS expects each function to have a return value. There are a number of side-effecting nodes, such as send/receive operations, asserts, covers, input / output ports, register read / writes, or parameters (and a few more). Because of their side effects, they must not be eliminated by DCE. Next the pass iterates over all nodes in the function and adds deletable nodes with no users to a worklist. Those are leaf nodes, they are the initial candidates for deletion: std :: deque < Node *> worklist ; for ( Node * n : f -> nodes ()) { if ( n -> users (). empty () && is_deletable ( n )) { worklist . push_back ( n ); } } Now on to the heart of the DCE algorithm. The algorithm iterates over nodes in the worklist until it is empty, popping elements from the front of the list and potentially adding new elements to the list. For example, assume there was a leaf node A with no further users. Further assume that its operand(s) only have node A as user, then the operand will be added to the worklist and visited in the next iteration over the worklist. There is a minor subtlety here - the code has to ensure that operands are only visited once, hence the use of a flat_hash_set<Node*> to check whether an operand has been visited already. After all operands have been visited and potentially added to the worklist, the original leaf node A is being removed and a corresponding logging statement (level 3) is generated. int64_t removed_count = 0 ; absl :: flat_hash_set < Node *> unique_operands ; while ( ! worklist . empty ()) { Node * node = worklist . front (); worklist . pop_front (); // A node may appear more than once as an operand of 'node'. Keep track of // which operands have been handled in a set. unique_operands . clear (); for ( Node * operand : node -> operands ()) { if ( unique_operands . insert ( operand ). second ) { if ( operand -> users (). size () == 1 && is_deletable ( operand )) { worklist . push_back ( operand ); } } } XLS_VLOG ( 3 ) << \"DCE removing \" << node -> ToString (); XLS_RETURN_IF_ERROR ( f -> RemoveNode ( node )); removed_count ++ ; } Finally, a pass has to indicate whether or not it made any changes to the IR. For this pass, this amounts to returning whether or not a single IR node has been DCE'ed: XLS_VLOG ( 2 ) << \"Removed \" << removed_count << \" dead nodes\" ; return removed_count > 0 ; }","title":"Dead Code Elimination (DCE)"},{"location":"optimizations/#common-subexpression-elimination-cse","text":"Common subexpression elimination is another example of a classic compiler optimization that equally applies to high-level synthesis. The heuristics on which specific expressions to commonize may differ, given that commonizing expressions can increase fan-out and connectivity of the IR, complicating place and route. Currently, XLS is greedy and does not apply heuristics. It commonizes any and all common expressions it can find. The CSE implementation can be found in files xls/passes/cse_pass.* . What does CSE actually do? The principles are quite simple and similar in classic control-flow based compilers. Yet, as mentioned, the heuristics may be moderately different. In CFG-based IRs, CSE would look for common expressions and substitute in temporary variables. For example, for code like this with the common expression a+b : x = a + b + c ; if ( cond ) { ... } else { y = b + a ; } The compiler would first determine that addition is commutative and that hence the expressions a+b and b+a can be canonicalized, eg., by ordering the operands alphabetically. Then the compiler would introduce a temporary variable for the expression and forward-substitute it into all occurances. For the example, the resulting code would be something like this: t1 = a + b x = t1 + c ; if ( cond ) { ... } else { y = t1 ; } In effect, an arithmetic operation has been traded against a register (or cache) access. Even in classic compilers, the CSE heuristics may consider factors such as the length and number of live ranges (and corresponding register pressure) to determine whether or not it may be better to just recompute the expression. In XLS's \"sea of nodes\" IR, this transformation is quite simple. Given a graph that contains multiple common subexpressions, for example: A B \\ / C1 A B \\ \\ / ... C2 / op(C2) XLS would find that C1 and C2 compute identical expressions and would simply replace the use of C2 with C1 , as in this graph (which will result in C2 being dead-code eliminated): A B \\ / C1 A B \\ \\ / ... C2 \\ op(C1) Now let's see how this is implemented in XLS. CSE is a function-level transformation and accordingly the pass is derived from FunctionBasePass . In the header file: class CsePass : public FunctionBasePass { public : CsePass () : FunctionBasePass ( \"cse\" , \"Common subexpression elimination\" ) {} ~ CsePass () override {} protected : absl :: StatusOr < bool > RunOnFunctionBaseInternal ( FunctionBase * f , const PassOptions & options , PassResults * results ) const override ; }; Several other optimizations passes expose new CSE opportunities. To make it easy to call CSE from these other passes, we declare a standalone function to call it. It accepts as input the function and returns a map containing the potential replacements of one node with another: absl :: StatusOr < bool > RunCse ( FunctionBase * f , absl :: flat_hash_map < Node * , Node *>* replacements ); CSE conceptually has to check for each op and its operands whether or not a similar op exists somewhere else in the IR. To make this more efficient, XLS first computes a 64-bit hash for each node. It combines the nodes' opcode with all operands' IDs into a vector and computes the hash function over this vector. auto hasher = absl :: Hash < std :: vector < int64_t >> (); auto node_hash = [ & ]( Node * n ) { std :: vector < int64_t > values_to_hash = { static_cast < int64_t > ( n -> op ())}; std :: vector < Node *> span_backing_store ; for ( Node * operand : GetOperandsForCse ( n , & span_backing_store )) { values_to_hash . push_back ( operand -> id ()); } // If this is slow because of many literals, the Literal values could be // combined into the hash. As is, all literals get the same hash value. return hasher ( values_to_hash ); }; Note that this procedure uses the function GetOperandsForCse to collect the operands. What does this function do? For nodes to be considered as equivalent, the operands must be in the same order. Commutative operands are agnostic to operand order. So in order to expand the opportunities for CSE, XLS sorts commutative operands by their ID. As an optimization, to avoid having to construct and return a full vector for each node and operands, the function gets a parameter to a vector of nodes to use as storage and returns a absl::Span<Node * const> over this backing store. This may look a bit confusing in the code but is really just a performance optimization: absl :: Span < Node * const > GetOperandsForCse ( Node * node , std :: vector < Node *>* span_backing_store ) { XLS_CHECK ( span_backing_store -> empty ()); if ( ! OpIsCommutative ( node -> op ())) { return node -> operands (); } span_backing_store -> insert ( span_backing_store -> begin (), node -> operands (). begin (), node -> operands (). end ()); SortByNodeId ( span_backing_store ); return * span_backing_store ; } Now on to the meat of the optimization pass. As always, we have to maintain whether or not the pass modified the IR: bool changed = false ; We store each first occurance of an expression in a map which is indexed by the expression's hash value. Since potentially there is no redundancy in the IR, we can pre-allocate this map to the size of function's IR. Note that non-common expressions may result in the same hash value. Because of that, node_buckets is a map from the hash value to a vector of nodes with the same hash value: absl :: flat_hash_map < int64_t , std :: vector < Node *>> node_buckets ; node_buckets . reserve ( f -> node_count ()); Now we iterate over the nodes in the IR, ignoring nodes that have side effects: for ( Node * node : TopoSort ( f )) { if ( OpIsSideEffecting ( node -> op ())) { continue ; } First thing to check is whether or not the op represents an expression that we have potentially already seen. If this is the first occurrance of the expression, which is efficient to check via the hash value, we store the op in node_buckets and continue with the next node. int64_t hash = node_hash ( node ); if ( ! node_buckets . contains ( hash )) { node_buckets [ hash ]. push_back ( node ); continue ; } Now it is getting more interesting. We may have found a node that is common with a previously seen node. We collect the nodes operands (again, this looks a bit complicated because of the performance optimization): std :: vector < Node *> node_span_backing_store ; absl :: Span < Node * const > node_operands_for_cse = GetOperandsForCse ( node , & node_span_backing_store ); Then we iterate over all previously seen nodes with the same hash value which are stored in node_buckets . Again, the may be multiple expressions with the same hash value, hence we have to iterate over all candidates with that hash value. For each candidate, we collect the operands and then check whether the node is definitely identical to a previously seen node. In this case the node's uses can be replaced: for ( Node * candidate : node_buckets . at ( hash )) { std :: vector < Node *> candidate_span_backing_store ; if ( node_operands_for_cse == GetOperandsForCse ( candidate , & candidate_span_backing_store ) && node -> IsDefinitelyEqualTo ( candidate )) { [...] If it was a match we replace the nodes, fill in the resulting replacement map, and mark the IR as modified. We also note whether a true match was found (via replaced ): XLS_VLOG ( 3 ) << absl :: StreamFormat ( \"Replacing %s with equivalent node %s\" , node -> GetName (), candidate -> GetName ()); XLS_RETURN_IF_ERROR ( node -> ReplaceUsesWith ( candidate )); if ( replacements != nullptr ) { ( * replacements )[ node ] = candidate ; } changed = true ; replaced = true ; break ; } } If, however, it turns out that while the hash value was identical but the ops were not identical, we have to update node_buckets and insert the new candidate. if ( ! replaced ) { node_buckets [ hash ]. push_back ( node ); } As a final step, we return whether or not the IR was modified: return changed ;","title":"Common Subexpression Elimination (CSE)"},{"location":"optimizations/#constant-folding","text":"Constant folding is another classic compiler optimization that equally applies to high-level synthesis. What does it do? Given an arithmetic expression that as inputs only as constant values, the compiler computes this expression at compile time and replaces it with the result. There are two complexities: The IR has to be updated after the transformation. More importantly, the evaluation of the expression must match the semantics of the target architecture! Both problems are solved elegantly in XLS. The IR update itself is trivial to do with the sea-of-nodes IR. For expression evaluation, XLS simply re-uses the interpreter, which implements the correct semantics. Let's look again at the implementation, which is in file xls/passes/constant_folding_pass.* . We define the pass as usual and maintain whether or not the pass modified the IR in a boolean variable changed : absl :: StatusOr < bool > ConstantFoldingPass :: RunOnFunctionBaseInternal ( FunctionBase * f , const PassOptions & options , PassResults * results ) const { bool changed = false ; We now iterate over all nodes in the IR and check whether the node only has literals as operands as well as whether it is safe to replace the node. for ( Node * node : TopoSort ( f )) { [...] if ( ! node -> Is < Literal > () && ! TypeHasToken ( node -> GetType ()) && ! OpIsSideEffecting ( node -> op ()) && std :: all_of ( node -> operands (). begin (), node -> operands (). end (), []( Node * o ) { return o -> Is < Literal > (); })) { XLS_VLOG ( 2 ) << \"Folding: \" << * node ; Here now comes the fun part. If the condition is true, XLS simply collects the operands in a vector and calls the interpreter to compute the result. Again, the interpreter has to implement the proper semantics, which leads to this exceedingly simple implementation. std :: vector < Value > operand_values ; for ( Node * operand : node -> operands ()) { operand_values . push_back ( operand -> As < Literal > () -> value ()); } XLS_ASSIGN_OR_RETURN ( Value result , InterpretNode ( node , operand_values )); XLS_RETURN_IF_ERROR ( node -> ReplaceUsesWithNew < Literal > ( result ). status ()); changed = true ;","title":"Constant Folding"},{"location":"optimizations/#assert-cleanup","text":"Asserts whose condition is known to be true (represented by a 1-bit value of 1) are removed as they will never trigger. This pass is implemented in file xls/passes/useless_assert_removel_pass.* and is rather trivial. Again, it shows in a simple way how to navigate the IR: https://github.com/google/xls/blob/main/xls/passes/useless_assert_removal_pass.cc#L27-L43","title":"Assert Cleanup"},{"location":"optimizations/#io-simplifications","text":"Conditional sends and receives that have a condition known to be false are replaced with their input token (in the case of sends) or a tuple containing their input token and a literal representing the zero value of the appropriate channel (in the case of receives). Conditional sends and receives that have a condition known to be true are replaced with unconditional sends and receives. These two transforms are implemented in file xls/passes/useless_io_removal_pass.* .","title":"IO Simplifications"},{"location":"optimizations/#reassociation","text":"Reassociation in XLS uses the associative and commutative property of arithmetic operations (such as adds and multiplies) to rearrange expressions of identical operations to minimize delay and area. Delay is reduced by transforming chains of operations into balanced trees which reduces the critical-path delay. For example, given the following expression: This can be reassociated into the following balanced tree: The transformation has reduced the critical path through the expression from three adds down to two adds. Reassociation can also create opportunities for constant folding. If an expression contains multiple literal values (constants) the expressions can be reassociated to gather literals into the same subexpression which can then be folded. Generally this requires the operation to be commutative as well as associative. For example, given the following expression: This can be reassociated into: The right-most add of the two literals can be folded reducing the number of adds in the expression to two.","title":"Reassociation"},{"location":"optimizations/#narrowing-optimizations","text":"The XLS compiler performs bitwise flow analysis , and so can deduce that certain bits in the output of operations are always-zero or always-one. As a result, after running this bit-level tracking flow analysis, some of the bits on the output of an operation may be \"known\". With known bits in the output of an operation, we can often narrow the operation to only produce the unknown bits (those bits that are not known static constants), which reduces the \"cost\" of the operation (amount of delay through the operation) by reducing its bitwidth.","title":"Narrowing Optimizations"},{"location":"optimizations/#select-operations","text":"An example of this is select narrowing, as shown in the following -- beforehand we have three operands, but from our bitwise analysis we know that there are two constant bits in the MSb as well as two constant bits in the LSb being propagated from all of our input operands. Recognizing that property, we squeeze the one hot select operation -- in the \"after\" diagram below observe we've narrowed the operation by slicing the known-constant bits out of the one hot select operation, making it cheaper in terms of delay, and propagated the slices up to the input operands -- these slices being presented at the output of the operands may in turn let them narrow their operation and become cheaper, and this can continue transitively):","title":"Select operations"},{"location":"optimizations/#arithmetic-and-shift-operations","text":"Most arithmetic ops support mixed bit widths where the operand widths may not be the same width as each other or the result. This provides opportunities for narrowing. Specifically (for multiplies, adds and subtracts): If the operands are wider than the result, the operand can be truncated to the result width. If the operation result is wider than the full-precision width of the operation, the operation result can be narrowed to the full-precision width then sign- or zero-extended (depending upon the sign of the operation) to the original width to produce the desired result. The full-precision width of an add or subtract is one more than the width of the widest operand, and the full-precision width of a multiply is the sum of the operand widths. If the most-significant bit of an operand are zeros (for unsigned operations) or the same as the sign-bit value (for signed operations), the operand can be narrowed to remove these known bits. As a special case, adds can be narrowed if the least-significant bits of an is all zeros. The add operation is narrowed to exclude this range of least-significant bits. The least-significant bits of the result are simply the least-significant bits of the non-zero operand: Similarly, if the most-significant bits of the shift-amount of a shift operation are zero the shift amount can be narrowed.","title":"Arithmetic and shift operations"},{"location":"optimizations/#comparison-operations","text":"Leading and trailing bits can be stripped from the operands of comparison operations if these bits do not affect the result of the comparison. For unsigned comparisons, leading and trailing bits which are identical between the two operands can be stripped which narrows the comparison and reduces the cost of the operation. Signed comparison are more complicated to handle because the sign bit (most-significant bit) affects the interpretation of the value of the remaining bits. Stripping leading bits must preserve the sign bit of the operand.","title":"Comparison operations"},{"location":"optimizations/#known-literals-and-arrayindex","text":"The narrowing pass also converts any node that is determined by range analysis to have a range containing only one value into a literal. As a further extension of this idea, it also converts ArrayIndex nodes that are determined to have a small number of possible indices into select chains. This latter optimization is sometimes harmful, so we currently hide it behind the --convert_array_index_to_select=<n> flag in opt_main and benchmark_main , where <n> controls the number of possible array indices above which the optimization does not fire. The right number to put there is highly contextual, since this optimization relies heavily on later passes to clean up its output.","title":"Known-literals and ArrayIndex"},{"location":"optimizations/#strength-reductions","text":"","title":"Strength Reductions"},{"location":"optimizations/#arithmetic-comparison-strength-reductions","text":"When arithmetic comparisons occur with respect to constant values, comparisons which are be arithmetic in the general case may be strength reduced to more boolean-analyzeable patterns; for example, comparison with mask constants: u4:0bwxyz > u4:0b0011 Can be strength reduced -- for the left hand side to be greater one of the wx bits must be set, so we can simply replace this with an or-reduction of wx . Similarly, a trailing-bit and-reduce is possible for less-than comparisons. NOTE These examples highlight a set of optimizations that are not applicable (profitable) on traditional CPUs: the ability to use bit slice values below what we'd traditionally think of as a fundamental comparison \"instruction\", which would nominally take a single cycle.","title":"Arithmetic Comparison Strength Reductions"},{"location":"optimizations/#simple-boolean-simplification","text":"NOTE This optimization is a prelude to a more general optimization we expect to come in the near future that is based on Binary Decision Diagrams. It is documented largely as a historical note of an early / simple optimization approach. With the addition of the one_hot_select and its corresponding optimizations, a larger amount of boolean logic appears in XLS's optimized graphs (e.g. or-ing together bits in the selector to eliminate duplicate one_hot_select operands). Un-simplified boolean operations compound their delay on the critical path; with the process independent constant \\(\\tau\\) a single bit or might be \\(6\\tau\\) , which is just to say that having lots of dependent or operations can meaningfully add up in the delay estimation for the critical path. If we can simplify several layers of boolean operations into one operation (say, perhaps with inputs optionally inverted) we could save a meaningful number of tau versus a series of dependent boolean operations. For a simple approach to boolean simplification: The number of parameters to the boolean function is limited to three inputs The truth table is computed for the aggregate boolean function by flowing input bit vectors through all the boolean operation nodes. The result bit vectors on the output frontier are matched the resulting truth table from the flow against one of our standard operations (perhaps with the input operands inverted). The algorithm starts by giving x and y their vectors (columns) from the truth table, enumerating all possible bit combinations for those operands. For example, consider two operands and a (bitwise) boolean function, the following is the truth table: X Y | X+~Y ----+------ 0 0 | 1 0 1 | 0 1 0 | 1 1 1 | 1 Each column in this table is a representation of the possibilities for a node in the graph to take on, as a vector. After giving the vector [0, 0, 1, 1] to the first input node (which is arbitrarily called X) and the vector [0, 1, 0, 1] to the second input node (which is arbitrarily called Y), and flowing those bit vectors through a network of boolean operations, if you wind up with a vector [1, 0, 1, 1] at the end, it is sound to replace that whole network with the expression X+~Y . Similarly, if the algorithm arrived at the vector [1, 1, 1, 1] at the end of the network, you could replace the result with a literal 1 , because it has been proven for all input operand possibilities the result is always 1 in every bit. Effectively, this method works by brute force enumerating all the possibilities for input bits and operating on all of those possibilities at the same time. In the end, the algorithm arrives at a composite boolean function that can be pattern matched against XLS's set of \"simple boolean functions\". In the following example there are two nodes on the \"input frontier\" for the boolean operations ( sub and add , which we \"rename\" to x and y for the purposes of analysis). As shown in the picture, the algorithm starts flowing the bit vector, which represents all possible input values for x and y . You can see that the not which produces \\(\\bar{x}\\) (marked with a red star) simply inverts all the entries in the vector and corresponds to the \\(\\bar{x}\\) column in the truth table. Similarly the and operation joins the two vector with the binary & operation, and finally we end up with the blue-starred bit vector on the \"output frontier\", feeding the dependent one_hot_select (marked as ohs in the diagram). When we resolve that final result bit vector with the blue star against our table of known function substitutions, we see that the final result can be replaced with a node that is simply or(x, y) , saving two unnecessary levels of logic, and reducing the critical path delay in this example from something like \\(13\\tau\\) to something like \\(6\\tau\\) . This basic procedure is then extended to permit three variables on the input frontier to the boolean expression nodes, and the \"known function\" table is extended to include all of our supported logical operators (i.e. nand , nor , xor , and , or ) with bit vectors for all combinations of inputs being present, and when present, either asserted, or their inversions (e.g. we can find \\(nand(\\bar{X}, Y)\\) even though X is inverted).","title":"\"Simple\" Boolean Simplification"},{"location":"optimizations/#bit-slice-optimizations","text":"Bit-slice operations narrow values by selecting a contiguous subset of bits from their operand. Bit-slices are zero-cost operations as no computation is performed. However, optimization of bit-slices can be beneficial as bit-slices can interfere with optimizations and hoisting bit-slices can narrow other operations reducing their computation cost.","title":"Bit-slice optimizations"},{"location":"optimizations/#slicing-sign-extended-values","text":"A bit-slice of a sign-extended value is a widening operation followed by a narrowing operation and can be optimized. The details of the transformation depends upon relative position of the slice and the sign bit of the original value. Let sssssssXXXXXXXX be the sign-extended value where s is the sign bit and X represents the bits of the original value. There are three possible cases: Slice is entirely within the sign-extend operand. Transformation: replace the bit-slice of the sign-extended value with a bit-slice of the original value. Slice spans the sign bit of the sign-extend operand. Transformation: slice the most significant bits from the original value and sign-extend the result. Slice is entirely within the sign-extended bits. Transformation: slice the sign bit from the original value and sign-extend it. To avoid introducing additional sign-extension operations cases (2) and (3) should only be performed if the bit-slice is the only user of the sign-extension.","title":"Slicing sign-extended values"},{"location":"optimizations/#concat-optimizations","text":"Concat (short for concatenation) operations join their operands into a single word. Like BitSlice s, Concat operations have no cost since since they simply create a new label to refer to a set of bits, performing no actual computation. However, Concat optimizations can still provide benefit by reducing the number of IR nodes (increases human readability) or by refactoring the IR in a way that allows other optimizations to be applied. Several Concat optimizations involve hoisting an operation on one or more Concat s to above the Concat such that the operation is applied on the Concat operands directly. This may provide opportunities for optimization by bringing operations which actually perform logic closer to other operations performing logic.","title":"Concat optimizations"},{"location":"optimizations/#hoisting-a-reverse-above-a-concat","text":"A Reverse operation reverses the order of the bits of the input operand. If a Concat is reversed and the Concat has no other consumers except for reduction operations (which are not sensitive to bit order), we hoist the Reverse above the Concat . In the modified IR, the Concat input operands are Reverse 'd and then concatenated in reverse order, e.g. : Reverse(Concat(a, b, c)) => Concat(Reverse(c), Reverse(b), Reverse(a))","title":"Hoisting a reverse above a concat"},{"location":"optimizations/#hoisting-a-bitwise-operation-above-a-concat","text":"If the output of multiple Concat operations are combined with a bitwise operation, the bitwise operation is hoisted above the Concat s. In the modified IR, we have a single Concat whose operands are bitwise'd BitSlice s of the original Concat s, e.g. : Or(Concat(A, B), Concat(C, D)), where A,B,C, and D are 1-bit values => Concat(Or(Concat(A, B)[1], Concat(C, D)[1]), Or(Concat(A, B)[0], Concat(C, D)[0])) In the case that an added BitSlice exactly aligns with an original Concat operand, other optimizations (bit slice simplification, constant folding, dead code elimination) will replace the BitSlice with the operand, e.g. for the above example: => Concat(Or(A, C), Or(B, D))","title":"Hoisting a bitwise operation above a concat"},{"location":"optimizations/#merging-consecutive-bit-slices","text":"If consecutive Concat operands are consecutive BitSlice s, we create a new, merged BitSlice spanning the range of the consecutive BitSlice s. Then, we create a new Concat that includes this BitSlice as an operand, e.g. Concat(A[3:2], A[1:0], B) => Concat(A[3:0], B) This optimization is sometimes helpful and sometimes harmful, e.g. in the case that there are other consumers of the original BitSlice s, we may only end up adding more IR nodes since the original BitSlice s will not be removed by DCE once they are replaced in the Concat . Some adjustments might be able to help with this issue. An initial attempt at this limited the application of this optimization to cases where the given Concat is the only consumer of the consecutive BitSlice s. This limited the more harmful applications of this optimization, but also reduced instances in which the optimization was beneficially applied (e.g. the same consecutive BitSlice s could be merged in multiple Concat s).","title":"Merging consecutive bit-slices"},{"location":"optimizations/#select-optimizations","text":"XLS supports two types of select operations: Select (opcode Op::kSel ) is a traditional multiplexer. An n -bit binary-encoded selector chooses among 2** n inputs. OneHotSelect (opcode Op::kOneHotSel ) has one bit in the selector for each input. The output of the operation is equal to the logical-or reduction of the inputs corresponding to the set bits of the selector. Generally, a OneHotSelect is lower latency and area than a Select as a Select is effectively a decode operation followed by a OneHotSelect .","title":"Select optimizations"},{"location":"optimizations/#converting-chains-of-selects-to-into-a-single-onehotselect","text":"A linear chain of binary Select operations may be produced by the front end to select amongst a number of different values. This is equivalent to nested ternary operators in C++. A chain of Select s has high latency, but latency may be reduced by converting the Select chain into a single OneHotSelect . This may only be performed if the single-bit selectors of the Select instructions are one-hot (at most one selector is set at one time). In this case, the single-bit Select selectors are concatenated together to produce the selector for the one hot. This effectively turns a serial operation into a lower latency parallel one. If the selectors of the original Select instructions can be all zero (but still at most one selector is asserted) the transformation is slightly modified. An additional selector which is the logical NOR of all of the original Select selector bits is appended to the OneHotSelect selector and the respective case is the value selected when all selector bits are zero ( case_0 in the diagram).","title":"Converting chains of Selects to into a single OneHotSelect"},{"location":"optimizations/#specializing-select-arms","text":"Within any given arm of a Select multiplexer, we can assume that the selector has the specific value required to select that arm. This assumption is safe because in the event that the selector has a different value, the arm is dead. This makes it possible to try specialize the arms based on the selector value. In the above example, a LHS of Selector + x could be simplified to 0 + x . The current optimization simply substitutes any usages of the selector in a Select arm with its known value in that arm. Future improvements could use range based analysis or other techniques to narrow down the possible values of any variables within the selector, and use that information to optimize the select arms.","title":"Specializing select arms"},{"location":"optimizations/#consecutive-selects-with-identical-selectors","text":"Two consecutive two-way selects which use the same selector can be compacted into a single select statement. The selector only has two states so only two of the three different cases may be selected. The third is dead. Visually, the transformation looks like: The specific cases which remain in the new select instruction depends on whether the upper select feeds the true or false input of the lower select.","title":"Consecutive selects with identical selectors"},{"location":"optimizations/#sparsifying-selects-with-range-analysis","text":"If range analysis determines that the selector of a select has fewer possible values than the number of cases, we can do a form of dead code elimination to remove the impossible cases. Currently, we do this in the following way: // Suppose bar has interval set {[1, 4], [6, 7], [10, 13]} foo = sel(bar, cases=[a1, ..., a16]) // Sparsification will lead to the following code: foo = sel((bar >= 1) && (bar <= 4), cases=[ sel((bar >= 6) && (bar <= 7), cases=[ sel(bar - 10, cases=[a11, ..., a14], default=0), sel(bar - 6, cases=[a7, a8], default=0) ]), sel(bar - 1, cases=[a2, ..., a5], default=0) ]) This adds a little bit of code for the comparisons and subtractions but is generally worth it since eliminating a case branch can be a big win.","title":"Sparsifying selects with range analysis"},{"location":"optimizations/#binary-decision-diagram-based-optimizations","text":"A binary decision diagram (BDD) is a data structure that can represent arbitrary boolean expressions. Properties of the BDD enable easy determination of relationships between different expressions (equality, implication, etc.) which makes them useful for optimization and analysis","title":"Binary Decision Diagram based optimizations"},{"location":"optimizations/#bdd-common-subexpression-elimination","text":"Determining whether two expression are equivalent is trivial using BDDs. This can be used to identify operations in the graph which produce identical results. BDD CSE is an optimization pass which commons these equivalent operations.","title":"BDD common subexpression elimination"},{"location":"optimizations/#onehot-msb-elimination","text":"A OneHot instruction returns a bitmask with exactly one bit equal to 1. If the input is not all-0 bits, this is the first 1 bit encountered in the input going from least to most significant bit or vice-versa depending on the priority specified. If the input is all-0 bits, the most significant bit of the OneHot output is set to 1. The semantics of OneHot are described in detail here . If the MSB of a OneHot does not affect the functionality of a program, we replace the MSB with a 0-bit, e.g. OneHot(A) such that the MSB has no effect \u21d2 Concat(0, OneHot(A)[all bits except MSB])) This can open up opportunities for further optimization. To determine if a OneHot \u2019s MSB has any effect on a function, we iterate over the OneHot \u2019s post-dominators. We use the BDD to test if setting the OneHot \u2019s MSB to 0 or to 1 (other bits are 0 in both cases) implies the same value for the post-dominator node. If so, we know the value of the MSB cannot possibly affect the function output, so the MSB can safely be replaced with a 0 bit. Note: This approach assumes that IR nodes do not have any side effects. When IR nodes with side effects are introduced (i.e. channels) the analysis for this optimization will have to be adjusted slightly to account for this.","title":"OneHot MSB elimination"},{"location":"scheduling/","text":"XLS Pipeline scheduling XLS Pipeline scheduling Scheduling process Step 1: determine the effective clock period Step 2: schedule to minimize pipeline registers Options for common scheduling objectives Minimizing pipeline registers via min-cut Rematerialization Pipeline scheduling divides the IR nodes of an XLS function or proc into a sequence of stages constituting a feed-forward pipeline. Sequential stages are separated by registers enabling pipeline parallelism. The schedule must satisfy dependency constraints between XLS nodes as well as timing constraints imposed by the target clock frequency. Pipeline scheduling has multiple competing optimization objectives: minimize number of stages (minimize pipeline latency), minimize maximum delay of any stage (maximize clock frequency), and minimize the number of pipeline registers. Scheduling process Pipeline scheduling occurs in two phases: Determine the effective clock period. This clock period defines the maximum delay, based on XLS's internal delay model , through any pipeline stage and limits how many IR operations might be placed in each stage. Given the constraints of the effective clock period and, optionally, a user-defined number of pipeline stages, find the schedule which minimizes the number of pipeline registers. Pipeline registers are required for any IR operation whose value which is used in a later stage. The schedule process is controlled via several options defined here . These options are typically passed in as flags to the codegen_main binary but maybe set programmatically. Each is optional though at least one of clock period or pipeline stages must be specified. Different combinations of options result in different strategies as described below . Clock period : The target clock period. Pipeline stages : The number of stages in the pipeline. Clock margin percent : The percentage to reduce the target clock period before scheduling. May only be specified with clock period . This option is equivalent to specifying a reduced value for clock period . Clock period relaxation percent: : This is the percentage that the computed minimum clock period, as determined by the number of pipeline stages, is increased (relaxed) prior to scheduling. May not be specified with clock period . Step 1: determine the effective clock period The effective clock period determines the maximum delay through any pipeline stage for the purpose of scheduling. The value is determined in one of two ways depending upon whether the clock period option is specified. clock period specified The effective clock period is set the clock period value. If clock margin percent is also specified, then the effective clock period is also reduced by the given percentage. Example: if clock period is 800ps and clock margin percent is 20% then the effective clock period is 640ps. clock period not specified In this case, pipeline stages must be specified. The effective clock period is computed as the minimum clock period in which a schedule may be found that meets timing with the specified number of pipeline stages. This is done via a binary search through clock period values. If clock period relaxation percent is specified then the computed effective clock period is increased by the given percentage. The motivation is that this relaxation may result in fewer pipeline registers because of increased scheduling flexibility. Example: if the minimum clock period found by XLS was 1000ps and clock period relaxation percent is 10% the effective clock period is 1100ps. Step 2: schedule to minimize pipeline registers Once an effective clock period is determined, XLS computes a schedule which minimizes the number of registers (see below for details) while satisfying the critical path delay constraints imposed by the effective clock period. The number of stages in the pipeline may be specified by the user via the pipeline stages option. If the number of pipeline stages specified is too small an error such that no feasible schedule can be found then an error is returned. If pipeline stages is not given then the minimum number of stages which meets the delay constraint imposed by the effective clock period is used. Options for common scheduling objectives Different scheduling options result in different optimization strategies for the scheduler. Below are several common scheduling objectives and options which should be set to enable them. Minimize the number of pipeline registers for a given clock period and given number of pipeline stages. Specify both clock period and pipeline stages . The scheduler will attempt to minimize the number of pipeline registers given those constraints. The option clock margin percent can be swept to search the local design space (or equivalently, sweep clock period ) Minimize the clock period for a given number of pipeline stages Specify only pipeline stages . XLS will find a schedule with minimum clock period with a secondary objective of minimizing the number of pipeline registers. Sweeping clock period relaxation percent explores relaxing the timing constraint which may result in fewer pipeline registers. Minimize the number of pipeline stages for a given clock period Specify only clock period . XLS will find a schedule of the minimum number of stages with a secondary objective of minimizing the number of pipeline registers. The option clock margin percent can be swept to search the local design space (or equivalently, sweep clock period ) Minimize the number of pipeline registers for a given clock period Specify only clock period and sweep pipeline stages . Pick the schedule which produces the minimum number of pipeline registers. Sweep the entire scheduling space The various options directly or indirectly control the two degrees of freedom within the scheduler: pipeline stages and clock period. Sweeping these two degrees of freedom is most easily done by sweeping pipeline stages and clock period relaxation percent . The advantage of sweeping clock period relaxation percent instead of clock period directly is that the percent relaxation can be a fixed range (e.g., 0 to 50%) for all designs and each value will produce a feasible schedule. If clock period is swept some combinations of pipeline stages* and clock period** values will result in an error returned because the design point is infeasible. Minimizing pipeline registers via min-cut Scheduling to minimize pipeline registers can be formulated as a graph min-cut problem where the graph cut divides the nodes of the graph into separate pipeline stages and the cost of the cut is the number of bits in the pipeline register between the stages. This formulation of the pipeline scheduling problem is attractive because the graph min-cut problem can be solved in polynomial time. In general, the XLS IR graph cannot be used directly by the min-cut algorithm because of additional constraints imposed by pipeline scheduling and features of the IR graph. As a motivating example, consider the following graph of an XLS function to be scheduled into a two-stage pipeline: Node x is a parameter to the function, and node F is the return value. The width of the edges correlates with the (labeled) bit width of the respective operation. The bit width of the edges are edge weights in the min-cut algorithm. The drawing above shows the initial and final pipeline registers which flops the input output on the interface boundary. In this example, a two-stage pipeline is desired so one additional pipeline register is required. The scheduling problem is to partition the nodes A through F into the two pipeline stages while minimizing register count, or alternatively formulated, identify a cut through the graph of minimum cost. Below is one possible cut resulting in a pipeline schedule where nodes A and B are in stage one and nodes C through F are in stage two. In the pipeline generated from the example cut above pipeline registers are required for nodes A and B of bit widths 2 and 32 respectively for a total of 34 flops. However this value is inconsistent with the cost of the cut in the IR graph which is equal to the sum of weights of the cut edges: 2 + 32 + 32 = 66. To reconcile the cost function, transformations are applied to the graph about nodes with fan-out. Specifically, an artificial node N' is added for each node N with fan-out and an edge is added from each immediate successor of N to N' . The weight of each edge fanning out from N and fanning into N' is set to the original weight of the edges of N divided by the fan-out factor. In the example, the edge weight is set to 32 / 2 = 16. Below is the transformed graph. As shown, the cost of the cut (sum of edge weights) now equals the number of flops in the pipeline register (34). For scheduling the example graph, assume the target clock period be three time units and all nodes have unit delay. In this case, not all cuts will produce a schedule which satisfies the timing constraint. Specifically, if B is scheduled in the second stage, or F is scheduled in the first stage the pipeline cannot meet the timing constraint. To ensure the min-cut results in a schedule which satisfies timing an artificial source and sink node is added to the graph. Edges with infinite weight are added from the source to nodes which must be scheduled in the first stage (or earlier in the case of parameter nodes) to satisfy timing constraints. Similar edges are added from nodes which must be scheduled in the second stage to the sink node as shown below: One additional transformation (not shown) is required to ensure a correct pipeline. Generally, a partitioning created by a min-cut allows edges going in both directions between the partitions. However, pipeline stages do not allow circular dependencies. To enforce directionality to the edges of the cut, an edge of infinite weight is added parallel to and in the opposite direction of every edge in the graph. After transforming the graph, a min-cut is found by applying a max flow algorithm (Ford-Fulkerson) from the artificial source node to the artificial sink node. In the running example, the min cut is edges C -> F and E -> F of cost 12. All nodes except F are placed in the first stage of the pipeline. Generally, a pipeline can have more than two stages so a single cut is insufficient to determine a schedule. In this case a sequence of cuts is performed, one for each boundary between pipeline stages. Each min-cut partitions the nodes into two parts: the set of nodes scheduled before the respective stage boundary, and the set of node scheduled after. This imposes additional constraints on later min-cut computations. These constraints are imposed by extending infinite weight edges between these nodes and the source or sink node in the graph. The order in which the sequence of cuts is performed (e.g., cut the boundary between stage 0 and stage 1, then between 1 and 2, then between 2 and 3, and so on) can affect the total number of pipeline flops so, in general, multiple orders are attempted and the result with the fewest pipeline flops is kept. Rematerialization TODO(meheff): Finish.","title":"Overview"},{"location":"scheduling/#xls-pipeline-scheduling","text":"XLS Pipeline scheduling Scheduling process Step 1: determine the effective clock period Step 2: schedule to minimize pipeline registers Options for common scheduling objectives Minimizing pipeline registers via min-cut Rematerialization Pipeline scheduling divides the IR nodes of an XLS function or proc into a sequence of stages constituting a feed-forward pipeline. Sequential stages are separated by registers enabling pipeline parallelism. The schedule must satisfy dependency constraints between XLS nodes as well as timing constraints imposed by the target clock frequency. Pipeline scheduling has multiple competing optimization objectives: minimize number of stages (minimize pipeline latency), minimize maximum delay of any stage (maximize clock frequency), and minimize the number of pipeline registers.","title":"XLS Pipeline scheduling"},{"location":"scheduling/#scheduling-process","text":"Pipeline scheduling occurs in two phases: Determine the effective clock period. This clock period defines the maximum delay, based on XLS's internal delay model , through any pipeline stage and limits how many IR operations might be placed in each stage. Given the constraints of the effective clock period and, optionally, a user-defined number of pipeline stages, find the schedule which minimizes the number of pipeline registers. Pipeline registers are required for any IR operation whose value which is used in a later stage. The schedule process is controlled via several options defined here . These options are typically passed in as flags to the codegen_main binary but maybe set programmatically. Each is optional though at least one of clock period or pipeline stages must be specified. Different combinations of options result in different strategies as described below . Clock period : The target clock period. Pipeline stages : The number of stages in the pipeline. Clock margin percent : The percentage to reduce the target clock period before scheduling. May only be specified with clock period . This option is equivalent to specifying a reduced value for clock period . Clock period relaxation percent: : This is the percentage that the computed minimum clock period, as determined by the number of pipeline stages, is increased (relaxed) prior to scheduling. May not be specified with clock period .","title":"Scheduling process"},{"location":"scheduling/#step-1-determine-the-effective-clock-period","text":"The effective clock period determines the maximum delay through any pipeline stage for the purpose of scheduling. The value is determined in one of two ways depending upon whether the clock period option is specified. clock period specified The effective clock period is set the clock period value. If clock margin percent is also specified, then the effective clock period is also reduced by the given percentage. Example: if clock period is 800ps and clock margin percent is 20% then the effective clock period is 640ps. clock period not specified In this case, pipeline stages must be specified. The effective clock period is computed as the minimum clock period in which a schedule may be found that meets timing with the specified number of pipeline stages. This is done via a binary search through clock period values. If clock period relaxation percent is specified then the computed effective clock period is increased by the given percentage. The motivation is that this relaxation may result in fewer pipeline registers because of increased scheduling flexibility. Example: if the minimum clock period found by XLS was 1000ps and clock period relaxation percent is 10% the effective clock period is 1100ps.","title":"Step 1: determine the effective clock period"},{"location":"scheduling/#step-2-schedule-to-minimize-pipeline-registers","text":"Once an effective clock period is determined, XLS computes a schedule which minimizes the number of registers (see below for details) while satisfying the critical path delay constraints imposed by the effective clock period. The number of stages in the pipeline may be specified by the user via the pipeline stages option. If the number of pipeline stages specified is too small an error such that no feasible schedule can be found then an error is returned. If pipeline stages is not given then the minimum number of stages which meets the delay constraint imposed by the effective clock period is used.","title":"Step 2: schedule to minimize pipeline registers"},{"location":"scheduling/#common-options","text":"Different scheduling options result in different optimization strategies for the scheduler. Below are several common scheduling objectives and options which should be set to enable them. Minimize the number of pipeline registers for a given clock period and given number of pipeline stages. Specify both clock period and pipeline stages . The scheduler will attempt to minimize the number of pipeline registers given those constraints. The option clock margin percent can be swept to search the local design space (or equivalently, sweep clock period ) Minimize the clock period for a given number of pipeline stages Specify only pipeline stages . XLS will find a schedule with minimum clock period with a secondary objective of minimizing the number of pipeline registers. Sweeping clock period relaxation percent explores relaxing the timing constraint which may result in fewer pipeline registers. Minimize the number of pipeline stages for a given clock period Specify only clock period . XLS will find a schedule of the minimum number of stages with a secondary objective of minimizing the number of pipeline registers. The option clock margin percent can be swept to search the local design space (or equivalently, sweep clock period ) Minimize the number of pipeline registers for a given clock period Specify only clock period and sweep pipeline stages . Pick the schedule which produces the minimum number of pipeline registers. Sweep the entire scheduling space The various options directly or indirectly control the two degrees of freedom within the scheduler: pipeline stages and clock period. Sweeping these two degrees of freedom is most easily done by sweeping pipeline stages and clock period relaxation percent . The advantage of sweeping clock period relaxation percent instead of clock period directly is that the percent relaxation can be a fixed range (e.g., 0 to 50%) for all designs and each value will produce a feasible schedule. If clock period is swept some combinations of pipeline stages* and clock period** values will result in an error returned because the design point is infeasible.","title":"Options for common scheduling objectives"},{"location":"scheduling/#min-cut","text":"Scheduling to minimize pipeline registers can be formulated as a graph min-cut problem where the graph cut divides the nodes of the graph into separate pipeline stages and the cost of the cut is the number of bits in the pipeline register between the stages. This formulation of the pipeline scheduling problem is attractive because the graph min-cut problem can be solved in polynomial time. In general, the XLS IR graph cannot be used directly by the min-cut algorithm because of additional constraints imposed by pipeline scheduling and features of the IR graph. As a motivating example, consider the following graph of an XLS function to be scheduled into a two-stage pipeline: Node x is a parameter to the function, and node F is the return value. The width of the edges correlates with the (labeled) bit width of the respective operation. The bit width of the edges are edge weights in the min-cut algorithm. The drawing above shows the initial and final pipeline registers which flops the input output on the interface boundary. In this example, a two-stage pipeline is desired so one additional pipeline register is required. The scheduling problem is to partition the nodes A through F into the two pipeline stages while minimizing register count, or alternatively formulated, identify a cut through the graph of minimum cost. Below is one possible cut resulting in a pipeline schedule where nodes A and B are in stage one and nodes C through F are in stage two. In the pipeline generated from the example cut above pipeline registers are required for nodes A and B of bit widths 2 and 32 respectively for a total of 34 flops. However this value is inconsistent with the cost of the cut in the IR graph which is equal to the sum of weights of the cut edges: 2 + 32 + 32 = 66. To reconcile the cost function, transformations are applied to the graph about nodes with fan-out. Specifically, an artificial node N' is added for each node N with fan-out and an edge is added from each immediate successor of N to N' . The weight of each edge fanning out from N and fanning into N' is set to the original weight of the edges of N divided by the fan-out factor. In the example, the edge weight is set to 32 / 2 = 16. Below is the transformed graph. As shown, the cost of the cut (sum of edge weights) now equals the number of flops in the pipeline register (34). For scheduling the example graph, assume the target clock period be three time units and all nodes have unit delay. In this case, not all cuts will produce a schedule which satisfies the timing constraint. Specifically, if B is scheduled in the second stage, or F is scheduled in the first stage the pipeline cannot meet the timing constraint. To ensure the min-cut results in a schedule which satisfies timing an artificial source and sink node is added to the graph. Edges with infinite weight are added from the source to nodes which must be scheduled in the first stage (or earlier in the case of parameter nodes) to satisfy timing constraints. Similar edges are added from nodes which must be scheduled in the second stage to the sink node as shown below: One additional transformation (not shown) is required to ensure a correct pipeline. Generally, a partitioning created by a min-cut allows edges going in both directions between the partitions. However, pipeline stages do not allow circular dependencies. To enforce directionality to the edges of the cut, an edge of infinite weight is added parallel to and in the opposite direction of every edge in the graph. After transforming the graph, a min-cut is found by applying a max flow algorithm (Ford-Fulkerson) from the artificial source node to the artificial sink node. In the running example, the min cut is edges C -> F and E -> F of cost 12. All nodes except F are placed in the first stage of the pipeline. Generally, a pipeline can have more than two stages so a single cut is insufficient to determine a schedule. In this case a sequence of cuts is performed, one for each boundary between pipeline stages. Each min-cut partitions the nodes into two parts: the set of nodes scheduled before the respective stage boundary, and the set of node scheduled after. This imposes additional constraints on later min-cut computations. These constraints are imposed by extending infinite weight edges between these nodes and the source or sink node in the graph. The order in which the sequence of cuts is performed (e.g., cut the boundary between stage 0 and stage 1, then between 1 and 2, then between 2 and 3, and so on) can affect the total number of pipeline flops so, in general, multiple orders are attempted and the result with the fewest pipeline flops is kept.","title":"Minimizing pipeline registers via min-cut"},{"location":"scheduling/#rematerialization","text":"TODO(meheff): Finish.","title":"Rematerialization"},{"location":"solvers/","text":"XLS Solvers Programs that are represented as optimized XLS IR are converted into circuits based on boolean logic, and so it is also possible to feed those as logical operations to a theorem prover. We have implemented that conversion with the Z3 theorem prover using its \"bit vector\" type support. As a result, you can conceptually ask Z3 to prove any predicate that can be expressed as XLS, over all possible parameter inputs. See the tools documentation for usage information on related command line tools. Applications This facility is expected to be useful to augment random testing. While profiling the values in an XLS IR function that is given random stimulus, we may observe bits that result from nodes that appear to be constant (but are not created via a \"literal\" or a \"concat\" of a literal). Example: Say the value resulting from and.1234 in the graph appears to be constant zero with all the stimulus provided via a fuzzer thus far -- the solver provides a facility whereby we can ask \"is there a counterexample to and.1234 always being zero?\" and the solver will either say \"no, it is always zero\", or it will yield a counterexample, or will not terminate within the allocated deadline. Assuming we can prove useful properties in a reasonable amount of time, we can use this proof capability to help find interesting example inputs that provide unique stimulus. Correctness WRT reference: 32-bit Floating-Point Adder The full input space for a 32-bit adder is a whopping 64 bits - far more than is possible to exhaustively test for correctness. Proving correctness via Z3, however, is relatively straightforward: at a high level, one simply compares the output from the DSLX (translated into Z3) to the same operation performed solely in Z3. In detail, the steps are: Translate the DSLX implementation into Z3 via Z3Translator::CreateAndTranslate() . Create a Z3 implementation of the same addition. This is nearly trivial, as Z3 helpfully has built-in support for floating-point values and theories. Take the result nodes from each \"branch\" above and create a new node subtracting the two. This is the absolute error. Note: Usually, one is interested in relative error when working with FP values, but here, our target is absolute equivalence, so absolute error sufficies (and is simpler). Create a Z3 node comparing that error to the maximum bound (here 0.0f). Feed that error node into a Z3 solver, asking it to prove that the error could be greater than that bound. If the solver can not satisfy that criterion, then that means the error is never greater than that bound, i.e., that the implementations are equivalent (with our 0.0f bound). IR Transform validity It's usually not possible (or is merely extremely difficult) to write tests to prove that an optimization/transform is safe across all input IR. By comparing the optimized vs. unoptimized IR in a similar manner as the correctness proof above, we can symbolically prove safety. The only difference between this and the correctness proof is that both the optimized and unoptimized IR need to be fed into the same Z3Translator (the second via Z3Translator::AddFunction() ) and the result nodes each are used in the error comparison. IR to netlist Logical Equivalence Checking (LEC) After a user design has been lowered to IR, it is optimized (see the previous section), then Verilog is generated for that optimized IR. That Verilog is then compiled by an external tool, which, if successful, will output a \"netlist\" - a set of standard cells (think AND, OR, NOT, flops, etc.) and wires connecting them that realizes the design. Between the IR level and that netlist, many, many transformations are applied to the design. Before processing the netlist further - and certainly before sending the final design to fabrication - it's a very good idea to ensure that the netlist describes the correct logic! Demonstrating initial design correctness is up to the user, via unit tests or integration tests at the DSLX level. At all stages below that, though, ensuring logical equivalence between forms is XLS' responsibility. To prove equivalence between the IR and netlist forms of a design, XLS uses formal verification via solvers - currently only Z3, above. Performing IR-to-netlist LEC is very similar to the checking above - the source IR is one half of the comparison. Here, the second half is the netlist translated into IR, which only requires a small amount of extra work. Consider the snippet below: FOO p1_and_1 ( .A(p0_i0), .B(p0_i1), .Z(p1_and_1_comb) ); BAR p1_and_2 ( .A(p0_i2), .B(p0_i3), .Z(p1_and_2_comb) ); These lines describe, in order: One cell, called FOO , that takes two inputs, .A and .B, provided by the wires p0_i0 and p0_i1 , respectively, and one output, .Z, which will be assigned to the wire `p1_and_1_comb. One cell, called BAR , that takes two inputs, .A and .B, provided by the wires p0_i2 and p0_i2 , respectively, and one output, .Z, which will be assigned to the wire `p1_and_2_comb. Note that the values computed by the cells wasn't mentioned - that's because FOO and BAR are defined in the \"cell library\", the list of standard cells used to generate the netlist. Thus, to be able to model these gates in a solver, we need to take that cell library as input to the LEC tool. The netlist describes how cells are laid out, and the cell library indicates what cells actually do . With both of these in hand, preparing the netlist half of a LEC is a [relatively] straightforward matter of parsing a netlist and cell library and converting those together into a description of logic. See z3_netlist_translator.cc for full details. Utilities tools/lec_main.cc : Driver function for performing IR-to-netlist LEC. solvers/python/z3_lec.cc : Wrapper to perform IR-to-netlist LEC from Python. Current Limitations Time-to-result Under the hood, Z3 (and many other tools in this space) is an SMT solver . At a high level, think of an SMT solver as a SAT solver that has special handling for certain classes of data (bit vectors, floating-point numbers). Many sufficiently complicated problems will reduce to raw SAT solving (especially those involving netlists, which have to implement complex logic at the gate level. Consider what that means for a multiply, for example!). Since SAT scales exponentially with the size of its inputs, execution time can quickly grow past a point of utility for complex operations, notably multiplication. Fortunately, for most designs (without such complex ops), proving equivalence of a single pipeline stage can complete in a small amount of time (O(minutes)). Predicate coverage Hypothetically, any XLS function that computes a predicate (bool) can be fed to Z3 for satisfiability testing. Currently a more limited set of predicates are exposed that can be easily expressed on the command line; however, it should be possible to provide: an XLS IR file a set of nodes in the entry function a DSLX function that computes a predicate on those nodes Which would allow the user to compute arbitrary properties of nodes in the function with the concise DSL syntax. Subroutines Z3 doesn't intrinsically have support for subroutines, or as they're called in Z3, \"macros\", instead requiring that all function calls be inlined . There is an extension that adds support for recursive function decls and defs, but in our experience, it doesn't behave the way we'd expect. Consider the following example: package p fn mapper(value: bits[32]) -> bits[32] { ret value } fn main() -> bits[1] { literal_0: bits[32] = literal(value=0) literal_1: bits[1] = literal(value=1) elem_0: bits[32] = invoke(literal_0, to_apply=mapper) eq_12: bits[1] = eq(literal_0, elem_0) ret and_13: bits[1] = and(eq_12, literal_1) } Here, it's trivial for a human reader to see that the results are the same; the output should be equal to 1. Z3, however, reports that this is not necessarily the case, suggesting that literal_0 and elem_0 would not be equal in the case where the input to mapper was 1...which is clearly never the case here. To address this, we require that all subroutines (including those used in maps and counted fors) be inlined before consumption by Z3.","title":"Formal"},{"location":"solvers/#xls-solvers","text":"Programs that are represented as optimized XLS IR are converted into circuits based on boolean logic, and so it is also possible to feed those as logical operations to a theorem prover. We have implemented that conversion with the Z3 theorem prover using its \"bit vector\" type support. As a result, you can conceptually ask Z3 to prove any predicate that can be expressed as XLS, over all possible parameter inputs. See the tools documentation for usage information on related command line tools.","title":"XLS Solvers"},{"location":"solvers/#applications","text":"This facility is expected to be useful to augment random testing. While profiling the values in an XLS IR function that is given random stimulus, we may observe bits that result from nodes that appear to be constant (but are not created via a \"literal\" or a \"concat\" of a literal). Example: Say the value resulting from and.1234 in the graph appears to be constant zero with all the stimulus provided via a fuzzer thus far -- the solver provides a facility whereby we can ask \"is there a counterexample to and.1234 always being zero?\" and the solver will either say \"no, it is always zero\", or it will yield a counterexample, or will not terminate within the allocated deadline. Assuming we can prove useful properties in a reasonable amount of time, we can use this proof capability to help find interesting example inputs that provide unique stimulus.","title":"Applications"},{"location":"solvers/#correctness-wrt-reference-32-bit-floating-point-adder","text":"The full input space for a 32-bit adder is a whopping 64 bits - far more than is possible to exhaustively test for correctness. Proving correctness via Z3, however, is relatively straightforward: at a high level, one simply compares the output from the DSLX (translated into Z3) to the same operation performed solely in Z3. In detail, the steps are: Translate the DSLX implementation into Z3 via Z3Translator::CreateAndTranslate() . Create a Z3 implementation of the same addition. This is nearly trivial, as Z3 helpfully has built-in support for floating-point values and theories. Take the result nodes from each \"branch\" above and create a new node subtracting the two. This is the absolute error. Note: Usually, one is interested in relative error when working with FP values, but here, our target is absolute equivalence, so absolute error sufficies (and is simpler). Create a Z3 node comparing that error to the maximum bound (here 0.0f). Feed that error node into a Z3 solver, asking it to prove that the error could be greater than that bound. If the solver can not satisfy that criterion, then that means the error is never greater than that bound, i.e., that the implementations are equivalent (with our 0.0f bound).","title":"Correctness WRT reference: 32-bit Floating-Point Adder"},{"location":"solvers/#ir-transform-validity","text":"It's usually not possible (or is merely extremely difficult) to write tests to prove that an optimization/transform is safe across all input IR. By comparing the optimized vs. unoptimized IR in a similar manner as the correctness proof above, we can symbolically prove safety. The only difference between this and the correctness proof is that both the optimized and unoptimized IR need to be fed into the same Z3Translator (the second via Z3Translator::AddFunction() ) and the result nodes each are used in the error comparison.","title":"IR Transform validity"},{"location":"solvers/#ir-to-netlist-logical-equivalence-checking-lec","text":"After a user design has been lowered to IR, it is optimized (see the previous section), then Verilog is generated for that optimized IR. That Verilog is then compiled by an external tool, which, if successful, will output a \"netlist\" - a set of standard cells (think AND, OR, NOT, flops, etc.) and wires connecting them that realizes the design. Between the IR level and that netlist, many, many transformations are applied to the design. Before processing the netlist further - and certainly before sending the final design to fabrication - it's a very good idea to ensure that the netlist describes the correct logic! Demonstrating initial design correctness is up to the user, via unit tests or integration tests at the DSLX level. At all stages below that, though, ensuring logical equivalence between forms is XLS' responsibility. To prove equivalence between the IR and netlist forms of a design, XLS uses formal verification via solvers - currently only Z3, above. Performing IR-to-netlist LEC is very similar to the checking above - the source IR is one half of the comparison. Here, the second half is the netlist translated into IR, which only requires a small amount of extra work. Consider the snippet below: FOO p1_and_1 ( .A(p0_i0), .B(p0_i1), .Z(p1_and_1_comb) ); BAR p1_and_2 ( .A(p0_i2), .B(p0_i3), .Z(p1_and_2_comb) ); These lines describe, in order: One cell, called FOO , that takes two inputs, .A and .B, provided by the wires p0_i0 and p0_i1 , respectively, and one output, .Z, which will be assigned to the wire `p1_and_1_comb. One cell, called BAR , that takes two inputs, .A and .B, provided by the wires p0_i2 and p0_i2 , respectively, and one output, .Z, which will be assigned to the wire `p1_and_2_comb. Note that the values computed by the cells wasn't mentioned - that's because FOO and BAR are defined in the \"cell library\", the list of standard cells used to generate the netlist. Thus, to be able to model these gates in a solver, we need to take that cell library as input to the LEC tool. The netlist describes how cells are laid out, and the cell library indicates what cells actually do . With both of these in hand, preparing the netlist half of a LEC is a [relatively] straightforward matter of parsing a netlist and cell library and converting those together into a description of logic. See z3_netlist_translator.cc for full details.","title":"IR to netlist Logical Equivalence Checking (LEC)"},{"location":"solvers/#utilities","text":"tools/lec_main.cc : Driver function for performing IR-to-netlist LEC. solvers/python/z3_lec.cc : Wrapper to perform IR-to-netlist LEC from Python.","title":"Utilities"},{"location":"solvers/#current-limitations","text":"","title":"Current Limitations"},{"location":"solvers/#time-to-result","text":"Under the hood, Z3 (and many other tools in this space) is an SMT solver . At a high level, think of an SMT solver as a SAT solver that has special handling for certain classes of data (bit vectors, floating-point numbers). Many sufficiently complicated problems will reduce to raw SAT solving (especially those involving netlists, which have to implement complex logic at the gate level. Consider what that means for a multiply, for example!). Since SAT scales exponentially with the size of its inputs, execution time can quickly grow past a point of utility for complex operations, notably multiplication. Fortunately, for most designs (without such complex ops), proving equivalence of a single pipeline stage can complete in a small amount of time (O(minutes)).","title":"Time-to-result"},{"location":"solvers/#predicate-coverage","text":"Hypothetically, any XLS function that computes a predicate (bool) can be fed to Z3 for satisfiability testing. Currently a more limited set of predicates are exposed that can be easily expressed on the command line; however, it should be possible to provide: an XLS IR file a set of nodes in the entry function a DSLX function that computes a predicate on those nodes Which would allow the user to compute arbitrary properties of nodes in the function with the concise DSL syntax.","title":"Predicate coverage"},{"location":"solvers/#subroutines","text":"Z3 doesn't intrinsically have support for subroutines, or as they're called in Z3, \"macros\", instead requiring that all function calls be inlined . There is an extension that adds support for recursive function decls and defs, but in our experience, it doesn't behave the way we'd expect. Consider the following example: package p fn mapper(value: bits[32]) -> bits[32] { ret value } fn main() -> bits[1] { literal_0: bits[32] = literal(value=0) literal_1: bits[1] = literal(value=1) elem_0: bits[32] = invoke(literal_0, to_apply=mapper) eq_12: bits[1] = eq(literal_0, elem_0) ret and_13: bits[1] = and(eq_12, literal_1) } Here, it's trivial for a human reader to see that the results are the same; the output should be equal to 1. Z3, however, reports that this is not necessarily the case, suggesting that literal_0 and elem_0 would not be equal in the case where the input to mapper was 1...which is clearly never the case here. To address this, we require that all subroutines (including those used in maps and counted fors) be inlined before consumption by Z3.","title":"Subroutines"},{"location":"tools/","text":"XLS Tools An index of XLS developer tools. bdd_stats Constructs a binary decision diagram (BDD) using a given XLS function and prints various statistics about the BDD. BDD construction can be very slow in pathological cases and this utility is useful for identifying the underlying causes. Accepts arbitrary IR as input or a benchmark specified by name. benchmark_main Prints numerous metrics and other information about an XLS IR file including: total delay, critical path, codegen information, optimization time, etc. This tool may be run against arbitrary IR not just the fixed set of XLS benchmarks. The output of this tool is scraped by run_benchmarks to construct a table comparing metrics against a mint CL across the benchmark suite. booleanify_main Rewrites an XLS IR function in terms of its ops' fundamental AND/OR/NOT constituents, i.e., makes all operations boolean, thus it's \"booleanifying\" the function. codegen_main Lowers an XLS IR file into Verilog. Options include emitting a feedforward pipeline or a purely combinational block. Emits both a Verilog file and a module signature which includes metadata about the block. The tool does not run any XLS passes so unoptimized IR may fail if the IR contains constructs not expected by the backend. I/O Configuration Options --flop_inputs=true --flop_inputs_kind=zerolatency For each ready-valid-data channel on the input, an additional register is added so that the XLS pipeline need not stall a specific input channel while waiting for all to become ready. A single set of registers permits storage of one channel write() . (These are input-side channels, so the external write() can proceed even if the pipeline is not ready to read() .) Being a zero-latency buffer, the additional input logic does not increase the pipeline latency, but there remains a combinational path from the input ports to the pipeline register after the first stage. --flop_outputs=true --flop_outputs_kind=skid A 1-cycle skid buffer is added to the output to break up output timing paths. (The output stage does contribute one additional cycle of latency.) --add_idle_output=true An additional output port is generated named idle . Idle is the NOR of: Pipeline registers storing the \"valid\" bit for that particular stage. All valid registers stored for the input/output buffers. All valid signals for the input channels. --reset_data_path=true With this option, all pipeline registers (both data and valid registers) are reset to their initial state upon reset. This results in the reset signal fanning out to all registers in the block. --flop_single_value_channels=false With this option, the port for any single-value channels are not flopped, but taken as direct wires. delay_info_main Dumps delay information about an XLS function including per-node delay information and critical-path. eval_ir_main Evaluates an XLS IR file with user-specified or random inputs. Includes features for evaluating the IR before and after optimizations which makes this tool very useful for identifying optimization bugs. This tool accepts two [mutually exclusive] optional args, --input_validator_expr and --input_validator_path , which allow the user to specify an expression to \"filter\" potential input values to discard invalid ones. For both, the filter must be a function, named validator , and must take params of the same layout as the function under test. This function should return true if the inputs are valid for the function and false otherwise. --input_validator_expr lists the function as an inline command-line argument, whereas --input_validator_path holds the path to a .x file containing the validation function. ir_minimizer_main Tool for reducing IR to a minimal test case based on an external test. ir_stats_main Prints summary information/stats on an IR [Package] file. An example: $ bazel-bin/xls/tools/ir_stats_main bazel-genfiles/xls/modules/fp32_add_2.ir Package \"fp32_add_2\" Function: \"__float32__is_inf\" Signature: ((bits[1], bits[8], bits[23])) -> bits[1] Nodes: 8 Function: \"__float32__is_nan\" Signature: ((bits[1], bits[8], bits[23])) -> bits[1] Nodes: 8 Function: \"__fp32_add_2__fp32_add_2\" Signature: ((bits[1], bits[8], bits[23]), (bits[1], bits[8], bits[23])) -> (bits[1], bits[8], bits[23]) Nodes: 252 check_ir_equivalence Verifies that two IR files (for example, optimized and unoptimized IR from the same source) are logically equivalent. opt_main Runs XLS IR through the optimization pipeline. proto_to_dslx_main Takes in a proto schema and a textproto instance thereof and outputs a DSLX module containing a DSLX type and constant matching both inputs, respectively. Not all protocol buffer types map to DSLX types, so there are some restrictions or other behaviors requiring explanation: Only scalar and repeated fields are supported (i.e., no maps or oneofs, etc.). Only recursively-integral messages are supported, that is to say, a message may contain submessages, as long as all non-Message fields are integral. Since DSLX doesn't support variable arrays and Protocol Buffers don't support fixed-length repeated fields. To unify this, all instances of repeated-field-containing Messages must have the same size of their repeated members (declared as arrays in DSLX). This size will be calculated as the maximum size of any instance of that repeated field across all instances in the input textproto. For example, if a message Foo has a repeated field bar , and this message is present multiple times in the input textproto, say as: foo: { bar: 1 } foo: { bar: 1 bar: 2 } foo: { bar: 1 bar: 2 bar: 3 } the DSLX version of Foo will declare bar has a 3-element array. An accessory field, bar_count , will also be created, which will contain the number of valid entries in an actual instance of Foo::bar . The \"Fields\" example in ./xls/tools/testdata/proto_to_dslx_main.* demonstrates this behavior. repl Allows you to interactively run various parts of the compiler, including parsing/type checking ( :reload ), lowering/optimization ( :ir ), Verilog codegen ( :verilog [identifier] ), and LLVM codegen ( :llvm , not yet implemented). You can also inspect the IR types of identifiers with :type , and even imported identifiers can be accessed with :type foo::bar . simulate_module_main Runs an Verilog block emitted by XLS through a Verilog simulator. Requires both the Verilog text and the module signature which includes metadata about the block. smtlib_emitter_main Simple driver for Z3IrTranslator - converts a given IR function into its Z3 representation and outputs that translation as SMTLIB2. First obtain an XLS IR file: $ bazel build -c opt //xls/examples:tiny_adder.opt.ir And then feed that XLS IR file into this binary: $ bazel run -c opt //xls/tools:smtlib_emitter_main -- --ir_path \\ $PWD/bazel-bin/xls/examples/tiny_adder.opt.ir (bvadd (concat #b0 x) (concat #b0 y)) To turn it into \"gate level\" SMTLib, we can do a pre-pass through the booleanify_main tool: $ bazel run -c opt //xls/tools:booleanify_main -- --ir_path \\ $PWD/bazel-bin/xls/examples/tiny_adder.opt.ir \\ > /tmp/tiny_adder.boolified.ir $ bazel run -c opt //xls/tools:smtlib_emitter_main -- \\ --ir_path /tmp/tiny_adder.boolified.ir (let ((a!1 (bvand (bvor ((_ extract 0 0) x) ((_ extract 0 0) y)) (bvnot (bvand ((_ extract 0 0) x) ((_ extract 0 0) y)))))) (let ((a!2 (bvor (bvand (bvor #b0 #b0) (bvnot (bvand #b0 #b0))) (bvor (bvand ((_ extract 0 0) x) ((_ extract 0 0) y)) (bvand a!1 #b0)))) (a!3 (bvand (bvand (bvor #b0 #b0) (bvnot (bvand #b0 #b0))) (bvor (bvand ((_ extract 0 0) x) ((_ extract 0 0) y)) (bvand a!1 #b0))))) (concat (bvand a!2 (bvnot a!3)) (bvand (bvor a!1 #b0) (bvnot (bvand a!1 #b0)))))) solver Uses a SMT solver (i.e. Z3) to prove properties of an XLS IR program from the command line. Currently the set of \"predicates\" that the solver supports from the command line are limited, but in theory it is capable of solving for arbitrary IR-function-specified predicates. This can be used to uncover opportunities for optimization that were missed, or to prove equivalence of transformed representations with their original version. cell_library_extract_formula Parses a cell library \".lib\" file and extracts boolean formulas from it that determine the functionality of cells. This is useful for LEC of the XLS IR against the post-synthesis netlist. dslx/highlight_main Performs terminal-based color code highlighting of a DSL file. dslx/typecheck_main Dumps type information that has been deduced for a given DSL file.","title":"Listing"},{"location":"tools/#xls-tools","text":"An index of XLS developer tools.","title":"XLS Tools"},{"location":"tools/#bdd_stats","text":"Constructs a binary decision diagram (BDD) using a given XLS function and prints various statistics about the BDD. BDD construction can be very slow in pathological cases and this utility is useful for identifying the underlying causes. Accepts arbitrary IR as input or a benchmark specified by name.","title":"bdd_stats"},{"location":"tools/#benchmark_main","text":"Prints numerous metrics and other information about an XLS IR file including: total delay, critical path, codegen information, optimization time, etc. This tool may be run against arbitrary IR not just the fixed set of XLS benchmarks. The output of this tool is scraped by run_benchmarks to construct a table comparing metrics against a mint CL across the benchmark suite.","title":"benchmark_main"},{"location":"tools/#booleanify_main","text":"Rewrites an XLS IR function in terms of its ops' fundamental AND/OR/NOT constituents, i.e., makes all operations boolean, thus it's \"booleanifying\" the function.","title":"booleanify_main"},{"location":"tools/#codegen_main","text":"Lowers an XLS IR file into Verilog. Options include emitting a feedforward pipeline or a purely combinational block. Emits both a Verilog file and a module signature which includes metadata about the block. The tool does not run any XLS passes so unoptimized IR may fail if the IR contains constructs not expected by the backend.","title":"codegen_main"},{"location":"tools/#io-configuration-options","text":"","title":"I/O Configuration Options"},{"location":"tools/#-flop_inputstrue-flop_inputs_kindzerolatency","text":"For each ready-valid-data channel on the input, an additional register is added so that the XLS pipeline need not stall a specific input channel while waiting for all to become ready. A single set of registers permits storage of one channel write() . (These are input-side channels, so the external write() can proceed even if the pipeline is not ready to read() .) Being a zero-latency buffer, the additional input logic does not increase the pipeline latency, but there remains a combinational path from the input ports to the pipeline register after the first stage.","title":"--flop_inputs=true --flop_inputs_kind=zerolatency"},{"location":"tools/#-flop_outputstrue-flop_outputs_kindskid","text":"A 1-cycle skid buffer is added to the output to break up output timing paths. (The output stage does contribute one additional cycle of latency.)","title":"--flop_outputs=true --flop_outputs_kind=skid"},{"location":"tools/#-add_idle_outputtrue","text":"An additional output port is generated named idle . Idle is the NOR of: Pipeline registers storing the \"valid\" bit for that particular stage. All valid registers stored for the input/output buffers. All valid signals for the input channels.","title":"--add_idle_output=true"},{"location":"tools/#-reset_data_pathtrue","text":"With this option, all pipeline registers (both data and valid registers) are reset to their initial state upon reset. This results in the reset signal fanning out to all registers in the block.","title":"--reset_data_path=true"},{"location":"tools/#-flop_single_value_channelsfalse","text":"With this option, the port for any single-value channels are not flopped, but taken as direct wires.","title":"--flop_single_value_channels=false"},{"location":"tools/#delay_info_main","text":"Dumps delay information about an XLS function including per-node delay information and critical-path.","title":"delay_info_main"},{"location":"tools/#eval_ir_main","text":"Evaluates an XLS IR file with user-specified or random inputs. Includes features for evaluating the IR before and after optimizations which makes this tool very useful for identifying optimization bugs. This tool accepts two [mutually exclusive] optional args, --input_validator_expr and --input_validator_path , which allow the user to specify an expression to \"filter\" potential input values to discard invalid ones. For both, the filter must be a function, named validator , and must take params of the same layout as the function under test. This function should return true if the inputs are valid for the function and false otherwise. --input_validator_expr lists the function as an inline command-line argument, whereas --input_validator_path holds the path to a .x file containing the validation function.","title":"eval_ir_main"},{"location":"tools/#ir_minimizer_main","text":"Tool for reducing IR to a minimal test case based on an external test.","title":"ir_minimizer_main"},{"location":"tools/#ir_stats_main","text":"Prints summary information/stats on an IR [Package] file. An example: $ bazel-bin/xls/tools/ir_stats_main bazel-genfiles/xls/modules/fp32_add_2.ir Package \"fp32_add_2\" Function: \"__float32__is_inf\" Signature: ((bits[1], bits[8], bits[23])) -> bits[1] Nodes: 8 Function: \"__float32__is_nan\" Signature: ((bits[1], bits[8], bits[23])) -> bits[1] Nodes: 8 Function: \"__fp32_add_2__fp32_add_2\" Signature: ((bits[1], bits[8], bits[23]), (bits[1], bits[8], bits[23])) -> (bits[1], bits[8], bits[23]) Nodes: 252","title":"ir_stats_main"},{"location":"tools/#check_ir_equivalence","text":"Verifies that two IR files (for example, optimized and unoptimized IR from the same source) are logically equivalent.","title":"check_ir_equivalence"},{"location":"tools/#opt_main","text":"Runs XLS IR through the optimization pipeline.","title":"opt_main"},{"location":"tools/#proto_to_dslx_main","text":"Takes in a proto schema and a textproto instance thereof and outputs a DSLX module containing a DSLX type and constant matching both inputs, respectively. Not all protocol buffer types map to DSLX types, so there are some restrictions or other behaviors requiring explanation: Only scalar and repeated fields are supported (i.e., no maps or oneofs, etc.). Only recursively-integral messages are supported, that is to say, a message may contain submessages, as long as all non-Message fields are integral. Since DSLX doesn't support variable arrays and Protocol Buffers don't support fixed-length repeated fields. To unify this, all instances of repeated-field-containing Messages must have the same size of their repeated members (declared as arrays in DSLX). This size will be calculated as the maximum size of any instance of that repeated field across all instances in the input textproto. For example, if a message Foo has a repeated field bar , and this message is present multiple times in the input textproto, say as: foo: { bar: 1 } foo: { bar: 1 bar: 2 } foo: { bar: 1 bar: 2 bar: 3 } the DSLX version of Foo will declare bar has a 3-element array. An accessory field, bar_count , will also be created, which will contain the number of valid entries in an actual instance of Foo::bar . The \"Fields\" example in ./xls/tools/testdata/proto_to_dslx_main.* demonstrates this behavior.","title":"proto_to_dslx_main"},{"location":"tools/#repl","text":"Allows you to interactively run various parts of the compiler, including parsing/type checking ( :reload ), lowering/optimization ( :ir ), Verilog codegen ( :verilog [identifier] ), and LLVM codegen ( :llvm , not yet implemented). You can also inspect the IR types of identifiers with :type , and even imported identifiers can be accessed with :type foo::bar .","title":"repl"},{"location":"tools/#simulate_module_main","text":"Runs an Verilog block emitted by XLS through a Verilog simulator. Requires both the Verilog text and the module signature which includes metadata about the block.","title":"simulate_module_main"},{"location":"tools/#smtlib_emitter_main","text":"Simple driver for Z3IrTranslator - converts a given IR function into its Z3 representation and outputs that translation as SMTLIB2. First obtain an XLS IR file: $ bazel build -c opt //xls/examples:tiny_adder.opt.ir And then feed that XLS IR file into this binary: $ bazel run -c opt //xls/tools:smtlib_emitter_main -- --ir_path \\ $PWD/bazel-bin/xls/examples/tiny_adder.opt.ir (bvadd (concat #b0 x) (concat #b0 y)) To turn it into \"gate level\" SMTLib, we can do a pre-pass through the booleanify_main tool: $ bazel run -c opt //xls/tools:booleanify_main -- --ir_path \\ $PWD/bazel-bin/xls/examples/tiny_adder.opt.ir \\ > /tmp/tiny_adder.boolified.ir $ bazel run -c opt //xls/tools:smtlib_emitter_main -- \\ --ir_path /tmp/tiny_adder.boolified.ir (let ((a!1 (bvand (bvor ((_ extract 0 0) x) ((_ extract 0 0) y)) (bvnot (bvand ((_ extract 0 0) x) ((_ extract 0 0) y)))))) (let ((a!2 (bvor (bvand (bvor #b0 #b0) (bvnot (bvand #b0 #b0))) (bvor (bvand ((_ extract 0 0) x) ((_ extract 0 0) y)) (bvand a!1 #b0)))) (a!3 (bvand (bvand (bvor #b0 #b0) (bvnot (bvand #b0 #b0))) (bvor (bvand ((_ extract 0 0) x) ((_ extract 0 0) y)) (bvand a!1 #b0))))) (concat (bvand a!2 (bvnot a!3)) (bvand (bvor a!1 #b0) (bvnot (bvand a!1 #b0))))))","title":"smtlib_emitter_main"},{"location":"tools/#solver","text":"Uses a SMT solver (i.e. Z3) to prove properties of an XLS IR program from the command line. Currently the set of \"predicates\" that the solver supports from the command line are limited, but in theory it is capable of solving for arbitrary IR-function-specified predicates. This can be used to uncover opportunities for optimization that were missed, or to prove equivalence of transformed representations with their original version.","title":"solver"},{"location":"tools/#cell_library_extract_formula","text":"Parses a cell library \".lib\" file and extracts boolean formulas from it that determine the functionality of cells. This is useful for LEC of the XLS IR against the post-synthesis netlist.","title":"cell_library_extract_formula"},{"location":"tools/#dslxhighlight_main","text":"Performs terminal-based color code highlighting of a DSL file.","title":"dslx/highlight_main"},{"location":"tools/#dslxtypecheck_main","text":"Dumps type information that has been deduced for a given DSL file.","title":"dslx/typecheck_main"},{"location":"tools_quick_start/","text":"XLS Tools Quick Start This document is a quick start guide through the use of the individual XLS tools, from DSL input to RTL generation. Note: This guide assumes you have set up your system so it can build the XLS tools via Bazel . There is currently no binary tools distribution so building from source is required. Create a file /tmp/simple_add.x with the following contents: fn add(x: u32, y: u32) -> u32 { x + y + u32:0 // Something to optimize. } #[test] fn test_add() { assert_eq(add(u32:2, u32:3), u32:5) } This contains a function, and a unit test of that function. Interpreting the DSL file Now, run it through the DSL interpreter -- the DSL interpreter is useful for interactive development and debugging. $ bazel run -c opt //xls/dslx:interpreter_main -- /tmp/simple_add.x [ RUN ] add [ OK ] add The DSL interpreter is the execution engine running the test shown. In lieu of using bazel run for the subsequent commands, this document will assume bazel build -c opt //xls/... has been completed so the binaries in ./bazel-bin can be used directly: $ ./bazel-bin/xls/dslx/interpreter_main /tmp/simple_add.x [ RUN ] add [ OK ] add DSL to IR conversion To convert the DSL file to IR, run the following command: $ ./bazel-bin/xls/dslx/ir_converter_main --top=add /tmp/simple_add.x > /tmp/simple_add.ir IR optimization To optimize the IR, use the opt_main tool: $ ./bazel-bin/xls/tools/opt_main /tmp/simple_add.ir > /tmp/simple_add.opt.ir Check the output of diff -U8 /tmp/simple_add*.ir to see that the optimizer eliminated the useless add-with-zero. Verilog RTL generation To generate RTL from the optimized IR, use the codegen_main tool: $ ./bazel-bin/xls/tools/codegen_main --pipeline_stages=1 --delay_model=unit /tmp/simple_add.opt.ir > /tmp/simple_add.v IR visualizer To get a graphical view of the IR files, use the IR visualization tool: $ ./bazel-bin/xls/visualization/ir_viz/app --delay_model=unit --preload_ir_path=/tmp/simple_add.ir This starts a server on localhost port 5000 by default, so you can access it from your machine as http://localhost:5000 in a web browser.","title":"Quick Start"},{"location":"tools_quick_start/#xls-tools-quick-start","text":"This document is a quick start guide through the use of the individual XLS tools, from DSL input to RTL generation. Note: This guide assumes you have set up your system so it can build the XLS tools via Bazel . There is currently no binary tools distribution so building from source is required. Create a file /tmp/simple_add.x with the following contents: fn add(x: u32, y: u32) -> u32 { x + y + u32:0 // Something to optimize. } #[test] fn test_add() { assert_eq(add(u32:2, u32:3), u32:5) } This contains a function, and a unit test of that function.","title":"XLS Tools Quick Start"},{"location":"tools_quick_start/#interpreting-the-dsl-file","text":"Now, run it through the DSL interpreter -- the DSL interpreter is useful for interactive development and debugging. $ bazel run -c opt //xls/dslx:interpreter_main -- /tmp/simple_add.x [ RUN ] add [ OK ] add The DSL interpreter is the execution engine running the test shown. In lieu of using bazel run for the subsequent commands, this document will assume bazel build -c opt //xls/... has been completed so the binaries in ./bazel-bin can be used directly: $ ./bazel-bin/xls/dslx/interpreter_main /tmp/simple_add.x [ RUN ] add [ OK ] add","title":"Interpreting the DSL file"},{"location":"tools_quick_start/#dsl-to-ir-conversion","text":"To convert the DSL file to IR, run the following command: $ ./bazel-bin/xls/dslx/ir_converter_main --top=add /tmp/simple_add.x > /tmp/simple_add.ir","title":"DSL to IR conversion"},{"location":"tools_quick_start/#ir-optimization","text":"To optimize the IR, use the opt_main tool: $ ./bazel-bin/xls/tools/opt_main /tmp/simple_add.ir > /tmp/simple_add.opt.ir Check the output of diff -U8 /tmp/simple_add*.ir to see that the optimizer eliminated the useless add-with-zero.","title":"IR optimization"},{"location":"tools_quick_start/#verilog-rtl-generation","text":"To generate RTL from the optimized IR, use the codegen_main tool: $ ./bazel-bin/xls/tools/codegen_main --pipeline_stages=1 --delay_model=unit /tmp/simple_add.opt.ir > /tmp/simple_add.v","title":"Verilog RTL generation"},{"location":"tools_quick_start/#ir-visualizer","text":"To get a graphical view of the IR files, use the IR visualization tool: $ ./bazel-bin/xls/visualization/ir_viz/app --delay_model=unit --preload_ir_path=/tmp/simple_add.ir This starts a server on localhost port 5000 by default, so you can access it from your machine as http://localhost:5000 in a web browser.","title":"IR visualizer"},{"location":"vast/","text":"Verilog Abstract Syntax Tree (VAST) XLS outputs Verilog (or SystemVerilog) for synthesis and simulation. As a lowest common denominator, Verilog output enables XLS generated designs to integrate into existing design flows. To make generation of Verilog easier, XLS includes an abstract representation of Verilog called VAST (Verilog Abstract Syntax Tree). VAST is a C++ library which represents Verilog in a recursive tree data structure which is simple to construct and manipulate programmatically. Verilog source code is emitted directly from the VAST data structure. VAST is intentionally not a complete representation of the Verilog language. VAST is used to emit Verilog for the purposes of code generation within XLS. Given this limited use case, VAST is much smaller and simpler than a complete representation of the entire Verilog language as might be required for a parser, for example. VAST Overview Each supported Verilog construct is represented with a C++ class. These classes form a type hierarchy with the class VastNode at the root. Objects are gathered in tree-shaped structures to represent Verilog constructs. Ownership of all VAST objects is maintained by a VerilogFile object which represents a single file of Verilog source code. References between objects are stored as plain pointers. For example, consider the following Verilog expression: foo + 8 In VAST, this is represented with an object of the BinaryInfix class which is derived from the Expression class representing arbitrary Verilog expressions. A BinaryInfix object has three relevant data members: std::string op_; : The string representation of the operation to perform (e.g., + ). Expression* lhs_; : The left-hand-side of the expression. In this example, this points to a LogicRef object (derived from Expression class) referring to a Verilog reg or wire variable. Expression* rhs_; : The left-hand-side of the expression. In this example, this points to a Literal object (derived from Expression class) containing the number 8 with unspecified bit width. The BinaryInfix object representing foo + 8 might be used within other expressions or statements by referring to the object by pointer. For example, the representation of the statement assign bar = foo + 8 would contain an Expression* pointer referring to the foo + 8 object for the right-hand-side of the assignment. Operator Precedence To avoid ambiguity, operators in Verilog follow precedence rules. For example, multiplication is higher precedence than addition so the expression 2 + 4 * 10 evaluates to 42 (i.e., 2 + (4 * 10) ) not 60 (i.e., (2 + 4) * 10 ). In VAST, expressions are built as a trees which is evaluated from the leaves to the root. To ensure that the operations are evaluated in the correct order when emitted as Verilog text, VAST automatically adds parentheses where appropriate. For example, the VAST expression consisting of the product ( BinaryInfix with operation * ) of 10 and the sum of 2 and 4 ( BinaryInfix with operation + ) will be emitted as 10 * (2 + 4) . Containers VAST has a number of classes which hold a sequence of (pointers to) other VAST objects. At the top-level, this includes the VerilogFile class which can hold a sequence of objects such as include statements and modules. Verilog modules themselves are represented with the Module class containing a sequence of statements, declarations, comments, and other constructs. Other containers include always blocks and functions. Emitting Verilog text VAST classes include an Emit method which returns the represented Verilog construct as a string. Typically, Emit is called on the top-level VerilogFile object to create the text of the entire Verilog source file. Underneath the hood, this method calls the Emit method on all contained VAST objects and assembles the returned strings into the Verilog source code. SystemVerilog support XLS can emit either Verilog or SystemVerilog so VAST supports both languages. SystemVerilog constructs are included alongside Verilog constructs in VAST. Examples of SystemVerilog features supported by VAST include: always_ff procedure for modeling sequential logic (VAST AlwaysFlop class). Array assignment pattern (VAST ArrayAssignmentPattern class). Example: '{foo, bar, baz} Array declaration using sizes. Example: reg [7:0] foo[42]; Within VAST, there is no distinction between the two languages and it is up to the user of VAST to only use the supported features for the target language (Verilog or SystemVerilog).","title":"VAST"},{"location":"vast/#verilog-abstract-syntax-tree-vast","text":"XLS outputs Verilog (or SystemVerilog) for synthesis and simulation. As a lowest common denominator, Verilog output enables XLS generated designs to integrate into existing design flows. To make generation of Verilog easier, XLS includes an abstract representation of Verilog called VAST (Verilog Abstract Syntax Tree). VAST is a C++ library which represents Verilog in a recursive tree data structure which is simple to construct and manipulate programmatically. Verilog source code is emitted directly from the VAST data structure. VAST is intentionally not a complete representation of the Verilog language. VAST is used to emit Verilog for the purposes of code generation within XLS. Given this limited use case, VAST is much smaller and simpler than a complete representation of the entire Verilog language as might be required for a parser, for example.","title":"Verilog Abstract Syntax Tree (VAST)"},{"location":"vast/#vast-overview","text":"Each supported Verilog construct is represented with a C++ class. These classes form a type hierarchy with the class VastNode at the root. Objects are gathered in tree-shaped structures to represent Verilog constructs. Ownership of all VAST objects is maintained by a VerilogFile object which represents a single file of Verilog source code. References between objects are stored as plain pointers. For example, consider the following Verilog expression: foo + 8 In VAST, this is represented with an object of the BinaryInfix class which is derived from the Expression class representing arbitrary Verilog expressions. A BinaryInfix object has three relevant data members: std::string op_; : The string representation of the operation to perform (e.g., + ). Expression* lhs_; : The left-hand-side of the expression. In this example, this points to a LogicRef object (derived from Expression class) referring to a Verilog reg or wire variable. Expression* rhs_; : The left-hand-side of the expression. In this example, this points to a Literal object (derived from Expression class) containing the number 8 with unspecified bit width. The BinaryInfix object representing foo + 8 might be used within other expressions or statements by referring to the object by pointer. For example, the representation of the statement assign bar = foo + 8 would contain an Expression* pointer referring to the foo + 8 object for the right-hand-side of the assignment.","title":"VAST Overview"},{"location":"vast/#operator-precedence","text":"To avoid ambiguity, operators in Verilog follow precedence rules. For example, multiplication is higher precedence than addition so the expression 2 + 4 * 10 evaluates to 42 (i.e., 2 + (4 * 10) ) not 60 (i.e., (2 + 4) * 10 ). In VAST, expressions are built as a trees which is evaluated from the leaves to the root. To ensure that the operations are evaluated in the correct order when emitted as Verilog text, VAST automatically adds parentheses where appropriate. For example, the VAST expression consisting of the product ( BinaryInfix with operation * ) of 10 and the sum of 2 and 4 ( BinaryInfix with operation + ) will be emitted as 10 * (2 + 4) .","title":"Operator Precedence"},{"location":"vast/#containers","text":"VAST has a number of classes which hold a sequence of (pointers to) other VAST objects. At the top-level, this includes the VerilogFile class which can hold a sequence of objects such as include statements and modules. Verilog modules themselves are represented with the Module class containing a sequence of statements, declarations, comments, and other constructs. Other containers include always blocks and functions.","title":"Containers"},{"location":"vast/#emitting-verilog-text","text":"VAST classes include an Emit method which returns the represented Verilog construct as a string. Typically, Emit is called on the top-level VerilogFile object to create the text of the entire Verilog source file. Underneath the hood, this method calls the Emit method on all contained VAST objects and assembles the returned strings into the Verilog source code.","title":"Emitting Verilog text"},{"location":"vast/#systemverilog-support","text":"XLS can emit either Verilog or SystemVerilog so VAST supports both languages. SystemVerilog constructs are included alongside Verilog constructs in VAST. Examples of SystemVerilog features supported by VAST include: always_ff procedure for modeling sequential logic (VAST AlwaysFlop class). Array assignment pattern (VAST ArrayAssignmentPattern class). Example: '{foo, bar, baz} Array declaration using sizes. Example: reg [7:0] foo[42]; Within VAST, there is no distinction between the two languages and it is up to the user of VAST to only use the supported features for the target language (Verilog or SystemVerilog).","title":"SystemVerilog support"},{"location":"xls_noc_butterfly_topology/","text":"k-ary n-fly Butterfly Topology Types Overview A k -ary n -fly butterfly topology type is a multistage logarithm network. It is implemented using n stages of identical routers, where k is the number of channels of a router that connect to the previous and/or to the next stage. For example, a 2-ary 3-tree butterfly topology has three stages composed of routers with two channels connect to the routers of the previous and/or the next stage (see Figure Butterfly_Topology_Example). Each endpoint node and channel has an n -digit radix- k identifier, {d n\u22121 , d n\u22122 , ..., d 0 }. The first n\u22121 digits {d n\u22121 , d n\u22122 , ..., d 1 } of the identifier corresponds to the router that it is connected to. Each router node has an n\u22121 -digit radix- k identifier. To distinguish nodes and channels from different stages, the stage number is appended to their identifier separated by a period. For example, for a 2-ary 4-fly butterfly network: 2.1010 2 is channel 10 10 from stage 2. Butterfly_Ref_0 The connection between the stages is a permutation of the channel identifier. The connection of a channel from stage i -1 to stage i swaps digits d n\u2212i and d 0 of the channel identifier, with i >= 1 and i < n. For example, for a 2-ary 4-fly butterfly network: channel 9 10 of stage 1 [1.1001 2 ] is connected to router 4 [1. 100 1 2 ] of stage 1 and router 6 [2. 110 0 2 ] of stage 2. Butterfly_Ref_0 Figure Butterfly_Topology_Example . The router nodes of a 2-ary 3-tree butterfly topology. Figure Butterfly_Topology_Example shows the router nodes of a 2-ary 3-tree butterfly topology. The routers are labelled with an S.I format where S is the stage identifier and I is the router identifier. For example, router 0.2 is router 2 from stage 0. Channel 0.6 is connected to router 3 [0. 11 0 2 ] of stage 0 and router 1 of stage 1 [1. 01 1 2 ], Channel 1.3 is connected to router 1 [1. 01 1 2 ] of stage 1 and to router 1 [1. 01 1 2 ] of stage 2. Unidirectional Types For the unidirectional type of the k -ary n -fly, there are endpoints connected to stage 0 and stage n -1. There are two configurations: 1) the endpoints connected to stage 0 send to the network, and the endpoints connected to stage n -1 receive from the network, and 2) the endpoints connected to stage 0 receive from the network, and the endpoints connected to stage n -1 send to the network. A unidirectional k -ary n -fly butterfly topology has at most k n endpoints connected to stage 0 and at most k n endpoints connected to stage n -1. Typically, the endpoints nodes connected to stage 0 and stage n -1 are the same nodes, resulting in an equal number of nodes connected to stage 0 and stage n -1. Moreover, the endpoints nodes are connected to stage 0 in the same order as stage n -1 (mirrored from the vertical cross section). Figure Unidirectional_Butterfly_Topology_Example . A 2-ary 3-fly butterfly with the endpoints connected to stage 0 sending to the network, and the endpoints connected to stage 2 receiving from the network example. Figure Unidirectional_Butterfly_Topology_Example shows a 2-ary 3-fly butterfly with the endpoints connected to stage 0 sending to the network, and the endpoints connected to stage 2 receiving from the network example. Bidirectional Types In the bidirectional type, there are endpoints connected to stage 0 or stage n -1. A bidirectional k -ary n -fly butterfly topology has at most k n endpoints connected to stage 0 or stage n -1. A bidirectional k -ary n -fly butterfly topology with the endpoints connected to stage n -1 is also referred to as the k -ary n -fly fat tree butterfly topology or k -ary n -tree butterfly fat tree topology Fat_Tree_Ref_0 . For context, minimal routing between a pair of nodes on a bidirectional k-ary n-tree can be accomplished by sending the message to one of the nearest common ancestors of both source and destination and from there to the destination. That is, each message experiences two phases, an ascending phase to get to a nearest common ancestor, followed by a descending phase. Fat_Tree_Ref_1 Figure Bidirectional_Butterfly_Topology_Example . A bidirectional 2-ary 3-fly butterfly with the endpoints connected to stage 0 example. Figure Bidirectional_Butterfly_Topology_Example shows a bidirectional 2-ary 3-fly butterfly with the endpoints connected to stage 0 example. Cheat Sheet k -ary n -flies are implemented by using n stages of identical routers, where k is the number of channels of a router that connect to the previous and/or to the next stage. For unidirectional k -ary n -fly , there are endpoints connected to stage 0 and stage n -1. The communication flows from stage 0 to stage n -1 or from stage n -1 to stage 0. For bidirectional k -ary n -fly , there are endpoints connected to stage 0 or stage n -1. The communication flows from stage 0 to stage n -1 and from stage n -1 to stage 0. Variants This section describes variants to the k -ary n -fly butterfly topology. Flattened Butterfly The flattened butterfly is derived by flattening the routers in each row of a conventional butterfly topology while maintaining the same inter-router connections. Butterfly_Ref_1 Figure Flattened_Butterfly_Topology_Example . An example of a 2-ary 3-fly butterfly and its corresponding flattened butterfly topology. Figure Flattened_Butterfly_Topology_Example shows an example of a 2-ary 3-fly butterfly and its corresponding flattened butterfly topology. For each row, routers from stage 0, 1 and 2 are flattened into individual routers. For example, router 0.0, 1.0 and 2.0 are flattened into router 0. Endpoint Connections As mentioned, typically, for the unidirectional butterfly topology, the endpoints nodes are connected to stage 0 in the same order as stage n -1 (mirrored from the vertical cross section). However, different topologies are said to emerge by modifying the order of the endpoint nodes connected to the network. Such a change in connectivity may also change the routing algorithm. Ludovici et al present a Reduced Unidirectional Fat Tree derived from a k-ary n-fly topology. Fat_Tree_Ref_0 References [Butterfly_Ref_0] William James Dally and Brian Patrick Towles. 2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. [Fat_Tree_Ref_0] D. Ludovici et al., \"Assessing fat-tree topologies for regular network-on-chip design under nanoscale technology constraints,\" 2009 Design, Automation & Test in Europe Conference & Exhibition, Nice, 2009, pp. 562-565, doi: 10.1109/DATE.2009.5090727. [Fat_Tree_Ref_1] F. Petrini and M. Vanneschi, \"k-ary n-trees: high performance networks for massively parallel architectures,\" Proceedings 11th International Parallel Processing Symposium, Genva, Switzerland, 1997, pp. 87-93, doi: 10.1109/IPPS.1997.580853. [Butterfly_Ref_1] J. Kim, J. Balfour and W. Dally, \"Flattened Butterfly Topology for On-Chip Networks,\" 40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007), Chicago, IL, 2007, pp. 172-182, doi: 10.1109/MICRO.2007.29.","title":"k-ary n-fly Butterfly"},{"location":"xls_noc_butterfly_topology/#k-ary-n-fly-butterfly-topology-types","text":"","title":"k-ary n-fly Butterfly Topology Types"},{"location":"xls_noc_butterfly_topology/#overview","text":"A k -ary n -fly butterfly topology type is a multistage logarithm network. It is implemented using n stages of identical routers, where k is the number of channels of a router that connect to the previous and/or to the next stage. For example, a 2-ary 3-tree butterfly topology has three stages composed of routers with two channels connect to the routers of the previous and/or the next stage (see Figure Butterfly_Topology_Example). Each endpoint node and channel has an n -digit radix- k identifier, {d n\u22121 , d n\u22122 , ..., d 0 }. The first n\u22121 digits {d n\u22121 , d n\u22122 , ..., d 1 } of the identifier corresponds to the router that it is connected to. Each router node has an n\u22121 -digit radix- k identifier. To distinguish nodes and channels from different stages, the stage number is appended to their identifier separated by a period. For example, for a 2-ary 4-fly butterfly network: 2.1010 2 is channel 10 10 from stage 2. Butterfly_Ref_0 The connection between the stages is a permutation of the channel identifier. The connection of a channel from stage i -1 to stage i swaps digits d n\u2212i and d 0 of the channel identifier, with i >= 1 and i < n. For example, for a 2-ary 4-fly butterfly network: channel 9 10 of stage 1 [1.1001 2 ] is connected to router 4 [1. 100 1 2 ] of stage 1 and router 6 [2. 110 0 2 ] of stage 2. Butterfly_Ref_0 Figure Butterfly_Topology_Example . The router nodes of a 2-ary 3-tree butterfly topology. Figure Butterfly_Topology_Example shows the router nodes of a 2-ary 3-tree butterfly topology. The routers are labelled with an S.I format where S is the stage identifier and I is the router identifier. For example, router 0.2 is router 2 from stage 0. Channel 0.6 is connected to router 3 [0. 11 0 2 ] of stage 0 and router 1 of stage 1 [1. 01 1 2 ], Channel 1.3 is connected to router 1 [1. 01 1 2 ] of stage 1 and to router 1 [1. 01 1 2 ] of stage 2.","title":"Overview "},{"location":"xls_noc_butterfly_topology/#unidirectional-types","text":"For the unidirectional type of the k -ary n -fly, there are endpoints connected to stage 0 and stage n -1. There are two configurations: 1) the endpoints connected to stage 0 send to the network, and the endpoints connected to stage n -1 receive from the network, and 2) the endpoints connected to stage 0 receive from the network, and the endpoints connected to stage n -1 send to the network. A unidirectional k -ary n -fly butterfly topology has at most k n endpoints connected to stage 0 and at most k n endpoints connected to stage n -1. Typically, the endpoints nodes connected to stage 0 and stage n -1 are the same nodes, resulting in an equal number of nodes connected to stage 0 and stage n -1. Moreover, the endpoints nodes are connected to stage 0 in the same order as stage n -1 (mirrored from the vertical cross section). Figure Unidirectional_Butterfly_Topology_Example . A 2-ary 3-fly butterfly with the endpoints connected to stage 0 sending to the network, and the endpoints connected to stage 2 receiving from the network example. Figure Unidirectional_Butterfly_Topology_Example shows a 2-ary 3-fly butterfly with the endpoints connected to stage 0 sending to the network, and the endpoints connected to stage 2 receiving from the network example.","title":"Unidirectional Types "},{"location":"xls_noc_butterfly_topology/#bidirectional-types","text":"In the bidirectional type, there are endpoints connected to stage 0 or stage n -1. A bidirectional k -ary n -fly butterfly topology has at most k n endpoints connected to stage 0 or stage n -1. A bidirectional k -ary n -fly butterfly topology with the endpoints connected to stage n -1 is also referred to as the k -ary n -fly fat tree butterfly topology or k -ary n -tree butterfly fat tree topology Fat_Tree_Ref_0 . For context, minimal routing between a pair of nodes on a bidirectional k-ary n-tree can be accomplished by sending the message to one of the nearest common ancestors of both source and destination and from there to the destination. That is, each message experiences two phases, an ascending phase to get to a nearest common ancestor, followed by a descending phase. Fat_Tree_Ref_1 Figure Bidirectional_Butterfly_Topology_Example . A bidirectional 2-ary 3-fly butterfly with the endpoints connected to stage 0 example. Figure Bidirectional_Butterfly_Topology_Example shows a bidirectional 2-ary 3-fly butterfly with the endpoints connected to stage 0 example.","title":"Bidirectional Types "},{"location":"xls_noc_butterfly_topology/#cheat-sheet","text":"k -ary n -flies are implemented by using n stages of identical routers, where k is the number of channels of a router that connect to the previous and/or to the next stage. For unidirectional k -ary n -fly , there are endpoints connected to stage 0 and stage n -1. The communication flows from stage 0 to stage n -1 or from stage n -1 to stage 0. For bidirectional k -ary n -fly , there are endpoints connected to stage 0 or stage n -1. The communication flows from stage 0 to stage n -1 and from stage n -1 to stage 0.","title":"Cheat Sheet"},{"location":"xls_noc_butterfly_topology/#variants","text":"This section describes variants to the k -ary n -fly butterfly topology.","title":"Variants"},{"location":"xls_noc_butterfly_topology/#flattened-butterfly","text":"The flattened butterfly is derived by flattening the routers in each row of a conventional butterfly topology while maintaining the same inter-router connections. Butterfly_Ref_1 Figure Flattened_Butterfly_Topology_Example . An example of a 2-ary 3-fly butterfly and its corresponding flattened butterfly topology. Figure Flattened_Butterfly_Topology_Example shows an example of a 2-ary 3-fly butterfly and its corresponding flattened butterfly topology. For each row, routers from stage 0, 1 and 2 are flattened into individual routers. For example, router 0.0, 1.0 and 2.0 are flattened into router 0.","title":"Flattened Butterfly"},{"location":"xls_noc_butterfly_topology/#endpoint-connections","text":"As mentioned, typically, for the unidirectional butterfly topology, the endpoints nodes are connected to stage 0 in the same order as stage n -1 (mirrored from the vertical cross section). However, different topologies are said to emerge by modifying the order of the endpoint nodes connected to the network. Such a change in connectivity may also change the routing algorithm. Ludovici et al present a Reduced Unidirectional Fat Tree derived from a k-ary n-fly topology. Fat_Tree_Ref_0","title":"Endpoint Connections"},{"location":"xls_noc_butterfly_topology/#references","text":"[Butterfly_Ref_0] William James Dally and Brian Patrick Towles. 2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. [Fat_Tree_Ref_0] D. Ludovici et al., \"Assessing fat-tree topologies for regular network-on-chip design under nanoscale technology constraints,\" 2009 Design, Automation & Test in Europe Conference & Exhibition, Nice, 2009, pp. 562-565, doi: 10.1109/DATE.2009.5090727. [Fat_Tree_Ref_1] F. Petrini and M. Vanneschi, \"k-ary n-trees: high performance networks for massively parallel architectures,\" Proceedings 11th International Parallel Processing Symposium, Genva, Switzerland, 1997, pp. 87-93, doi: 10.1109/IPPS.1997.580853. [Butterfly_Ref_1] J. Kim, J. Balfour and W. Dally, \"Flattened Butterfly Topology for On-Chip Networks,\" 40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007), Chicago, IL, 2007, pp. 172-182, doi: 10.1109/MICRO.2007.29.","title":"References"},{"location":"xls_noc_dimension_order_topology/","text":"Dimension-Order Topology Types Overview The dimension-order topology is a topology where the arrangement of the routers are described by their location in a dimensional space. The topology has a hierarchical structure where dimension n is composed of structures from dimension n-1 . One or more endpoint nodes are connected to a router. k-ary n-cube A k -ary n -cube topology consists of k n routers arranged in an n -dimensional cube with k routers along each dimension. Each router is assigned an n -digit radix- k address {a n\u22121 , ..., a 0 } and is connected to all routers with addresses that differ by \u00b11(mod k ) in exactly one address digit. Each dimension is constructed using k k -ary ( n-1 )-cubes. The channels between the routers can be unidirectional or bidirectional. In practice, the channels are bidirectional. Ring Topology Figure Ring_4ary_1cube_example . A ring topology with four routers, also known as a 4-ary 1-cube topology. The ring topology is a special instance of the k -ary n -cube, where n is equal to one. Figure Ring_4ary_1cube_example shows a ring topology with four routers, also known as a 4-ary 1-cube topology. The dotted lines show the connectivity between routers 0 and k-1 for a dimension. Symmetric Torus Topology Figure Symmetric_Torus_4ary_2cube_example . A symmetric torus topology with four routers along each dimension, also known as a 4-ary 2-cube topology. The symmetric torus topology is a special instance of the k -ary n -cube, where n is equal to two. Figure Symmetric_Torus_4ary_2cube_example shows a symmetric torus topology with four routers along each dimension, also known as a 4-ary 2-cube topology. Router (3,0) has address 3 in a dimension and 0 in the other dimension. The dotted lines show the connectivity between routers 0 and k-1 for a dimension. Figure 4ary_2cube_construction_example . The construction of a symmetric torus (4-ary 2-cube) topology using 4-ary 1-cubes. Figure 4ary_2cube_construction_example shows the construction of a symmetric torus (4-ary 2-cube) topology using 4-ary 1-cubes (ring topologies). multi-radix n-cube A multi-radix n -cube is a generalized form of a k -ary n -cube topology where the radix (number of router nodes) for each dimension is explicitly stated. Each router is assigned an n -digit where the digit at index i has the radix of dimension i . A 2,3-ary 2-cube describes a grid-like topology with a radix of 2 in one dimension and radix of 3 in the other dimension. Figure 2,3-ary_2cube_example . A 2,3-ary 2-cube topology example. Figure 2,3-ary_2cube_example shows a 2,3-ary 2-cube topology example. The dotted lines show the connectivity between router 0 and router radix i - 1 for each dimension. k-ary n-mesh A k -ary n -mesh is a k -ary n -cube topology with the connection from address a k\u22121 to address a 0 omitted in each dimension. Line Figure Line_4ary_1mesh_example . A line topology with four routers, also known as a 4-ary 1-mesh topology. The line topology is a special instance of the k -ary n -mesh, where n is equal to one. Figure Line_4ary_1mesh_example shows a line topology with four routers, also known as a 4-ary 1-mesh topology. Compared to the ring topology in Figure Ring_4ary_1cube_example , the connection from router 0 to router 3 is omitted. Symmetric Mesh Figure Symmetric_Mesh_4ary_2mesh_example . A symmetric mesh topology with four routers along each dimension, also known as a 4-ary 2-mesh topology. The symmetric torus mesh is a special instance of the k -ary n -mesh, where n is equal to two. Figure Symmetric_Mesh_4ary_2mesh_example shows a symmetric mesh topology with four routers along each dimension, also known as a 4-ary 2-mesh topology. Router (1,2) has address 1 in a dimension and 2 in the other dimension. Compared to the torus topology in Figure Symmetric_Torus_4ary_2cube_example , the connections between the pair of routers [(0,0), (0,3)], [(1,0), (1,3)], [(2,0), (2,3)], [(3,0), (3,3)], [(0,0), (3,0)], [(0,1), (3,1)], [(0,2), (3,2)], and [(0,3), (3,3)] are omitted. multi-radix n-mesh Similar to the multi-radix n -cube, a multi-radix n -mesh is a generalized form of a k -ary n -mesh topology where the radix (number of router nodes) for each dimension is explicitly stated. Each router is assigned an n -digit where the digit at index i has the radix of dimension i . A 2,3-ary 2-mesh describes a grid-like topology with a radix of 2 in one dimension and radix of 3 in the other dimension. Figure 2,3-ary_2mesh_example . A 2,3-ary 2-mesh topology example. Figure 2,3-ary_2mesh_example shows a 2,3-ary 2-mesh topology example. Compared to the multi-radix n-cube topology in Figure 2,3-ary_2cube_example , the connections from the pair of routers [(0,0), (0,2)], [(1,0), (1,2)], [(0,0), (1,0)], [(0,1), (1,1)] and [(0,2), (1,2)] are omitted. Cheat Sheet A k -ary n -cube topology consists of k n routers arranged in an n-dimensional cube with k router nodes along each dimension. Each router is assigned an n -digit radix- k address {a n\u22121 , ..., a 0 } and is connected to all routers with addresses that differ by \u00b11 (mod k) in exactly one address digit. Each dimension is constructed using k k -ary ( n -1)-cubes where there are k routers in the first dimension. A multi-radix n-cube is a generalized form of a k -ary n -cube topology where the radix (number of routers) for each dimension is explicitly stated. Each router is assigned an n -digit where the digit at index i has the radix of dimension i . A k -ary n -mesh is a k -ary n -cube topology with the connection from address a k\u22121 to address a 0 omitted in each dimension. Similar to the multi-radix n -cube, a multi-radix n-mesh is a generalized form of a k -ary n -mesh topology where the radix (number of routers) for each dimension is explicitly stated. Each router is assigned an n -digit where the digit at index i has the radix of dimension i . Channel direction can be unidirectional or bidirectional. Ring : k-ary 1-cube Symmetric Torus : k-ary 2-cube Line : k-ary 1-mesh Symmetric Mesh : k-ary 2-mesh References William James Dally and Brian Patrick Towles. 2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.","title":"Dimension Order"},{"location":"xls_noc_dimension_order_topology/#dimension-order-topology-types","text":"","title":"Dimension-Order Topology Types"},{"location":"xls_noc_dimension_order_topology/#overview","text":"The dimension-order topology is a topology where the arrangement of the routers are described by their location in a dimensional space. The topology has a hierarchical structure where dimension n is composed of structures from dimension n-1 . One or more endpoint nodes are connected to a router.","title":"Overview"},{"location":"xls_noc_dimension_order_topology/#k-ary-n-cube","text":"A k -ary n -cube topology consists of k n routers arranged in an n -dimensional cube with k routers along each dimension. Each router is assigned an n -digit radix- k address {a n\u22121 , ..., a 0 } and is connected to all routers with addresses that differ by \u00b11(mod k ) in exactly one address digit. Each dimension is constructed using k k -ary ( n-1 )-cubes. The channels between the routers can be unidirectional or bidirectional. In practice, the channels are bidirectional.","title":"k-ary n-cube "},{"location":"xls_noc_dimension_order_topology/#ring-topology","text":"Figure Ring_4ary_1cube_example . A ring topology with four routers, also known as a 4-ary 1-cube topology. The ring topology is a special instance of the k -ary n -cube, where n is equal to one. Figure Ring_4ary_1cube_example shows a ring topology with four routers, also known as a 4-ary 1-cube topology. The dotted lines show the connectivity between routers 0 and k-1 for a dimension.","title":"Ring Topology "},{"location":"xls_noc_dimension_order_topology/#symmetric-torus-topology","text":"Figure Symmetric_Torus_4ary_2cube_example . A symmetric torus topology with four routers along each dimension, also known as a 4-ary 2-cube topology. The symmetric torus topology is a special instance of the k -ary n -cube, where n is equal to two. Figure Symmetric_Torus_4ary_2cube_example shows a symmetric torus topology with four routers along each dimension, also known as a 4-ary 2-cube topology. Router (3,0) has address 3 in a dimension and 0 in the other dimension. The dotted lines show the connectivity between routers 0 and k-1 for a dimension. Figure 4ary_2cube_construction_example . The construction of a symmetric torus (4-ary 2-cube) topology using 4-ary 1-cubes. Figure 4ary_2cube_construction_example shows the construction of a symmetric torus (4-ary 2-cube) topology using 4-ary 1-cubes (ring topologies).","title":"Symmetric Torus Topology "},{"location":"xls_noc_dimension_order_topology/#multi-radix-n-cube","text":"A multi-radix n -cube is a generalized form of a k -ary n -cube topology where the radix (number of router nodes) for each dimension is explicitly stated. Each router is assigned an n -digit where the digit at index i has the radix of dimension i . A 2,3-ary 2-cube describes a grid-like topology with a radix of 2 in one dimension and radix of 3 in the other dimension. Figure 2,3-ary_2cube_example . A 2,3-ary 2-cube topology example. Figure 2,3-ary_2cube_example shows a 2,3-ary 2-cube topology example. The dotted lines show the connectivity between router 0 and router radix i - 1 for each dimension.","title":"multi-radix n-cube "},{"location":"xls_noc_dimension_order_topology/#k-ary-n-mesh","text":"A k -ary n -mesh is a k -ary n -cube topology with the connection from address a k\u22121 to address a 0 omitted in each dimension.","title":"k-ary n-mesh "},{"location":"xls_noc_dimension_order_topology/#line","text":"Figure Line_4ary_1mesh_example . A line topology with four routers, also known as a 4-ary 1-mesh topology. The line topology is a special instance of the k -ary n -mesh, where n is equal to one. Figure Line_4ary_1mesh_example shows a line topology with four routers, also known as a 4-ary 1-mesh topology. Compared to the ring topology in Figure Ring_4ary_1cube_example , the connection from router 0 to router 3 is omitted.","title":"Line "},{"location":"xls_noc_dimension_order_topology/#symmetric-mesh","text":"Figure Symmetric_Mesh_4ary_2mesh_example . A symmetric mesh topology with four routers along each dimension, also known as a 4-ary 2-mesh topology. The symmetric torus mesh is a special instance of the k -ary n -mesh, where n is equal to two. Figure Symmetric_Mesh_4ary_2mesh_example shows a symmetric mesh topology with four routers along each dimension, also known as a 4-ary 2-mesh topology. Router (1,2) has address 1 in a dimension and 2 in the other dimension. Compared to the torus topology in Figure Symmetric_Torus_4ary_2cube_example , the connections between the pair of routers [(0,0), (0,3)], [(1,0), (1,3)], [(2,0), (2,3)], [(3,0), (3,3)], [(0,0), (3,0)], [(0,1), (3,1)], [(0,2), (3,2)], and [(0,3), (3,3)] are omitted.","title":"Symmetric Mesh "},{"location":"xls_noc_dimension_order_topology/#multi-radix-n-mesh","text":"Similar to the multi-radix n -cube, a multi-radix n -mesh is a generalized form of a k -ary n -mesh topology where the radix (number of router nodes) for each dimension is explicitly stated. Each router is assigned an n -digit where the digit at index i has the radix of dimension i . A 2,3-ary 2-mesh describes a grid-like topology with a radix of 2 in one dimension and radix of 3 in the other dimension. Figure 2,3-ary_2mesh_example . A 2,3-ary 2-mesh topology example. Figure 2,3-ary_2mesh_example shows a 2,3-ary 2-mesh topology example. Compared to the multi-radix n-cube topology in Figure 2,3-ary_2cube_example , the connections from the pair of routers [(0,0), (0,2)], [(1,0), (1,2)], [(0,0), (1,0)], [(0,1), (1,1)] and [(0,2), (1,2)] are omitted.","title":"multi-radix n-mesh "},{"location":"xls_noc_dimension_order_topology/#cheat-sheet","text":"A k -ary n -cube topology consists of k n routers arranged in an n-dimensional cube with k router nodes along each dimension. Each router is assigned an n -digit radix- k address {a n\u22121 , ..., a 0 } and is connected to all routers with addresses that differ by \u00b11 (mod k) in exactly one address digit. Each dimension is constructed using k k -ary ( n -1)-cubes where there are k routers in the first dimension. A multi-radix n-cube is a generalized form of a k -ary n -cube topology where the radix (number of routers) for each dimension is explicitly stated. Each router is assigned an n -digit where the digit at index i has the radix of dimension i . A k -ary n -mesh is a k -ary n -cube topology with the connection from address a k\u22121 to address a 0 omitted in each dimension. Similar to the multi-radix n -cube, a multi-radix n-mesh is a generalized form of a k -ary n -mesh topology where the radix (number of routers) for each dimension is explicitly stated. Each router is assigned an n -digit where the digit at index i has the radix of dimension i . Channel direction can be unidirectional or bidirectional. Ring : k-ary 1-cube Symmetric Torus : k-ary 2-cube Line : k-ary 1-mesh Symmetric Mesh : k-ary 2-mesh","title":"Cheat Sheet"},{"location":"xls_noc_dimension_order_topology/#references","text":"William James Dally and Brian Patrick Towles. 2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.","title":"References"},{"location":"xls_noc_fully_connected_topology/","text":"Fully-connected Topology Types The fully-connected topology is a topology where each router has a bidirectional channel connecting it to each remaining router of the topology. One or more endpoint nodes are connected to a router. Figure Fully_Connected_example . A fully-connected topology with six routers. Figure Fully_Connected_example shows a fully-connected topology with six routers. Cheat Sheet The fully-connected topology is a topology where each router has a bidirectional connecting it to each remaining router of the topology.","title":"Fully Connected"},{"location":"xls_noc_fully_connected_topology/#fully-connected-topology-types","text":"The fully-connected topology is a topology where each router has a bidirectional channel connecting it to each remaining router of the topology. One or more endpoint nodes are connected to a router. Figure Fully_Connected_example . A fully-connected topology with six routers. Figure Fully_Connected_example shows a fully-connected topology with six routers.","title":"Fully-connected Topology Types "},{"location":"xls_noc_fully_connected_topology/#cheat-sheet","text":"The fully-connected topology is a topology where each router has a bidirectional connecting it to each remaining router of the topology.","title":"Cheat Sheet"},{"location":"xls_noc_glossary/","text":"XLS-NoC Glossary Adaptive Routing An adaptive routing algorithm derives a route using any information about the network\u2019s state. Channel A channel interconnects a pair of nodes and is the medium that transfers data amongst the pair. A node is either a router input, a router output or an endpoint port. A channel may be unidirectional, transferring the data in a single direction, or bidirectional, transferring the data in both directions. Endpoint A node connected to the network that communicates to other endpoints using the network. Endpoint Port An endpoint port is a port that connects to an endpoint. There are two types of endpoint ports: a send port and a receive port. A send port enables an endpoint to send data to the network and a receive port enables an endpoint to receive data from the network. The naming convention for a send port and receive port is from the perspective of the endpoints. Deadlock A deadlock occurs when a set of agents holding resources are waiting on another set of resources such that a cycle of waiting agents is formed, implying that agents are unable to make progress. Figure Deadlock_Example . A deadlock example with two agents and two resources. Figure Deadlock_Example shows a deadlock example with two agents and two resources. In the example, agent 0 is waiting for resource B that is assigned to agent 1, and agent 1 is waiting for resource A that is assigned to agent 0. The dependency cycle created with agent 0 and 1 demonstrate that the agents are unable to make progress. Flit A flow control digit, or flit, is the smallest unit of resource allocation in a router. Variable length packets are divided into one or more fixed length flits to simplify the management and allocation of resources. Flits may be divided further into phits when traversing a router. Flow Control Flow control is the scheduling and allocation of a network\u2019s resources. For example, a virtual channel can have a wormhole flow control . Hotspot (Hot-spot) A hotspot resource is one whose demand is significantly greater than other, similar resources. For example, a particular destination terminal becomes a hotspot in a shared memory multicomputer when many processors are simultaneously reading from the same memory location (for example, a shared lock or data structure). Another example is traffic congestion within an area of the network. Livelock Livelock occurs when a packet is not able to make progress in the network and is never delivered to its destination. Unlike deadlock, a livelocked packet continues to move through the network. Figure Livelock_Example . A example with livelocked packet P1. Figure Livelock_Example shows a livelock example. In the example, there are two packets, P0 and P1. The destination router for the packets is router R2. R1 uses an adaptive routing algorithm. At timestep 1, P0 and P1 are at router R0. Given the routing algorithm at R0, R0 routes packet P0 to R2 and P1 to R1 (timestep 2). At timestep 3, another instance of P0 is routed to R0 and P1 is routed to R2. At timestep 4, P1 is routed to R0. After the arrival of P1 at R0. The state of R0 is identical to timestep1 where these sequence of events will repeat. In the end, P1 does not arrive to its destination although it makes progress, thus livelocked. Network-On-Chip (NoC) At a high level, a Network-On-Chip (NoC) is a network designed for one chip. It is composed of routers, channels and endpoint ports. It transports data between endpoints connected to it. Although the design is intended for a single chip, the logical description can be partitioned across multiple chips. Oblivious Routing An oblivious routing algorithm derives a route without using any information about the network\u2019s state, where, fundamentally, the route is computed using solely the source and the destination. Packet Packets are the unit of routing within an interconnection network. Messages are broken into one or more variable, but bounded, length packets for processing by the network. All data contained within a packet follow the same route through the network and packets are reassembled into messages at the destination node. A packet is divided further into flits. Phit A physical digit, or phit, is the smallest unit of data processed (e.g. traversing or accessed) by a router. One or more phits are combined to form a flit. Port A port is a physical gateway to a component (input port) or from a component (output port). Router A router receives packets on its inputs, determines the packets' destination based on the routing algorithm, and forwards the packets to the appropriate output. Routing Algorithm The series of steps for choosing a path for a packet through the network. For a packet, the routing algorithm determines the router's output from its input. Topology The static arrangement of router nodes, channels, and endpoint ports in a network. The topology affects the routing in the network. Virtual Channel A virtual channel (VC) is a logical representation of a channel at a router's input or output. It is composed of flit buffers within the router. In a router that handles virtual channels, a packet or flit is assigned to a virtual channel. Hence, the presence of virtual channels at a router's input or output enables the transfer of multiple packets through a single channel. Wormhole Flow Control Wormhole flow control defines the allocation of a resource at the flit granularity. Upon a successful allocation, the transfer of the flit is permitted to commence. References William James Dally and Brian Patrick Towles. 2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.","title":"Glossary"},{"location":"xls_noc_glossary/#xls-noc-glossary","text":"","title":"XLS-NoC Glossary"},{"location":"xls_noc_glossary/#adaptive-routing","text":"An adaptive routing algorithm derives a route using any information about the network\u2019s state.","title":"Adaptive Routing"},{"location":"xls_noc_glossary/#channel","text":"A channel interconnects a pair of nodes and is the medium that transfers data amongst the pair. A node is either a router input, a router output or an endpoint port. A channel may be unidirectional, transferring the data in a single direction, or bidirectional, transferring the data in both directions.","title":"Channel"},{"location":"xls_noc_glossary/#endpoint","text":"A node connected to the network that communicates to other endpoints using the network.","title":"Endpoint"},{"location":"xls_noc_glossary/#endpoint-port","text":"An endpoint port is a port that connects to an endpoint. There are two types of endpoint ports: a send port and a receive port. A send port enables an endpoint to send data to the network and a receive port enables an endpoint to receive data from the network. The naming convention for a send port and receive port is from the perspective of the endpoints.","title":"Endpoint Port"},{"location":"xls_noc_glossary/#deadlock","text":"A deadlock occurs when a set of agents holding resources are waiting on another set of resources such that a cycle of waiting agents is formed, implying that agents are unable to make progress. Figure Deadlock_Example . A deadlock example with two agents and two resources. Figure Deadlock_Example shows a deadlock example with two agents and two resources. In the example, agent 0 is waiting for resource B that is assigned to agent 1, and agent 1 is waiting for resource A that is assigned to agent 0. The dependency cycle created with agent 0 and 1 demonstrate that the agents are unable to make progress.","title":"Deadlock"},{"location":"xls_noc_glossary/#flit","text":"A flow control digit, or flit, is the smallest unit of resource allocation in a router. Variable length packets are divided into one or more fixed length flits to simplify the management and allocation of resources. Flits may be divided further into phits when traversing a router.","title":"Flit"},{"location":"xls_noc_glossary/#flow-control","text":"Flow control is the scheduling and allocation of a network\u2019s resources. For example, a virtual channel can have a wormhole flow control .","title":"Flow Control"},{"location":"xls_noc_glossary/#hotspot-hot-spot","text":"A hotspot resource is one whose demand is significantly greater than other, similar resources. For example, a particular destination terminal becomes a hotspot in a shared memory multicomputer when many processors are simultaneously reading from the same memory location (for example, a shared lock or data structure). Another example is traffic congestion within an area of the network.","title":"Hotspot (Hot-spot)"},{"location":"xls_noc_glossary/#livelock","text":"Livelock occurs when a packet is not able to make progress in the network and is never delivered to its destination. Unlike deadlock, a livelocked packet continues to move through the network. Figure Livelock_Example . A example with livelocked packet P1. Figure Livelock_Example shows a livelock example. In the example, there are two packets, P0 and P1. The destination router for the packets is router R2. R1 uses an adaptive routing algorithm. At timestep 1, P0 and P1 are at router R0. Given the routing algorithm at R0, R0 routes packet P0 to R2 and P1 to R1 (timestep 2). At timestep 3, another instance of P0 is routed to R0 and P1 is routed to R2. At timestep 4, P1 is routed to R0. After the arrival of P1 at R0. The state of R0 is identical to timestep1 where these sequence of events will repeat. In the end, P1 does not arrive to its destination although it makes progress, thus livelocked.","title":"Livelock"},{"location":"xls_noc_glossary/#network-on-chip-noc","text":"At a high level, a Network-On-Chip (NoC) is a network designed for one chip. It is composed of routers, channels and endpoint ports. It transports data between endpoints connected to it. Although the design is intended for a single chip, the logical description can be partitioned across multiple chips.","title":"Network-On-Chip (NoC)"},{"location":"xls_noc_glossary/#oblivious-routing","text":"An oblivious routing algorithm derives a route without using any information about the network\u2019s state, where, fundamentally, the route is computed using solely the source and the destination.","title":"Oblivious Routing"},{"location":"xls_noc_glossary/#packet","text":"Packets are the unit of routing within an interconnection network. Messages are broken into one or more variable, but bounded, length packets for processing by the network. All data contained within a packet follow the same route through the network and packets are reassembled into messages at the destination node. A packet is divided further into flits.","title":"Packet"},{"location":"xls_noc_glossary/#phit","text":"A physical digit, or phit, is the smallest unit of data processed (e.g. traversing or accessed) by a router. One or more phits are combined to form a flit.","title":"Phit"},{"location":"xls_noc_glossary/#port","text":"A port is a physical gateway to a component (input port) or from a component (output port).","title":"Port"},{"location":"xls_noc_glossary/#router","text":"A router receives packets on its inputs, determines the packets' destination based on the routing algorithm, and forwards the packets to the appropriate output.","title":"Router"},{"location":"xls_noc_glossary/#routing-algorithm","text":"The series of steps for choosing a path for a packet through the network. For a packet, the routing algorithm determines the router's output from its input.","title":"Routing Algorithm"},{"location":"xls_noc_glossary/#topology","text":"The static arrangement of router nodes, channels, and endpoint ports in a network. The topology affects the routing in the network.","title":"Topology"},{"location":"xls_noc_glossary/#virtual-channel","text":"A virtual channel (VC) is a logical representation of a channel at a router's input or output. It is composed of flit buffers within the router. In a router that handles virtual channels, a packet or flit is assigned to a virtual channel. Hence, the presence of virtual channels at a router's input or output enables the transfer of multiple packets through a single channel.","title":"Virtual Channel"},{"location":"xls_noc_glossary/#wormhole-flow-control","text":"Wormhole flow control defines the allocation of a resource at the flit granularity. Upon a successful allocation, the transfer of the flit is permitted to commence.","title":"Wormhole Flow Control"},{"location":"xls_noc_glossary/#references","text":"William James Dally and Brian Patrick Towles. 2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.","title":"References"},{"location":"xls_noc_readme/","text":"XLS-NoC Project The XLS-NoC subproject is a project that leverages XLS's facilities for optimizing parameterized hardware designs in the realm of NoC generation. A declarative specification drives the NoC generation process for optimization (e.g. of performance/area tradeoffs), traffic simulations and software-level functional verification, and the creation of (System)Verilog RTL artifacts such as routers and network-interfacing adapter components.","title":"Overview"},{"location":"xls_noc_readme/#xls-noc-project","text":"The XLS-NoC subproject is a project that leverages XLS's facilities for optimizing parameterized hardware designs in the realm of NoC generation. A declarative specification drives the NoC generation process for optimization (e.g. of performance/area tradeoffs), traffic simulations and software-level functional verification, and the creation of (System)Verilog RTL artifacts such as routers and network-interfacing adapter components.","title":"XLS-NoC Project"},{"location":"xls_noc_star_topology/","text":"Star Topology Types Overview The star topology is a topology with a central router node. Hierarchical Star Topology The hierarchical star topology is a hierarchical topology with a central router node that is only connected to router nodes, and the endpoint nodes are only connected to leaf routers. It is identical to a bidirectional tree topology with the exception that there are no endpoints connected to the root router. The channels are unidirectional or bidirectional. In practice, it is common that all channels of the network be bidirectional. Figure Hierarchical_Star_example . A hierarchical star topology example with eight endpoint nodes and five router nodes. Figure Hierarchical_Star_example shows a hierarchical star topology example with eight endpoint nodes and five router nodes. Router 8 is the central router that is only connected to router nodes 9, 10, 11 and 12. The endpoints [0, 1], [2, 3], [4, 5] and [6, 7] are connected to leaf routers 9, 10, 11 and 12 respectively. All the channels of the network are bidirectional. Star Topology The star topology is a topology with a single router: the central router node. All communication flows through the central router (all traffic traverses through the central router). The channels are bidirectional. Figure Star_example . A star topology example with four endpoint nodes and the central router. Figure Star_example shows a star topology example with four endpoint nodes and the central router. Single Router Topology The single router topology is the star topology with the channels being unidirectional. The single router topology has a single router: the central router node. All communication flows through the central router (all traffic traverses through the central router). Figure Single_Router_example . A single router topology example with four endpoint nodes and the central router. Figure Single_Router_example shows a single router topology example with four endpoint nodes and the central router. Compared to Figure Star_example, the channels are unidirectional. Cheat Sheet The hierarchical star topology is a hierarchical topology with a central router node that is only connected to router nodes, and the endpoint nodes are only connected to leaf routers. The star topology is a topology with a single router: the central router node. All traffic traverses through the central router. The channels are bidirectional. The single router topology is a topology with a single router: the central router node. All traffic traverses through the central router. The channels are unidirectional.","title":"Star"},{"location":"xls_noc_star_topology/#star-topology-types","text":"","title":"Star Topology Types"},{"location":"xls_noc_star_topology/#overview","text":"The star topology is a topology with a central router node.","title":"Overview"},{"location":"xls_noc_star_topology/#hierarchical-star-topology","text":"The hierarchical star topology is a hierarchical topology with a central router node that is only connected to router nodes, and the endpoint nodes are only connected to leaf routers. It is identical to a bidirectional tree topology with the exception that there are no endpoints connected to the root router. The channels are unidirectional or bidirectional. In practice, it is common that all channels of the network be bidirectional. Figure Hierarchical_Star_example . A hierarchical star topology example with eight endpoint nodes and five router nodes. Figure Hierarchical_Star_example shows a hierarchical star topology example with eight endpoint nodes and five router nodes. Router 8 is the central router that is only connected to router nodes 9, 10, 11 and 12. The endpoints [0, 1], [2, 3], [4, 5] and [6, 7] are connected to leaf routers 9, 10, 11 and 12 respectively. All the channels of the network are bidirectional.","title":"Hierarchical Star Topology "},{"location":"xls_noc_star_topology/#star-topology","text":"The star topology is a topology with a single router: the central router node. All communication flows through the central router (all traffic traverses through the central router). The channels are bidirectional. Figure Star_example . A star topology example with four endpoint nodes and the central router. Figure Star_example shows a star topology example with four endpoint nodes and the central router.","title":"Star Topology "},{"location":"xls_noc_star_topology/#single-router-topology","text":"The single router topology is the star topology with the channels being unidirectional. The single router topology has a single router: the central router node. All communication flows through the central router (all traffic traverses through the central router). Figure Single_Router_example . A single router topology example with four endpoint nodes and the central router. Figure Single_Router_example shows a single router topology example with four endpoint nodes and the central router. Compared to Figure Star_example, the channels are unidirectional.","title":"Single Router Topology "},{"location":"xls_noc_star_topology/#cheat-sheet","text":"The hierarchical star topology is a hierarchical topology with a central router node that is only connected to router nodes, and the endpoint nodes are only connected to leaf routers. The star topology is a topology with a single router: the central router node. All traffic traverses through the central router. The channels are bidirectional. The single router topology is a topology with a single router: the central router node. All traffic traverses through the central router. The channels are unidirectional.","title":"Cheat Sheet"},{"location":"xls_noc_topologies/","text":"XLS-NoC Topologies The network topology is the static arrangement of router nodes, channels, and endpoints in a network. The document will describe the logical representation of common topology types and define the communication flow within the topology. At the physical level, the arrangement of the topology influences the floorplan, layout and packaging. The supported set of topologies types are: Dimension Order Tree k-ary n-fly Butterfly Fully Connected Star","title":"Overview"},{"location":"xls_noc_topologies/#xls-noc-topologies","text":"The network topology is the static arrangement of router nodes, channels, and endpoints in a network. The document will describe the logical representation of common topology types and define the communication flow within the topology. At the physical level, the arrangement of the topology influences the floorplan, layout and packaging. The supported set of topologies types are: Dimension Order Tree k-ary n-fly Butterfly Fully Connected Star","title":"XLS-NoC Topologies"},{"location":"xls_noc_tree_topology/","text":"Tree Topology Types Overview The tree topology is a hierarchical topology with a root node and children nodes. The radix of a node defines the number of children nodes for the given node. The nodes represent the router nodes and endpoint nodes of a network. A tree topology has endpoints connected to the root router and leaf routers. It is called a tree topology because the router nodes form a tree data structure. Figure Tree_Topology_Example . A tree topology example. Figure Tree_Topology_Example shows an example of a tree topology. In the figure, there are nine nodes: three router nodes and six endpoint nodes. Router 1 is a root router, and routers 2 and 5 are leaf routers. Router 1 has a radix of four, and routers 2 and 5 have a radix of two. Routers 2 and 5 are child routers of router 1. Endpoints 3 and 4 are child endpoints of router 1. Endpoints 6 and 7, and endpoints 8 and 9 are child endpoints of router 2 and router 5 respectively. Unidirectional Types A unidirectional tree is a tree topology where the communication flow is unidirectional, thus all channels in the tree are unidirectional. There are two unidirectional tree types: aggregation tree and distribution tree. Aggregation Tree In the aggregation tree, the communication flow is from the leaf routers of the tree to the root router of the tree. The endpoints connected to the root router receive from the network, and the endpoints connected to the leaf routers send to the network. Figure Aggregation_Tree_Topology_Example . The aggregation tree topology representation of the tree in Figure Tree_Topology_Example . Figure Aggregation_Tree_Topology_Example shows the aggregation tree topology representation of the tree in Figure Tree_Topology_Example . Endpoints 6, 7, 8 and 9 send to the network, and endpoints 3 and 4 receive from the network. Distribution Tree In the distribution tree, the communication flow is from the root router of the tree to the leaf routers of the tree. The endpoints connected to the root router send to the network, and the endpoints connected to the leaf routers receive from the network. Figure Distribution_Tree_Topology_Example . The distribution tree topology representation of the tree in Figure Tree_Topology_Example . Figure Distribution_Tree_Topology_Example shows the distribution tree topology representation of the tree in Figure Tree_Topology_Example . Endpoints 3 and 4 send to the network, and endpoints 6, 7, 8 and 9 receive from the network. Bidirectional Type A bidirectional tree is a tree topology where the communication flow is bidirectional. The communication flows: from the root router of the tree to the leaf routers of the tree and from the leaf routers of the tree to the root router of the tree. By definition, a bidirectional tree requires: 1) at least one endpoint connected to the root router that sends to the network, 2) at least one endpoint connected to the root router that receives from the network, 3) at least one endpoint connected to the leaf routers that sends to the network, and 4) at least one endpoint connected to the leaf routers that receives from the network. In practice, it is common to have all endpoints connected to the root router and leaf routers send to and receive from the network. Figure Bidirectional_Tree_Topology_Example . The bidirectional tree topology representation of the tree in Figure Tree_Topology_Example . Figure Bidirectional_Tree_Topology_Example shows the bidirectional tree topology representation of the tree in Figure Tree_Topology_Example . All endpoints in the tree send to and receive from the network. Cheat Sheet A tree topology has endpoints connected to the root router and leaf routers. Unidirectional Trees In the aggregation tree , the communication flow is from the leaf routers of the tree to the root router of the tree. In the distribution tree , the communication flow is from the root router of the tree to the leaf routers of the tree. The communication flow of a bidirectional tree is: from the root router of the tree to the leaf routers of the tree and from the leaf routers of the tree to the root router of the tree.","title":"Tree"},{"location":"xls_noc_tree_topology/#tree-topology-types","text":"","title":"Tree Topology Types"},{"location":"xls_noc_tree_topology/#overview","text":"The tree topology is a hierarchical topology with a root node and children nodes. The radix of a node defines the number of children nodes for the given node. The nodes represent the router nodes and endpoint nodes of a network. A tree topology has endpoints connected to the root router and leaf routers. It is called a tree topology because the router nodes form a tree data structure. Figure Tree_Topology_Example . A tree topology example. Figure Tree_Topology_Example shows an example of a tree topology. In the figure, there are nine nodes: three router nodes and six endpoint nodes. Router 1 is a root router, and routers 2 and 5 are leaf routers. Router 1 has a radix of four, and routers 2 and 5 have a radix of two. Routers 2 and 5 are child routers of router 1. Endpoints 3 and 4 are child endpoints of router 1. Endpoints 6 and 7, and endpoints 8 and 9 are child endpoints of router 2 and router 5 respectively.","title":"Overview "},{"location":"xls_noc_tree_topology/#unidirectional-types","text":"A unidirectional tree is a tree topology where the communication flow is unidirectional, thus all channels in the tree are unidirectional. There are two unidirectional tree types: aggregation tree and distribution tree.","title":"Unidirectional Types "},{"location":"xls_noc_tree_topology/#aggregation-tree","text":"In the aggregation tree, the communication flow is from the leaf routers of the tree to the root router of the tree. The endpoints connected to the root router receive from the network, and the endpoints connected to the leaf routers send to the network. Figure Aggregation_Tree_Topology_Example . The aggregation tree topology representation of the tree in Figure Tree_Topology_Example . Figure Aggregation_Tree_Topology_Example shows the aggregation tree topology representation of the tree in Figure Tree_Topology_Example . Endpoints 6, 7, 8 and 9 send to the network, and endpoints 3 and 4 receive from the network.","title":"Aggregation Tree "},{"location":"xls_noc_tree_topology/#distribution-tree","text":"In the distribution tree, the communication flow is from the root router of the tree to the leaf routers of the tree. The endpoints connected to the root router send to the network, and the endpoints connected to the leaf routers receive from the network. Figure Distribution_Tree_Topology_Example . The distribution tree topology representation of the tree in Figure Tree_Topology_Example . Figure Distribution_Tree_Topology_Example shows the distribution tree topology representation of the tree in Figure Tree_Topology_Example . Endpoints 3 and 4 send to the network, and endpoints 6, 7, 8 and 9 receive from the network.","title":"Distribution Tree "},{"location":"xls_noc_tree_topology/#bidirectional-type","text":"A bidirectional tree is a tree topology where the communication flow is bidirectional. The communication flows: from the root router of the tree to the leaf routers of the tree and from the leaf routers of the tree to the root router of the tree. By definition, a bidirectional tree requires: 1) at least one endpoint connected to the root router that sends to the network, 2) at least one endpoint connected to the root router that receives from the network, 3) at least one endpoint connected to the leaf routers that sends to the network, and 4) at least one endpoint connected to the leaf routers that receives from the network. In practice, it is common to have all endpoints connected to the root router and leaf routers send to and receive from the network. Figure Bidirectional_Tree_Topology_Example . The bidirectional tree topology representation of the tree in Figure Tree_Topology_Example . Figure Bidirectional_Tree_Topology_Example shows the bidirectional tree topology representation of the tree in Figure Tree_Topology_Example . All endpoints in the tree send to and receive from the network.","title":"Bidirectional Type "},{"location":"xls_noc_tree_topology/#cheat-sheet","text":"A tree topology has endpoints connected to the root router and leaf routers. Unidirectional Trees In the aggregation tree , the communication flow is from the leaf routers of the tree to the root router of the tree. In the distribution tree , the communication flow is from the root router of the tree to the leaf routers of the tree. The communication flow of a bidirectional tree is: from the root router of the tree to the leaf routers of the tree and from the leaf routers of the tree to the root router of the tree.","title":"Cheat Sheet"},{"location":"xls_style/","text":"XLS Style Guide The Google style guides recommend enforcing local consistency where stylistic choices are not predefined. This file notes some of the choices we make locally in the XLS project, with the relevant Google style guides ( C++ , Python ) as their bases. C++ Align the pointer or reference modifier token with the type; e.g. Foo& foo = ... instead of Foo &foo = ... , and Foo* foo = ... instead of Foo *foo= ... . Use /*parameter_name=*/value style comments if you choose to annotate arguments in a function invocation. clang-tidy recognizes this form, and provides a Tricorder notification if parameter_name is mismatched against the parameter name of the callee. Prefer int64_t over int to avoid any possibility of overflow. Always use Status or StatusOr for any error that a user could encounter. Other than user-facing errors, use Status only in exceptional situations. For example, Status is good to signal that a required file does not exist but not for signaling that constant folding did not constant fold an expression. See how heavyweight is StatusOr for more details on thinking about the costs involved. Internal errors for conditions that should never be false can use CHECK , but may also use Status or StatusOr . Prefer using XLS_ASSIGN_OR_RETURN / XLS_RETURN_IF_ERROR when appropriate, but when binding a StatusOr wrapped value prefer to name it thing_or so that it can be referenced without the wrapper as thing ; e.g. absl::StatusOr<Thing> thing_or = f(); if (!thing_or.ok()) { // ... handling of the status via thing_or.status() and returning ... } const Thing& thing = thing_or.value(); Prefer CHECK to DCHECK , except that DCHECK can be used to verify conditions that it would be too expensive to verify in production, but that are fast enough to include outside of production. Follow the C++ style guide for capitalization guidelines; however, in the somewhat ambiguous case of I/O (short for Input/Output, which we use often), the slash counts as internal spacing and therefore the capitalization we use is IO , as in WrapIO or StreamingIOReader . Prefer to use the XLS_FRIEND_TEST macro vs friending manually-mangled test names. At times it can be useful to test unit test a private/protected member of a class, and the XLS_FRIEND_TEST macro makes this possible. Note that the test case must live outside an unnamed namespace in the test file for the \"friending\" to work properly. Functions Short or easily-explained argument lists (as defined by the developer) can be explained inline with the rest of the function comment. For more complex argument lists, the following pattern should be used: // <Function description> // Args: // arg1: <arg1 description> // arg2: <arg2 description> // ... IR nodes Unlike most data, IR elements should be passed as non-const pointers, even when expected to be const (which would usually indicate passing them as const references). Experience has shown that IR elements often develop non-const usages over time. Consider the case of IR analysis passes - those passes themselves rarely need to mutate their input data, but they build up data structures whose users often need to mutate their contents. In addition, treating elements as pointers makes equality comparisons more straightforward (avoid taking an address of a reference) and helps avoid accidental copies (assigning a reference to local, etc.). Non-const pointer usage propagates outwards such that the few cases where a const reference could actually be appropriate become odd outliers, so our guidance is that IR elements should uniformly be passed as non-const pointers. FAQ How heavyweight is StatusOr ? What follows is the general guidance on how absl::StatusOr is used -- it is used extensively throughout the XLS code base as an error-style indicator object wrapper, so it is important to understand the mental model used for its cost. Consider cost wise that: a) creating an ok StatusOr is cheap, b) creating a non-ok StatusOr is expensive (that is, imagine the non-ok Status within a StatusOr is the expensive part). The implication being: if there's an API where \"not found\" is a reasonable outcome, prefer absl::optional<> as a return value to indicate that / go with the grain of cost. Something like a filesystem API would be a classic example -- where you shouldn't be rooting around looking for files that aren't there -- so a not-found absl::StatusOr result would be fine to use. A good potential mental model is to imagine the program may run with logging of a traceback for every non-ok status that is created. (This is related to a debugging capability in Google internally called --util_status_save_stack_trace that captures backtraces when error Status es are created.) Ideally, with such a logging flag turned on, the screen wouldn't fill up with \"non error tracebacks\", only tracebacks from events where something really went wrong.","title":"Style Guide"},{"location":"xls_style/#xls-style-guide","text":"The Google style guides recommend enforcing local consistency where stylistic choices are not predefined. This file notes some of the choices we make locally in the XLS project, with the relevant Google style guides ( C++ , Python ) as their bases.","title":"XLS Style Guide"},{"location":"xls_style/#c","text":"Align the pointer or reference modifier token with the type; e.g. Foo& foo = ... instead of Foo &foo = ... , and Foo* foo = ... instead of Foo *foo= ... . Use /*parameter_name=*/value style comments if you choose to annotate arguments in a function invocation. clang-tidy recognizes this form, and provides a Tricorder notification if parameter_name is mismatched against the parameter name of the callee. Prefer int64_t over int to avoid any possibility of overflow. Always use Status or StatusOr for any error that a user could encounter. Other than user-facing errors, use Status only in exceptional situations. For example, Status is good to signal that a required file does not exist but not for signaling that constant folding did not constant fold an expression. See how heavyweight is StatusOr for more details on thinking about the costs involved. Internal errors for conditions that should never be false can use CHECK , but may also use Status or StatusOr . Prefer using XLS_ASSIGN_OR_RETURN / XLS_RETURN_IF_ERROR when appropriate, but when binding a StatusOr wrapped value prefer to name it thing_or so that it can be referenced without the wrapper as thing ; e.g. absl::StatusOr<Thing> thing_or = f(); if (!thing_or.ok()) { // ... handling of the status via thing_or.status() and returning ... } const Thing& thing = thing_or.value(); Prefer CHECK to DCHECK , except that DCHECK can be used to verify conditions that it would be too expensive to verify in production, but that are fast enough to include outside of production. Follow the C++ style guide for capitalization guidelines; however, in the somewhat ambiguous case of I/O (short for Input/Output, which we use often), the slash counts as internal spacing and therefore the capitalization we use is IO , as in WrapIO or StreamingIOReader . Prefer to use the XLS_FRIEND_TEST macro vs friending manually-mangled test names. At times it can be useful to test unit test a private/protected member of a class, and the XLS_FRIEND_TEST macro makes this possible. Note that the test case must live outside an unnamed namespace in the test file for the \"friending\" to work properly.","title":"C++"},{"location":"xls_style/#functions","text":"Short or easily-explained argument lists (as defined by the developer) can be explained inline with the rest of the function comment. For more complex argument lists, the following pattern should be used: // <Function description> // Args: // arg1: <arg1 description> // arg2: <arg2 description> // ...","title":"Functions"},{"location":"xls_style/#ir-nodes","text":"Unlike most data, IR elements should be passed as non-const pointers, even when expected to be const (which would usually indicate passing them as const references). Experience has shown that IR elements often develop non-const usages over time. Consider the case of IR analysis passes - those passes themselves rarely need to mutate their input data, but they build up data structures whose users often need to mutate their contents. In addition, treating elements as pointers makes equality comparisons more straightforward (avoid taking an address of a reference) and helps avoid accidental copies (assigning a reference to local, etc.). Non-const pointer usage propagates outwards such that the few cases where a const reference could actually be appropriate become odd outliers, so our guidance is that IR elements should uniformly be passed as non-const pointers.","title":"IR nodes"},{"location":"xls_style/#faq","text":"","title":"FAQ"},{"location":"xls_style/#how-heavyweight-is-statusor","text":"What follows is the general guidance on how absl::StatusOr is used -- it is used extensively throughout the XLS code base as an error-style indicator object wrapper, so it is important to understand the mental model used for its cost. Consider cost wise that: a) creating an ok StatusOr is cheap, b) creating a non-ok StatusOr is expensive (that is, imagine the non-ok Status within a StatusOr is the expensive part). The implication being: if there's an API where \"not found\" is a reasonable outcome, prefer absl::optional<> as a return value to indicate that / go with the grain of cost. Something like a filesystem API would be a classic example -- where you shouldn't be rooting around looking for files that aren't there -- so a not-found absl::StatusOr result would be fine to use. A good potential mental model is to imagine the program may run with logging of a traceback for every non-ok status that is created. (This is related to a debugging capability in Google internally called --util_status_save_stack_trace that captures backtraces when error Status es are created.) Ideally, with such a logging flag turned on, the screen wouldn't fill up with \"non error tracebacks\", only tracebacks from events where something really went wrong.","title":"How heavyweight is StatusOr?"},{"location":"tutorials/","text":"XLS Tutorials The XLS team has written several tutorials to help explain language features and design techniques. Here they are, grouped by topic: DSLX Hello, XLS! : A brief introduction to writing and evaluating your first design in DSLX. Float-to-int conversion : A guide to writing \"real\" logic in DSLX, demonstrated by creating an IEEE-754 binary32, i.e., C float to int32_t converter. Intro to parametrics : A demonstration on how functions and types can be parameterized to allow a single implementation to apply to many different data layouts. for expressions : Explains how to understand and write looping constructs in DSLX. enumerate and match expressions : Explains how to use enumerate() expressions to control loop iteration and how to use the match pattern-matching expression for selecting between alternatives. Intro to procs (communicating sequential processes) : Provides a basic introduction to writing stateful and communicating modules, i.e., procs.","title":"Overview"},{"location":"tutorials/#xls-tutorials","text":"The XLS team has written several tutorials to help explain language features and design techniques. Here they are, grouped by topic:","title":"XLS Tutorials"},{"location":"tutorials/#dslx","text":"Hello, XLS! : A brief introduction to writing and evaluating your first design in DSLX. Float-to-int conversion : A guide to writing \"real\" logic in DSLX, demonstrated by creating an IEEE-754 binary32, i.e., C float to int32_t converter. Intro to parametrics : A demonstration on how functions and types can be parameterized to allow a single implementation to apply to many different data layouts. for expressions : Explains how to understand and write looping constructs in DSLX. enumerate and match expressions : Explains how to use enumerate() expressions to control loop iteration and how to use the match pattern-matching expression for selecting between alternatives. Intro to procs (communicating sequential processes) : Provides a basic introduction to writing stateful and communicating modules, i.e., procs.","title":"DSLX"},{"location":"tutorials/crc32/","text":"Tutorial: for expressions In this document we explain in detail the implementation of routine to compute a CRC32 checksum on a single 8-bit input. We don't discuss the algorithm here, only the language features necessary to implement the algorithm. Refer to the full implementation while following this document. Function Prototype The signature and first line of this function should look familiar enough now, but the for construct is new: DSLX provides a means of iterating a fixed number of times within a function. A DSLX for loop has the following structure: The loop signature: this consists of three elements: 1. An (index, ) tuple. The index holds the current iteration number, and the accumulator vars are user-specified data carried into the current iteration. 2. The type specification for the index/accumulators tuple. Note that the index type can be controlled by the user (i.e., doesn't have to be u32, but it should be able to hold all possible loop index values). 3. An iterable , either the range() or enumerate() expressions, either of which dictates the number of iterations of the loop to complete. The loop body: this has the same general form as a DSLX function. Particularly noteworthy is that the loop body ends by stating the \"return\" value. In a for loop, this \"return\" value is either used as the input to the next iteration of the loop (for non-terminal iterations) or as the result of the entire expression (for the terminal iteration). For this specific for loop, the index variable and accumulator are i and crc , both of type u32 . The iterable range expression specifies that the loop should execute 8 times. // 8 rounds of updates. for ( i , crc ) : ( u32 , u32 ) in range ( u32 : 8 ) { At the end of the loop, the calculated value is being assigned to the accumulator crc - the last expression in the loop body is assigned to the accumulator: let mask : u32 = - ( crc & u32 : 1 ); ( crc >> u32 : 1 ) ^ ( polynomial & mask ) Finally, the accumulator's initial value is being passed to the for expression as a parameter. This can be confusing, especially when compared to other languages, where the init value typically is provided at or near the top of a loop. }( crc ) Since the for loop is the last expression in the function, it's also the function's return value, but in other contexts, it could be assigned to a variable and used elsewhere. In general, the result of a for expression can be used in the same manner as any other expression's result.","title":"For expressions"},{"location":"tutorials/crc32/#tutorial-for-expressions","text":"In this document we explain in detail the implementation of routine to compute a CRC32 checksum on a single 8-bit input. We don't discuss the algorithm here, only the language features necessary to implement the algorithm. Refer to the full implementation while following this document.","title":"Tutorial: for expressions"},{"location":"tutorials/crc32/#function-prototype","text":"The signature and first line of this function should look familiar enough now, but the for construct is new: DSLX provides a means of iterating a fixed number of times within a function. A DSLX for loop has the following structure: The loop signature: this consists of three elements: 1. An (index, ) tuple. The index holds the current iteration number, and the accumulator vars are user-specified data carried into the current iteration. 2. The type specification for the index/accumulators tuple. Note that the index type can be controlled by the user (i.e., doesn't have to be u32, but it should be able to hold all possible loop index values). 3. An iterable , either the range() or enumerate() expressions, either of which dictates the number of iterations of the loop to complete. The loop body: this has the same general form as a DSLX function. Particularly noteworthy is that the loop body ends by stating the \"return\" value. In a for loop, this \"return\" value is either used as the input to the next iteration of the loop (for non-terminal iterations) or as the result of the entire expression (for the terminal iteration). For this specific for loop, the index variable and accumulator are i and crc , both of type u32 . The iterable range expression specifies that the loop should execute 8 times. // 8 rounds of updates. for ( i , crc ) : ( u32 , u32 ) in range ( u32 : 8 ) { At the end of the loop, the calculated value is being assigned to the accumulator crc - the last expression in the loop body is assigned to the accumulator: let mask : u32 = - ( crc & u32 : 1 ); ( crc >> u32 : 1 ) ^ ( polynomial & mask ) Finally, the accumulator's initial value is being passed to the for expression as a parameter. This can be confusing, especially when compared to other languages, where the init value typically is provided at or near the top of a loop. }( crc ) Since the for loop is the last expression in the function, it's also the function's return value, but in other contexts, it could be assigned to a variable and used elsewhere. In general, the result of a for expression can be used in the same manner as any other expression's result.","title":"Function Prototype"},{"location":"tutorials/float_to_int/","text":"Tutorial: basic logic This tutorial demonstrates how to use XLS to create a simple combinational module, in this case one that performs floating-point-to-integer conversion. The first task is to define the full semantics of the module. We wish for the module to accept a IEEE-754 floating-point number and to output the integer representing the same. All fractional elements will be discarded. Overflow, NaN, and infinite values will be clamped to the maximum or minimum representable integer value with the sign of the input number. Tutorial: basic logic 1. Bootstrapping 2. Simple logic 3. Conclusion 1. Bootstrapping When creating a module, one first needs to define the signature and skeleton of the entry function. If you recall, a IEEE binary32 (the C float type) has 1 sign bit, 8 [biased] exponent bits, and 23 fractional bits. These values can be packed into a tuple, and so, the signature of our function can be defined as: pub fn float_to_int ( x : ( u1 , u8 , u23 )) -> s32 { s32 : 0xbeef } DSLX syntax is intended to follow Rust syntax as much as possible, so this may look familiar if you're a Rustacean. In any case, let's walk through this code, line-by-line: This line declares a public function ( pub fn ), named float_to_int . Since this function is \"public\", it can be referenced from other modules (i.e., files) if imported therein. Function parameter declarations! This function only takes one parameter, x , whose type follows its name. In this case, it's a tuple : a grouping of potentially disparate elements into a single quantity. A tuple is specified by listing a set of types in parentheses, as here. Our tuple has a 1-bit element for the sign, an 8-bit element for the biased exponent, and a 23-bit element for the fractional part. In what is the complete opposite of a coincidence, these fields match those of an IEEE float32 number. If a function takes more than one argument, they'll be comma-separated. - u1, u8, and u23 are all shortcuts for the type uN[1], uN[8], and uN[23]. The uN[X] construct declares an X-bit wide unsigned type. There is also sN[X], which declares an X-bit wide signed type. - Other type shortcuts exist such as bits X , bool (alias for uN[1]), and u[1-64] and s[1-64], being aliases for uN[1] through uN[64] and sN[1] through sN[64]. Function return type. This function returns a signed 32-bit type, matching the intentions of float-to-int conversion (since floats are signed). Finally, the last line: the final statement in a function is its return value. Here, we're unconditionally returning a signed 32-bit number with the value 48879 . This is only temporary to make the function syntactically valid - we're still learning the basics! Gimme a second! As an aside, it's a good idea to keep a bookmark to the DSLX language reference handy. It has the full details on language features and syntax and even we XLS devs frequently reference it. Anyway...the tuple representation of our input is a bit cumbersome, so let's define our floating-point number as a struct instead: pub struct float32 { sign : u1 , bexp : u8 , fraction : u23 , } pub fn float_to_int ( x : float32 ) -> s32 { s32 : 0xbeef } Finally, let's write a quick test to make sure things work. Add the following code to your file. #[test] fn float_to_int_test () { // 0xbeef in float32. let test_input = float32 { sign : u1 : 0x0 , bexp : u8 : 0x8e , fraction : u23 : 0x3eef00 }; assert_eq ( s32 : 0xbeef , float_to_int ( test_input )) } Now run the test through the DSLX interpreter: $ ./bazel-bin/xls/dslx/interpreter_main float_to_int.x You should see something like the following: [ RUN UNITTEST ] float_to_int_test [ OK ] [===============] 1 test(s) ran; 0 failed; 0 skipped. If so, then congrats! You've written - and tested - your first DSLX module! Next up: let's make it do something more interesting. 2. Simple logic After getting the trivial module up and running, the next step is to add real logic to the implementation. Recall that a floating-point number's fractional part has an implicit leading 1 , so a floating-point number is representable as an integer if its exponent is < 30, that is to say, if its value is between [-2^31, 2^31). To get that exponent, we need to unbias it. In its binary representation, a valid floating-point number's exponent is a value from 0 to 254, being an 8-bit value (an exponent of 255 indicates either NaN or an infinity). The range of a floating-point number's exponent is from -128 to 127, though, so we need to subtract 127 from that value to get the actual exponent. Let's write a function to do just that: fn unbias_exponent ( exp : u8 ) -> s9 { exp as s9 - s9 : 127 } Notice that we need to expand the exponent to add on the sign bit before the subtraction! Note : The repeated s9 type specifications are a bit redundant. They're needed because we've not yet fully built out DSLX' type inference capabilities, but this is an area targeted for improvement. Now that we can get the proper exponent, we can code up the rest of the simple in-bound cases. To do that, we need to prepend that leading 1 and shift the fractional part into its proper location in the final integer. Here's what that looks like when we add that to our original function: pub struct float32 { sign : u1 , bexp : u8 , fraction : u23 , } fn unbias_exponent ( exp : u8 ) -> s9 { exp as s9 - s9 : 127 } pub fn float_to_int ( x : float32 ) -> s32 { let exp = unbias_exponent ( x . bexp ); // Add the implicit leading one. // Note that we need to add one bit to the fraction to hold it. let fraction = u33 : 1 << 23 | ( x . fraction as u33 ); // Shift the result to the right if the exponent is less than 23. let fraction = if ( exp as u8 ) < u8 : 23 { fraction >> ( u8 : 23 - ( exp as u8 )) } else { fraction }; // Shift the result to the left if the exponent is greater than 23. let fraction = if ( exp as u8 ) > u8 : 23 { fraction << (( exp as u8 ) - u8 : 23 ) } else { fraction }; let result = fraction as s32 ; let result = if x . sign { - result } else { result }; result } If we run this function with our original test case, it still works! Of course, one should run additional test cases to see what happens with other inputs, particularly because this implementation will fail for some important values. Try adding tests on your own to find these cases - and to fix them! If you're stumped, hints (and answers) are hidden below: Missing case 1 What if the input is 0.0? What should the result be? To fix this, add a specific check for a zero exponent and fractional part. Missing case 2 Are NaNs or infinite numbers handled correctly? To fix, add a special check for NaN or infinities at function end. Consider making `is_inf` and `is_nan` functions! 3. Conclusion I hope that these examples help you get a better grasp on DSLX and writing modules in them. While our float-to-int function correctly handles float -to- int32_t conversions, what if we wanted to convert double to int64_t ? Or even float to int64_t ? Even worse , will we have to write separate floating-point operators for every floating-point type we (or our users) wish to support? Fortunately, the answer is no! The next tutorial covers type parameterization, and demonstrates how we can write a single int-to-float routine that covers all our possible conversions. See you there!","title":"Basic logic"},{"location":"tutorials/float_to_int/#tutorial-basic-logic","text":"This tutorial demonstrates how to use XLS to create a simple combinational module, in this case one that performs floating-point-to-integer conversion. The first task is to define the full semantics of the module. We wish for the module to accept a IEEE-754 floating-point number and to output the integer representing the same. All fractional elements will be discarded. Overflow, NaN, and infinite values will be clamped to the maximum or minimum representable integer value with the sign of the input number. Tutorial: basic logic 1. Bootstrapping 2. Simple logic 3. Conclusion","title":"Tutorial: basic logic"},{"location":"tutorials/float_to_int/#1-bootstrapping","text":"When creating a module, one first needs to define the signature and skeleton of the entry function. If you recall, a IEEE binary32 (the C float type) has 1 sign bit, 8 [biased] exponent bits, and 23 fractional bits. These values can be packed into a tuple, and so, the signature of our function can be defined as: pub fn float_to_int ( x : ( u1 , u8 , u23 )) -> s32 { s32 : 0xbeef } DSLX syntax is intended to follow Rust syntax as much as possible, so this may look familiar if you're a Rustacean. In any case, let's walk through this code, line-by-line: This line declares a public function ( pub fn ), named float_to_int . Since this function is \"public\", it can be referenced from other modules (i.e., files) if imported therein. Function parameter declarations! This function only takes one parameter, x , whose type follows its name. In this case, it's a tuple : a grouping of potentially disparate elements into a single quantity. A tuple is specified by listing a set of types in parentheses, as here. Our tuple has a 1-bit element for the sign, an 8-bit element for the biased exponent, and a 23-bit element for the fractional part. In what is the complete opposite of a coincidence, these fields match those of an IEEE float32 number. If a function takes more than one argument, they'll be comma-separated. - u1, u8, and u23 are all shortcuts for the type uN[1], uN[8], and uN[23]. The uN[X] construct declares an X-bit wide unsigned type. There is also sN[X], which declares an X-bit wide signed type. - Other type shortcuts exist such as bits X , bool (alias for uN[1]), and u[1-64] and s[1-64], being aliases for uN[1] through uN[64] and sN[1] through sN[64]. Function return type. This function returns a signed 32-bit type, matching the intentions of float-to-int conversion (since floats are signed). Finally, the last line: the final statement in a function is its return value. Here, we're unconditionally returning a signed 32-bit number with the value 48879 . This is only temporary to make the function syntactically valid - we're still learning the basics! Gimme a second! As an aside, it's a good idea to keep a bookmark to the DSLX language reference handy. It has the full details on language features and syntax and even we XLS devs frequently reference it. Anyway...the tuple representation of our input is a bit cumbersome, so let's define our floating-point number as a struct instead: pub struct float32 { sign : u1 , bexp : u8 , fraction : u23 , } pub fn float_to_int ( x : float32 ) -> s32 { s32 : 0xbeef } Finally, let's write a quick test to make sure things work. Add the following code to your file. #[test] fn float_to_int_test () { // 0xbeef in float32. let test_input = float32 { sign : u1 : 0x0 , bexp : u8 : 0x8e , fraction : u23 : 0x3eef00 }; assert_eq ( s32 : 0xbeef , float_to_int ( test_input )) } Now run the test through the DSLX interpreter: $ ./bazel-bin/xls/dslx/interpreter_main float_to_int.x You should see something like the following: [ RUN UNITTEST ] float_to_int_test [ OK ] [===============] 1 test(s) ran; 0 failed; 0 skipped. If so, then congrats! You've written - and tested - your first DSLX module! Next up: let's make it do something more interesting.","title":"1. Bootstrapping"},{"location":"tutorials/float_to_int/#2-simple-logic","text":"After getting the trivial module up and running, the next step is to add real logic to the implementation. Recall that a floating-point number's fractional part has an implicit leading 1 , so a floating-point number is representable as an integer if its exponent is < 30, that is to say, if its value is between [-2^31, 2^31). To get that exponent, we need to unbias it. In its binary representation, a valid floating-point number's exponent is a value from 0 to 254, being an 8-bit value (an exponent of 255 indicates either NaN or an infinity). The range of a floating-point number's exponent is from -128 to 127, though, so we need to subtract 127 from that value to get the actual exponent. Let's write a function to do just that: fn unbias_exponent ( exp : u8 ) -> s9 { exp as s9 - s9 : 127 } Notice that we need to expand the exponent to add on the sign bit before the subtraction! Note : The repeated s9 type specifications are a bit redundant. They're needed because we've not yet fully built out DSLX' type inference capabilities, but this is an area targeted for improvement. Now that we can get the proper exponent, we can code up the rest of the simple in-bound cases. To do that, we need to prepend that leading 1 and shift the fractional part into its proper location in the final integer. Here's what that looks like when we add that to our original function: pub struct float32 { sign : u1 , bexp : u8 , fraction : u23 , } fn unbias_exponent ( exp : u8 ) -> s9 { exp as s9 - s9 : 127 } pub fn float_to_int ( x : float32 ) -> s32 { let exp = unbias_exponent ( x . bexp ); // Add the implicit leading one. // Note that we need to add one bit to the fraction to hold it. let fraction = u33 : 1 << 23 | ( x . fraction as u33 ); // Shift the result to the right if the exponent is less than 23. let fraction = if ( exp as u8 ) < u8 : 23 { fraction >> ( u8 : 23 - ( exp as u8 )) } else { fraction }; // Shift the result to the left if the exponent is greater than 23. let fraction = if ( exp as u8 ) > u8 : 23 { fraction << (( exp as u8 ) - u8 : 23 ) } else { fraction }; let result = fraction as s32 ; let result = if x . sign { - result } else { result }; result } If we run this function with our original test case, it still works! Of course, one should run additional test cases to see what happens with other inputs, particularly because this implementation will fail for some important values. Try adding tests on your own to find these cases - and to fix them! If you're stumped, hints (and answers) are hidden below: Missing case 1 What if the input is 0.0? What should the result be? To fix this, add a specific check for a zero exponent and fractional part. Missing case 2 Are NaNs or infinite numbers handled correctly? To fix, add a special check for NaN or infinities at function end. Consider making `is_inf` and `is_nan` functions!","title":"2. Simple logic"},{"location":"tutorials/float_to_int/#3-conclusion","text":"I hope that these examples help you get a better grasp on DSLX and writing modules in them. While our float-to-int function correctly handles float -to- int32_t conversions, what if we wanted to convert double to int64_t ? Or even float to int64_t ? Even worse , will we have to write separate floating-point operators for every floating-point type we (or our users) wish to support? Fortunately, the answer is no! The next tutorial covers type parameterization, and demonstrates how we can write a single int-to-float routine that covers all our possible conversions. See you there!","title":"3. Conclusion"},{"location":"tutorials/hello_xls/","text":"Tutorial: Hello, XLS! So, you're interested in learning more about XLS and DSLX! Super! This tutorial is aimed at the very basics of getting started with XLS: getting your execution environment set up and running the most trivial of DSLX examples: printing the standard \"Hello, world!\" message to the terminal. Yes, even though XLS is a hardware synthesis language, it still needs basic printing and string support, if only for debugging! 1. Installation and building First things first: if you haven't yet done so, download the XLS sources from Github: git clone https://github.com/google/xls.git xls Next, build the project tree. XLS includes several dependencies that can take a while to build, so the first build may take a while; subsequent builds will be much shorter. NOTE : If you don't have Bazel installed, install it: check the Bazel website for instructions. The other prerequisites are a C++20-compliant compiler toolchain and a Python3 interpreter; check with your distribution for installation instructions for both. Start the XLS build by running: bazel build -c opt xls/... Then go get a cup of coffee. LLVM and Z3 are big projects, and will take a while to compile (but only the first time). Binary releases are coming soon: they'll avoid the need for long local compiles. 2. Create your module With your toolchain built, let's get to coding! Open up an editor and create a file called hello_xls.x in your XLS checkout root directory. Populate it with the following: fn hello_xls ( hello_string : u8 [ 11 ]) { let _ = trace ! ( hello_string ); () } Let's go over this, line-by-line: This first line declares a fn ( fn ) named \" hello_xls \". This function accepts an array of eleven characters (u8) called hello_string , and returns no value (the return type would be specified after the argument list's closing parenthesis and before the function-opening curly brace, if the function returned a value). This second line invokes the built-in trace! directive, passing it the function's input string, and throws away the result (assignment to _ ). The final line terminates the function, exiting with the empty return value () , representing an empty tuple. 3. Say hello, XLS! Let's run (and test) our code! First thing, though, we should make sure our module parses and passes type checking. The fastest way to do that is via the DSLX \" repl \", conveniently called repl . You can run it against the above example with the command: $ ./bazel-bin/xls/tools/repl hello_xls.x This tool first examines the specified module for language correctness, and will print an INVALID_ARGUMENT error if it fails to parse or typecheck. In that case, fix the errors and type :reload to try again. repl supports other features (IR, Verilog, and LLVM code examination), but those are outside the scope of this tutorial. Once you have a parsing DSLX file, the best way to \"smoke test\" a module is via the DSLX interpreter . First, though, we need a test case for it to execute. Add the following to the end of your hello_xls.x file: #[test] fn hello_test () { hello_xls ( \"Hello, XLS!\" ) } Again, going line-by-line: This directive tells the interpreter that the next function is a test function, meaning that it shouldn't be passed down the synthesis chain and that it should be executed by the interpreter. This line declares the [test] function hello_test , which takes no args and returns no value. The only line in this function invokes the hello_xls function and passes it a chipper greeting. With both the function and its corresponding test/driver in place, let's fire it up! Open a terminal and execute the following in the XLS checkout root directory: $ ./bazel-bin/xls/dslx/interpreter_main hello_xls.x You should see the following output: [ RUN UNITTEST ] hello_test trace of hello_string @ hello.x:4:17-4:31: [72, 101, 108, 108, 111, 44, 32, 88, 76, 83, 33] [ OK ] [===============] 1 test(s) ran; 0 failed; 0 skipped. Perfect! While this may not be what you initially expected, examine the output elements carefully: they correspond to the ASCII codes of the characters in \"Hello, XLS!\" When designing and debugging hardware, signals are more often numbers than strings, which is why they're represented as numbers here. Congrats! You've written your first piece of hardware in DSLX! It might be more satisfying, though, if your hardware actually did anything . For that, see the next tutorial, float-to-int conversion .","title":"Hello, XLS!"},{"location":"tutorials/hello_xls/#tutorial-hello-xls","text":"So, you're interested in learning more about XLS and DSLX! Super! This tutorial is aimed at the very basics of getting started with XLS: getting your execution environment set up and running the most trivial of DSLX examples: printing the standard \"Hello, world!\" message to the terminal. Yes, even though XLS is a hardware synthesis language, it still needs basic printing and string support, if only for debugging!","title":"Tutorial: Hello, XLS!"},{"location":"tutorials/hello_xls/#1-installation-and-building","text":"First things first: if you haven't yet done so, download the XLS sources from Github: git clone https://github.com/google/xls.git xls Next, build the project tree. XLS includes several dependencies that can take a while to build, so the first build may take a while; subsequent builds will be much shorter. NOTE : If you don't have Bazel installed, install it: check the Bazel website for instructions. The other prerequisites are a C++20-compliant compiler toolchain and a Python3 interpreter; check with your distribution for installation instructions for both. Start the XLS build by running: bazel build -c opt xls/... Then go get a cup of coffee. LLVM and Z3 are big projects, and will take a while to compile (but only the first time). Binary releases are coming soon: they'll avoid the need for long local compiles.","title":"1. Installation and building"},{"location":"tutorials/hello_xls/#2-create-your-module","text":"With your toolchain built, let's get to coding! Open up an editor and create a file called hello_xls.x in your XLS checkout root directory. Populate it with the following: fn hello_xls ( hello_string : u8 [ 11 ]) { let _ = trace ! ( hello_string ); () } Let's go over this, line-by-line: This first line declares a fn ( fn ) named \" hello_xls \". This function accepts an array of eleven characters (u8) called hello_string , and returns no value (the return type would be specified after the argument list's closing parenthesis and before the function-opening curly brace, if the function returned a value). This second line invokes the built-in trace! directive, passing it the function's input string, and throws away the result (assignment to _ ). The final line terminates the function, exiting with the empty return value () , representing an empty tuple.","title":"2. Create your module"},{"location":"tutorials/hello_xls/#3-say-hello-xls","text":"Let's run (and test) our code! First thing, though, we should make sure our module parses and passes type checking. The fastest way to do that is via the DSLX \" repl \", conveniently called repl . You can run it against the above example with the command: $ ./bazel-bin/xls/tools/repl hello_xls.x This tool first examines the specified module for language correctness, and will print an INVALID_ARGUMENT error if it fails to parse or typecheck. In that case, fix the errors and type :reload to try again. repl supports other features (IR, Verilog, and LLVM code examination), but those are outside the scope of this tutorial. Once you have a parsing DSLX file, the best way to \"smoke test\" a module is via the DSLX interpreter . First, though, we need a test case for it to execute. Add the following to the end of your hello_xls.x file: #[test] fn hello_test () { hello_xls ( \"Hello, XLS!\" ) } Again, going line-by-line: This directive tells the interpreter that the next function is a test function, meaning that it shouldn't be passed down the synthesis chain and that it should be executed by the interpreter. This line declares the [test] function hello_test , which takes no args and returns no value. The only line in this function invokes the hello_xls function and passes it a chipper greeting. With both the function and its corresponding test/driver in place, let's fire it up! Open a terminal and execute the following in the XLS checkout root directory: $ ./bazel-bin/xls/dslx/interpreter_main hello_xls.x You should see the following output: [ RUN UNITTEST ] hello_test trace of hello_string @ hello.x:4:17-4:31: [72, 101, 108, 108, 111, 44, 32, 88, 76, 83, 33] [ OK ] [===============] 1 test(s) ran; 0 failed; 0 skipped. Perfect! While this may not be what you initially expected, examine the output elements carefully: they correspond to the ASCII codes of the characters in \"Hello, XLS!\" When designing and debugging hardware, signals are more often numbers than strings, which is why they're represented as numbers here. Congrats! You've written your first piece of hardware in DSLX! It might be more satisfying, though, if your hardware actually did anything . For that, see the next tutorial, float-to-int conversion .","title":"3. Say hello, XLS!"},{"location":"tutorials/intro_to_parametrics/","text":"Tutorial: parametric types and functions This tutorial demonstrates how types and functions can be parameterized to enable them to work on data of different formats and layouts, e.g., for a function foo to work on both u16 and u32 data types, and anywhere in between. It's recommended that you're familiar with the concepts in the previous tutorial, \" float-to-int conversion\" before following this tutorial. Simple parametrics Consider the simple example of the umax function in the DSLX standard library : pub fn umax < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> uN [ N ] { if x > y { x } else { y } } Most of this function looks like other DSLX functions you may have seen, except for the new-style parameter, N . The declaration of N inside angle brackets denotes that N is a parametric value whose value is a build-time invariant that will be specified by the caller. In other words, changing regular function parameters pumps different values through the circuit, while changing parametric values changes the circuit itself. Here, N is used to define the widths of the input and output types. It's plain to see, then, that specifying N = 16 would calculate the maximum of two 16-bit numbers, whereas N = 273 would calculate the maximum of two 273-bit numbers. That being said, the smaller the circuit, the faster, smaller, and lower-power it will be, so N should be as small as possible (but no smaller!). There are two ways invoke a parametric function: the first is to explicitly specify all parametric values, and the second is to rely on the language to infer them: Explicit specification: import std fn foo ( a : u32 , b : u16 ) -> u64 { std :: umax < u32 : 64 > ( a as u64 , b as u64 ) } Here, the user has directly told the language what the values of all parametrics are. Parametric inference: import std fn foo ( a : u32 , b : u16 ) -> u64 { std :: umax ( a as u64 , b as u64 ) } Here, though, the language is able to determine that N is 64, since that matches the types of the arguments to umax , and since both arg types agree. There may be times where inference isn't possible - for example, when there exist parametrics that aren't referenced in the argument list: fn my_parametric_sum < N : u32 > ( a : u32 , b : u32 ) -> uN [ N ] { let a_mod = a as uN [ N ]; let b_mod = a as uN [ N ]; a_mod + b_mod } To invoke this function, explicit specification is required. Derived parametrics It's common, when using parametric types, to need types similar, but not identical to the parametric type. Consider calculating the unbiased floating-point exponent (from the previous tutorial): while the biased exponent was 8 bits wide, the calculated unbiased exponent was 9 bits wide due to the additional sign bit. In this situation, if EXP_SZ was 8, then it'd be handy to also have a SIGNED_EXP_SZ symbol that was equal to 9. This can be done as follows: fn unbias_exponent < EXP_SZ : u32 , SIGNED_EXP_SZ : u32 = EXP_SZ + u32 : 1 > ( exp : uN [ EXP_SZ ]) -> sN [ SIGNED_EXP_SZ ] { exp as sN [ SIGNED_EXP_SZ ] - sN [ SIGNED_EXP_SZ ] : ??? } Oh no! Specifying parametrics in this way has revealed a problem! If we parameterize types, then in some situations, we'll need to also parameterize values ! Of course, we'd not be writing this tutorial if that wasn't possible. DSLX supports \"constexpr\"-style evaluation, whereby constant expressions can be evaluated at interpretation or compilation time. In this case, we just need an expression that can calculate the correct bias adjustment: (sN[SIGNED_EXP_SZ]:1 << (EXP_SZ - u32:1)) - sN[SIGNED_EXP_SZ]:1 This is a bit unwieldy in practice, so we can wrap it in a function: fn bias_scaler < N : u32 , WIDE_N : u32 = N + u32 : 1 > () -> sN [ WIDE_N ] { ( sN [ WIDE_N ] : 1 << ( N - u32 : 1 )) - sN [ WIDE_N ] : 1 } fn unbias_exponent < EXP_SZ : u32 , SIGNED_EXP_SZ : u32 = EXP_SZ + u32 : 1 > ( exp : uN [ EXP_SZ ]) -> sN [ SIGNED_EXP_SZ ] { exp as sN [ SIGNED_EXP_SZ ] - bias_scaler < EXP_SZ > () } Parameterized float-to-int Finally, consider the 32-bit float-to-int program from the previous tutorial. That program was restricted to converting from one specific type to another. If, however, we wanted to convert from, say a double to an int32_t , we'd have to write a new function, even though the basic logic would be the same. Instead, armed with parametrics, we can write a single function to handle all such conversions - even to floating-point formats we haven't considered! The first step in such a parameterization is to have a working single-typed example, which we take from the previous codelab: pub struct float32 { sign : u1 , bexp : u8 , fraction : u23 , } fn unbias_exponent ( exp : u8 ) -> s9 { exp as s9 - s9 : 127 } pub fn float_to_int ( x : float32 ) -> s32 { let exp = unbias_exponent ( x . bexp ); // Add the implicit leading one. // Note that we need to add one bit to the fraction to hold it. let fraction = u33 : 1 << 23 | ( x . fraction as u33 ); // Shift the result to the right if the exponent is less than 23. let fraction = if ( exp as u8 ) < u8 : 23 { fraction >> ( u8 : 23 - ( exp as u8 )) } else { fraction }; // Shift the result to the left if the exponent is greater than 23. let fraction = if ( exp as u8 ) > u8 : 23 { fraction << (( exp as u8 ) - u8 : 23 ) } else { fraction }; let result = fraction as s32 ; let result = if x . sign { - result } else { result }; result } Next is to identify all types needing parameterization, here being the intended size of the result and the layout of the floating-point type itself; all other types flow from that base definition: exp: float32::bexp` size + 1 sign bit fraction: float32::fraction` size + 1 implicit leading bit Thus, the struct declaration and function signature will be: pub struct float < EXP_SZ : u32 , FRACTION_SZ : u32 > { sign : u1 , bexp : uN [ EXP_SZ ], fraction : uN [ FRACTION_SZ ], } pub fn float_to_int < EXP_SZ : u32 , FRACTION_SZ : u32 , RESULT_SZ : u32 > ( x : float < EXP_SZ , FRACTION_SZ > ) -> sN [ RESULT_SZ ] { .. . } From there, the rest of the function can be populated by replacing the types in the original implementation with the parameterized ones in the signature: pub struct float < EXP_SZ : u32 , FRACTION_SZ : u32 > { sign : u1 , bexp : uN [ EXP_SZ ], fraction : uN [ FRACTION_SZ ], } fn bias_scaler < N : u32 , WIDE_N : u32 = N + u32 : 1 > () -> sN [ WIDE_N ] { ( sN [ WIDE_N ] : 1 << ( N - u32 : 1 )) - sN [ WIDE_N ] : 1 } fn unbias_exponent < EXP_SZ : u32 , SIGNED_EXP_SZ : u32 = EXP_SZ + u32 : 1 > ( exp : uN [ EXP_SZ ]) -> sN [ SIGNED_EXP_SZ ] { exp as sN [ SIGNED_EXP_SZ ] - bias_scaler < EXP_SZ > () } pub fn float_to_int < EXP_SZ : u32 , FRACTION_SZ : u32 , RESULT_SZ : u32 , WIDE_EXP_SZ : u32 = EXP_SZ + u32 : 1 , WIDE_FRACTION_SZ : u32 = FRACTION_SZ + u32 : 1 > ( x : float < EXP_SZ , FRACTION_SZ > ) -> sN [ RESULT_SZ ] { let exp = unbias_exponent ( x . bexp ); let fraction = uN [ WIDE_FRACTION_SZ ] : 1 << FRACTION_SZ | ( x . fraction as uN [ WIDE_FRACTION_SZ ]); let fraction = if ( exp as u32 ) < FRACTION_SZ { fraction >> ( FRACTION_SZ - ( exp as u32 )) } else { fraction }; let fraction = if ( exp as u32 ) > FRACTION_SZ { fraction << (( exp as u32 ) - FRACTION_SZ ) } else { fraction }; let result = fraction as sN [ RESULT_SZ ]; let result = if x . sign { - result } else { result }; result } Note that unbias_exponent() didn't need type specification, since the type could be inferred from the argument! (Also note that this implementation doesn't contain the fixes from the missing cases from the previous tutorial. Exercise to the reader: apply those fixes here, too!) This technique underlies all of XLS' floating-point libraries. Common operations are defined in common files, such as apfloat.x (general utilities) or apfloat_add_2.x (two-way addition) or apfloat_fma.x (fused multiply-add). Specializations of the above are then available in, e.g., float32.x , fp32_add_2.x , and fma_32.x , respectively, to hide internal implementation details from end users. With this technique, you can write single implementations of functionality that can be applicable across all sorts of hardware configurations for minimal additional cost. Try it out! Create an 0xbeef -bit wide floating-point adder!","title":"Intro to parameterics"},{"location":"tutorials/intro_to_parametrics/#tutorial-parametric-types-and-functions","text":"This tutorial demonstrates how types and functions can be parameterized to enable them to work on data of different formats and layouts, e.g., for a function foo to work on both u16 and u32 data types, and anywhere in between. It's recommended that you're familiar with the concepts in the previous tutorial, \" float-to-int conversion\" before following this tutorial.","title":"Tutorial: parametric types and functions"},{"location":"tutorials/intro_to_parametrics/#simple-parametrics","text":"Consider the simple example of the umax function in the DSLX standard library : pub fn umax < N : u32 > ( x : uN [ N ], y : uN [ N ]) -> uN [ N ] { if x > y { x } else { y } } Most of this function looks like other DSLX functions you may have seen, except for the new-style parameter, N . The declaration of N inside angle brackets denotes that N is a parametric value whose value is a build-time invariant that will be specified by the caller. In other words, changing regular function parameters pumps different values through the circuit, while changing parametric values changes the circuit itself. Here, N is used to define the widths of the input and output types. It's plain to see, then, that specifying N = 16 would calculate the maximum of two 16-bit numbers, whereas N = 273 would calculate the maximum of two 273-bit numbers. That being said, the smaller the circuit, the faster, smaller, and lower-power it will be, so N should be as small as possible (but no smaller!). There are two ways invoke a parametric function: the first is to explicitly specify all parametric values, and the second is to rely on the language to infer them: Explicit specification: import std fn foo ( a : u32 , b : u16 ) -> u64 { std :: umax < u32 : 64 > ( a as u64 , b as u64 ) } Here, the user has directly told the language what the values of all parametrics are. Parametric inference: import std fn foo ( a : u32 , b : u16 ) -> u64 { std :: umax ( a as u64 , b as u64 ) } Here, though, the language is able to determine that N is 64, since that matches the types of the arguments to umax , and since both arg types agree. There may be times where inference isn't possible - for example, when there exist parametrics that aren't referenced in the argument list: fn my_parametric_sum < N : u32 > ( a : u32 , b : u32 ) -> uN [ N ] { let a_mod = a as uN [ N ]; let b_mod = a as uN [ N ]; a_mod + b_mod } To invoke this function, explicit specification is required.","title":"Simple parametrics"},{"location":"tutorials/intro_to_parametrics/#derived-parametrics","text":"It's common, when using parametric types, to need types similar, but not identical to the parametric type. Consider calculating the unbiased floating-point exponent (from the previous tutorial): while the biased exponent was 8 bits wide, the calculated unbiased exponent was 9 bits wide due to the additional sign bit. In this situation, if EXP_SZ was 8, then it'd be handy to also have a SIGNED_EXP_SZ symbol that was equal to 9. This can be done as follows: fn unbias_exponent < EXP_SZ : u32 , SIGNED_EXP_SZ : u32 = EXP_SZ + u32 : 1 > ( exp : uN [ EXP_SZ ]) -> sN [ SIGNED_EXP_SZ ] { exp as sN [ SIGNED_EXP_SZ ] - sN [ SIGNED_EXP_SZ ] : ??? } Oh no! Specifying parametrics in this way has revealed a problem! If we parameterize types, then in some situations, we'll need to also parameterize values ! Of course, we'd not be writing this tutorial if that wasn't possible. DSLX supports \"constexpr\"-style evaluation, whereby constant expressions can be evaluated at interpretation or compilation time. In this case, we just need an expression that can calculate the correct bias adjustment: (sN[SIGNED_EXP_SZ]:1 << (EXP_SZ - u32:1)) - sN[SIGNED_EXP_SZ]:1 This is a bit unwieldy in practice, so we can wrap it in a function: fn bias_scaler < N : u32 , WIDE_N : u32 = N + u32 : 1 > () -> sN [ WIDE_N ] { ( sN [ WIDE_N ] : 1 << ( N - u32 : 1 )) - sN [ WIDE_N ] : 1 } fn unbias_exponent < EXP_SZ : u32 , SIGNED_EXP_SZ : u32 = EXP_SZ + u32 : 1 > ( exp : uN [ EXP_SZ ]) -> sN [ SIGNED_EXP_SZ ] { exp as sN [ SIGNED_EXP_SZ ] - bias_scaler < EXP_SZ > () }","title":"Derived parametrics"},{"location":"tutorials/intro_to_parametrics/#parameterized-float-to-int","text":"Finally, consider the 32-bit float-to-int program from the previous tutorial. That program was restricted to converting from one specific type to another. If, however, we wanted to convert from, say a double to an int32_t , we'd have to write a new function, even though the basic logic would be the same. Instead, armed with parametrics, we can write a single function to handle all such conversions - even to floating-point formats we haven't considered! The first step in such a parameterization is to have a working single-typed example, which we take from the previous codelab: pub struct float32 { sign : u1 , bexp : u8 , fraction : u23 , } fn unbias_exponent ( exp : u8 ) -> s9 { exp as s9 - s9 : 127 } pub fn float_to_int ( x : float32 ) -> s32 { let exp = unbias_exponent ( x . bexp ); // Add the implicit leading one. // Note that we need to add one bit to the fraction to hold it. let fraction = u33 : 1 << 23 | ( x . fraction as u33 ); // Shift the result to the right if the exponent is less than 23. let fraction = if ( exp as u8 ) < u8 : 23 { fraction >> ( u8 : 23 - ( exp as u8 )) } else { fraction }; // Shift the result to the left if the exponent is greater than 23. let fraction = if ( exp as u8 ) > u8 : 23 { fraction << (( exp as u8 ) - u8 : 23 ) } else { fraction }; let result = fraction as s32 ; let result = if x . sign { - result } else { result }; result } Next is to identify all types needing parameterization, here being the intended size of the result and the layout of the floating-point type itself; all other types flow from that base definition: exp: float32::bexp` size + 1 sign bit fraction: float32::fraction` size + 1 implicit leading bit Thus, the struct declaration and function signature will be: pub struct float < EXP_SZ : u32 , FRACTION_SZ : u32 > { sign : u1 , bexp : uN [ EXP_SZ ], fraction : uN [ FRACTION_SZ ], } pub fn float_to_int < EXP_SZ : u32 , FRACTION_SZ : u32 , RESULT_SZ : u32 > ( x : float < EXP_SZ , FRACTION_SZ > ) -> sN [ RESULT_SZ ] { .. . } From there, the rest of the function can be populated by replacing the types in the original implementation with the parameterized ones in the signature: pub struct float < EXP_SZ : u32 , FRACTION_SZ : u32 > { sign : u1 , bexp : uN [ EXP_SZ ], fraction : uN [ FRACTION_SZ ], } fn bias_scaler < N : u32 , WIDE_N : u32 = N + u32 : 1 > () -> sN [ WIDE_N ] { ( sN [ WIDE_N ] : 1 << ( N - u32 : 1 )) - sN [ WIDE_N ] : 1 } fn unbias_exponent < EXP_SZ : u32 , SIGNED_EXP_SZ : u32 = EXP_SZ + u32 : 1 > ( exp : uN [ EXP_SZ ]) -> sN [ SIGNED_EXP_SZ ] { exp as sN [ SIGNED_EXP_SZ ] - bias_scaler < EXP_SZ > () } pub fn float_to_int < EXP_SZ : u32 , FRACTION_SZ : u32 , RESULT_SZ : u32 , WIDE_EXP_SZ : u32 = EXP_SZ + u32 : 1 , WIDE_FRACTION_SZ : u32 = FRACTION_SZ + u32 : 1 > ( x : float < EXP_SZ , FRACTION_SZ > ) -> sN [ RESULT_SZ ] { let exp = unbias_exponent ( x . bexp ); let fraction = uN [ WIDE_FRACTION_SZ ] : 1 << FRACTION_SZ | ( x . fraction as uN [ WIDE_FRACTION_SZ ]); let fraction = if ( exp as u32 ) < FRACTION_SZ { fraction >> ( FRACTION_SZ - ( exp as u32 )) } else { fraction }; let fraction = if ( exp as u32 ) > FRACTION_SZ { fraction << (( exp as u32 ) - FRACTION_SZ ) } else { fraction }; let result = fraction as sN [ RESULT_SZ ]; let result = if x . sign { - result } else { result }; result } Note that unbias_exponent() didn't need type specification, since the type could be inferred from the argument! (Also note that this implementation doesn't contain the fixes from the missing cases from the previous tutorial. Exercise to the reader: apply those fixes here, too!) This technique underlies all of XLS' floating-point libraries. Common operations are defined in common files, such as apfloat.x (general utilities) or apfloat_add_2.x (two-way addition) or apfloat_fma.x (fused multiply-add). Specializations of the above are then available in, e.g., float32.x , fp32_add_2.x , and fma_32.x , respectively, to hide internal implementation details from end users. With this technique, you can write single implementations of functionality that can be applicable across all sorts of hardware configurations for minimal additional cost. Try it out! Create an 0xbeef -bit wide floating-point adder!","title":"Parameterized float-to-int"},{"location":"tutorials/intro_to_procs/","text":"DSLX Tutorial: Intro to procs Up to this point, our tutorials have described stateless, non-communicating, combinational modules. To add state or communication with other actors, we need to venture into the exciting land of procs! Procs , short for \"communicating sequential processes\", are the means by which DSLX models sequential and stateful modules. A proc contains: A config function that initializes constant proc state and spawns any other dependent/child procs needed for execution. A recurrent (i.e., infinitely looping) next function that contains the actual logic to be executed by the proc. A critical component of procs is communication: every proc needs a means to share data with other procs, or else it'd just be spinning dead code. These means are channels : entities into which data can be sent and from which data can be received. Each channel has a send and a receive endpoint: data inserted into a channel by a send op can be pulled out by a recv op. Example proc Many concepts here are more easily explained via example, so here's a possible DSLX implementation of FMAC (fused multiply-accumulate) , which computes C = A * B + C : import xls . modules . fp . fp32_fma proc Fmac { input_a_consumer : chan < F32 > in ; input_b_consumer : chan < F32 > in ; output_producer : chan < F32 > out ; config ( input_a_consumer : chan < F32 > in , input_b_consumer : chan < F32 > in , output_producer : chan < F32 > out ) { ( input_a_consumer , input_b_consumer , output_producer ) } next ( tok : token , state : F32 ) { let ( tok_a , input_a ) = recv ( tok , input_a_consumer ); let ( tok_b , input_b ) = recv ( tok , input_b_consumer ); let result = fp32_fma :: fp32_fma ( input_a , input_b , state ); let tok = join ( tok_a , tok_b ); let tok = send ( tok , output_producer , result ); ( result ,) } } (In practice, a FMAC unit would want a reset signal, but we've leaving that out for simplicity.) There's a lot to unpack here, so we'll walk through the example. The first part of interest is the declaration of the proc member values: the three channels. Procs can have \"member\" state, similar to class data members in software. In DSLX, proc members are constant values, and are set by the output of the config function. Member values can be referred to inside the next function in the same way as locally-declared data. Next up is the config function itself. When a proc is \"spawned\", its given two sets of values: one set for the config function and one set to initialize next (following this example, we'll show how procs are spawned). Inside a config function, any constant values can be computed, any necessary procs can be spawned, and finally member values are set by the return value. Member values are assigned in declaration order: the first element of the return tuple corresponds to the first-declared member, and so on. After that, we encounter next . This function serves as the real \"body\" of the Proc. The next function maintains and evolves the proc's recurrent state and is responsible for communicating with the outside world, as well. In our example, the first two lines are that communication, receiving the input values for the computation. The token elements are used to sequence events: since the receives can happen in parallel, they can share the same token, but since sending the output must happen after that, their result tokens are \"joined\" (think joining two threads of execution in software), and the result is used to sequence the send. Between the communication routines is the actual computation. At the end of the proc, we terminate with the result value. This final value becomes the input state for the next iteration. This is how recurrent state is managed by procs: a state value is provided to the next function, and the result of that function is used as the next iteration's state input. Procs can have several state elements: in that case, there will be several input state args, e.g., next(tok: token, state_a: F32, state_b: F32) , and the output will be a tuple of values, corresponding in order to the input state values. Regardless of that, a next function will always take a token as the first parameter. Spawning procs In any real design, procs will form a network 1 , where one proc will spawn any number of child procs, which themselves might spawn other procs. (The \"root\" proc will be instantiated by some outside component in the outside RTL environment.) Procs may only be spawned in config functions, as they're part of statically configuring the hardware network to construct. As an example, spawning a couple of our procs above would look as follows: proc Spawner { fmac_1_a_producer = chan < F32 > out ; fmac_1_b_producer = chan < F32 > out ; fmac_1_output_consumer = chan < F32 > in ; fmac_2_a_producer = chan < F32 > out ; fmac_2_b_producer = chan < F32 > out ; fmac_2_output_consumer = chan < F32 > in ; config () { let ( fmac_1_a_p , fmac_1_a_c ) = chan < F32 > ; let ( fmac_1_b_p , fmac_1_b_c ) = chan < F32 > ; let ( fmac_1_output_p , fmac_1_output_c ) = chan < F32 > ; spawn fmac ( fmac_1_a_c , fmac_1_b_c , fmac_1_output_p )( float32 :: zero ( false )); let ( fmac_2_a_p , fmac_2_a_c ) = chan < F32 > ; let ( fmac_2_b_p , fmac_2_b_c ) = chan < F32 > ; let ( fmac_2_output_p , fmac_2_output_c ) = chan < F32 > ; spawn fmac ( fmac_2_a_c , fmac_2_b_c , fmac_2_output_p )( float32 :: zero ( false )); ( fmac_1_a_p , fmac_1_b_p , fmac_1_output_c , fmac_2_a_p , fmac_2_b_p , fmac_2_output_c ) } } For each child proc, we first declare the necessary channels (each channel declaration produces a producer and consumer channel, respectively), then we actually spawn it. The first set of arguments is passed to the child's config function and the second is the initial state for the child proc. A spawn produces no value, hence no let on the left-hand side. Advanced features Channel arrays and loop-based spawning Note : This feature is currently WIP and is not yet available. Many hardware layouts have regular arrays of components, such as systolic arrays, vector units, etc. Individually specifying these quickly grows cumbersome, so users can instead declare channels of arrays and spawn procs inside for loops. This looks as follows: proc Spawner4x4 { input_producers : chan < F32 > out [ 4 ][ 4 ]; output_consumers : chan < F32 > out [ 4 ][ 4 ]; config () { let ( input_producers , input_consumers ) = chan [ 4 ][ 4 ] F32 ; let ( output_producers , output_consumers ) = chan [ 4 ][ 4 ] F32 ; for ( i ) : ( u32 ) in range ( 0 , 4 ) { for ( j ) : ( u32 ) in range ( 0 , 4 ) { spawn Node ( input_consumers [ i ][ j ], output_producers [ i ][ j ])( float32 :: zero ( false )) }() }() ( input_producers , output_consumers ) } } Parametric procs Just as with other DSLX constructs, Procs can be parameterized. Parametrics must be specified at the proc level, and not at the component function level (i.e., not on config or next ). Building off of the previous example, this looks as follows: proc Parametric < N : u32 , M : u32 > { input_producers : chan < F32 > out [ N ][ M ]; output_consumers : chan < F32 > out [ N ][ M ]; config () { let ( input_producers , input_consumers ) = chan [ N ][ M ] F32 ; let ( output_producers , output_consumers ) = chan [ N ][ M ] F32 ; for ( i ) : ( u32 ) in range ( 0 , N ) { for ( j ) : ( u32 ) in range ( 0 , M ) { spawn Node ( input_consumers [ i ][ j ], output_producers [ i ][ j ])( float32 :: zero ( false )) }() }() ( input_producers , output_consumers ) } } These two features are very powerful together: users can specify a broad variety of designs simply by adjusting a few parametric values. Proc testing The DSLX interpreter supports testing procs via the test_proc construct. A test proc is very similar to a normal proc with the following changes: A test proc is preceded by the #[test_proc()] directive. This directive, as one might expect, notifies the interpreter that the following proc is a test proc. Any initial state needed by the test proc should go inside the parentheses. A test proc's config function must accept a single argument: a boolean input channel for terminating interpretation. When the test is complete, the proc should send the test's status ( true on success, false on failure) on that channel (commonly called the \"terminator\" channel). A skeletal example: #[test_proc(u32:0)] proc Tester { terminator : chan < bool > out ; config ( terminator : chan < bool > out ) { spawn proc_under_test ( .. .)( .. .); ( terminator ,) } next ( tok : token , state : u32 ) { new_state = .. . ( new_state ,) } } The FP32 fmac module has a more complete proc test that may be used for reference. Scheduling constraints If you want to interface with something in the outside world that is latency sensitive (for example, an SRAM -- though we have separate infrastructure for making SRAMs work that builds on top of this feature), you can create external channels representing the interface you want to use, e.g.: proc main { req : chan < u32 > out ; resp : chan < u32 > in ; init { u32 : 0 } config ( req : chan < u32 > out , resp : chan < u32 > in ) { ( req , resp ) } next ( tok : token , state : u32 ) { let request = state * state ; let tok = send ( tok , req , request ); let ( tok , response ) = recv ( tok , resp ); state + u32 : 1 } } where the fact that req and resp are parameters of config , and main is the top proc during IR conversion, is what makes them \"external\". Then when you codegen this, you can pass in --io_constraints=foo__req:send:foo__resp:recv:2:2 where foo__req is the mangled name of the channel, which you can see by examining the generated IR prior to codegen. That constraint means \"a send on any channel named req must occur exactly two cycles before a receive on any channel named resp \"; the 2 is specified twice because it is possible to give a range of allowed cycle differences. For more details on --io_constraints , check out the docs . For a complete example, see //xls/examples:constraint_sv and associated build targets; the target you'd build to get the mangled channel names is :constraint_ir . The proc network itself will form a tree, but channels may make point-to-point connections between any two procs. \u21a9","title":"Intro to Procs"},{"location":"tutorials/intro_to_procs/#dslx-tutorial-intro-to-procs","text":"Up to this point, our tutorials have described stateless, non-communicating, combinational modules. To add state or communication with other actors, we need to venture into the exciting land of procs! Procs , short for \"communicating sequential processes\", are the means by which DSLX models sequential and stateful modules. A proc contains: A config function that initializes constant proc state and spawns any other dependent/child procs needed for execution. A recurrent (i.e., infinitely looping) next function that contains the actual logic to be executed by the proc. A critical component of procs is communication: every proc needs a means to share data with other procs, or else it'd just be spinning dead code. These means are channels : entities into which data can be sent and from which data can be received. Each channel has a send and a receive endpoint: data inserted into a channel by a send op can be pulled out by a recv op.","title":"DSLX Tutorial: Intro to procs"},{"location":"tutorials/intro_to_procs/#example-proc","text":"Many concepts here are more easily explained via example, so here's a possible DSLX implementation of FMAC (fused multiply-accumulate) , which computes C = A * B + C : import xls . modules . fp . fp32_fma proc Fmac { input_a_consumer : chan < F32 > in ; input_b_consumer : chan < F32 > in ; output_producer : chan < F32 > out ; config ( input_a_consumer : chan < F32 > in , input_b_consumer : chan < F32 > in , output_producer : chan < F32 > out ) { ( input_a_consumer , input_b_consumer , output_producer ) } next ( tok : token , state : F32 ) { let ( tok_a , input_a ) = recv ( tok , input_a_consumer ); let ( tok_b , input_b ) = recv ( tok , input_b_consumer ); let result = fp32_fma :: fp32_fma ( input_a , input_b , state ); let tok = join ( tok_a , tok_b ); let tok = send ( tok , output_producer , result ); ( result ,) } } (In practice, a FMAC unit would want a reset signal, but we've leaving that out for simplicity.) There's a lot to unpack here, so we'll walk through the example. The first part of interest is the declaration of the proc member values: the three channels. Procs can have \"member\" state, similar to class data members in software. In DSLX, proc members are constant values, and are set by the output of the config function. Member values can be referred to inside the next function in the same way as locally-declared data. Next up is the config function itself. When a proc is \"spawned\", its given two sets of values: one set for the config function and one set to initialize next (following this example, we'll show how procs are spawned). Inside a config function, any constant values can be computed, any necessary procs can be spawned, and finally member values are set by the return value. Member values are assigned in declaration order: the first element of the return tuple corresponds to the first-declared member, and so on. After that, we encounter next . This function serves as the real \"body\" of the Proc. The next function maintains and evolves the proc's recurrent state and is responsible for communicating with the outside world, as well. In our example, the first two lines are that communication, receiving the input values for the computation. The token elements are used to sequence events: since the receives can happen in parallel, they can share the same token, but since sending the output must happen after that, their result tokens are \"joined\" (think joining two threads of execution in software), and the result is used to sequence the send. Between the communication routines is the actual computation. At the end of the proc, we terminate with the result value. This final value becomes the input state for the next iteration. This is how recurrent state is managed by procs: a state value is provided to the next function, and the result of that function is used as the next iteration's state input. Procs can have several state elements: in that case, there will be several input state args, e.g., next(tok: token, state_a: F32, state_b: F32) , and the output will be a tuple of values, corresponding in order to the input state values. Regardless of that, a next function will always take a token as the first parameter.","title":"Example proc"},{"location":"tutorials/intro_to_procs/#spawning-procs","text":"In any real design, procs will form a network 1 , where one proc will spawn any number of child procs, which themselves might spawn other procs. (The \"root\" proc will be instantiated by some outside component in the outside RTL environment.) Procs may only be spawned in config functions, as they're part of statically configuring the hardware network to construct. As an example, spawning a couple of our procs above would look as follows: proc Spawner { fmac_1_a_producer = chan < F32 > out ; fmac_1_b_producer = chan < F32 > out ; fmac_1_output_consumer = chan < F32 > in ; fmac_2_a_producer = chan < F32 > out ; fmac_2_b_producer = chan < F32 > out ; fmac_2_output_consumer = chan < F32 > in ; config () { let ( fmac_1_a_p , fmac_1_a_c ) = chan < F32 > ; let ( fmac_1_b_p , fmac_1_b_c ) = chan < F32 > ; let ( fmac_1_output_p , fmac_1_output_c ) = chan < F32 > ; spawn fmac ( fmac_1_a_c , fmac_1_b_c , fmac_1_output_p )( float32 :: zero ( false )); let ( fmac_2_a_p , fmac_2_a_c ) = chan < F32 > ; let ( fmac_2_b_p , fmac_2_b_c ) = chan < F32 > ; let ( fmac_2_output_p , fmac_2_output_c ) = chan < F32 > ; spawn fmac ( fmac_2_a_c , fmac_2_b_c , fmac_2_output_p )( float32 :: zero ( false )); ( fmac_1_a_p , fmac_1_b_p , fmac_1_output_c , fmac_2_a_p , fmac_2_b_p , fmac_2_output_c ) } } For each child proc, we first declare the necessary channels (each channel declaration produces a producer and consumer channel, respectively), then we actually spawn it. The first set of arguments is passed to the child's config function and the second is the initial state for the child proc. A spawn produces no value, hence no let on the left-hand side.","title":"Spawning procs"},{"location":"tutorials/intro_to_procs/#advanced-features","text":"","title":"Advanced features"},{"location":"tutorials/intro_to_procs/#channel-arrays-and-loop-based-spawning","text":"Note : This feature is currently WIP and is not yet available. Many hardware layouts have regular arrays of components, such as systolic arrays, vector units, etc. Individually specifying these quickly grows cumbersome, so users can instead declare channels of arrays and spawn procs inside for loops. This looks as follows: proc Spawner4x4 { input_producers : chan < F32 > out [ 4 ][ 4 ]; output_consumers : chan < F32 > out [ 4 ][ 4 ]; config () { let ( input_producers , input_consumers ) = chan [ 4 ][ 4 ] F32 ; let ( output_producers , output_consumers ) = chan [ 4 ][ 4 ] F32 ; for ( i ) : ( u32 ) in range ( 0 , 4 ) { for ( j ) : ( u32 ) in range ( 0 , 4 ) { spawn Node ( input_consumers [ i ][ j ], output_producers [ i ][ j ])( float32 :: zero ( false )) }() }() ( input_producers , output_consumers ) } }","title":"Channel arrays and loop-based spawning"},{"location":"tutorials/intro_to_procs/#parametric-procs","text":"Just as with other DSLX constructs, Procs can be parameterized. Parametrics must be specified at the proc level, and not at the component function level (i.e., not on config or next ). Building off of the previous example, this looks as follows: proc Parametric < N : u32 , M : u32 > { input_producers : chan < F32 > out [ N ][ M ]; output_consumers : chan < F32 > out [ N ][ M ]; config () { let ( input_producers , input_consumers ) = chan [ N ][ M ] F32 ; let ( output_producers , output_consumers ) = chan [ N ][ M ] F32 ; for ( i ) : ( u32 ) in range ( 0 , N ) { for ( j ) : ( u32 ) in range ( 0 , M ) { spawn Node ( input_consumers [ i ][ j ], output_producers [ i ][ j ])( float32 :: zero ( false )) }() }() ( input_producers , output_consumers ) } } These two features are very powerful together: users can specify a broad variety of designs simply by adjusting a few parametric values.","title":"Parametric procs"},{"location":"tutorials/intro_to_procs/#proc-testing","text":"The DSLX interpreter supports testing procs via the test_proc construct. A test proc is very similar to a normal proc with the following changes: A test proc is preceded by the #[test_proc()] directive. This directive, as one might expect, notifies the interpreter that the following proc is a test proc. Any initial state needed by the test proc should go inside the parentheses. A test proc's config function must accept a single argument: a boolean input channel for terminating interpretation. When the test is complete, the proc should send the test's status ( true on success, false on failure) on that channel (commonly called the \"terminator\" channel). A skeletal example: #[test_proc(u32:0)] proc Tester { terminator : chan < bool > out ; config ( terminator : chan < bool > out ) { spawn proc_under_test ( .. .)( .. .); ( terminator ,) } next ( tok : token , state : u32 ) { new_state = .. . ( new_state ,) } } The FP32 fmac module has a more complete proc test that may be used for reference.","title":"Proc testing"},{"location":"tutorials/intro_to_procs/#scheduling-constraints","text":"If you want to interface with something in the outside world that is latency sensitive (for example, an SRAM -- though we have separate infrastructure for making SRAMs work that builds on top of this feature), you can create external channels representing the interface you want to use, e.g.: proc main { req : chan < u32 > out ; resp : chan < u32 > in ; init { u32 : 0 } config ( req : chan < u32 > out , resp : chan < u32 > in ) { ( req , resp ) } next ( tok : token , state : u32 ) { let request = state * state ; let tok = send ( tok , req , request ); let ( tok , response ) = recv ( tok , resp ); state + u32 : 1 } } where the fact that req and resp are parameters of config , and main is the top proc during IR conversion, is what makes them \"external\". Then when you codegen this, you can pass in --io_constraints=foo__req:send:foo__resp:recv:2:2 where foo__req is the mangled name of the channel, which you can see by examining the generated IR prior to codegen. That constraint means \"a send on any channel named req must occur exactly two cycles before a receive on any channel named resp \"; the 2 is specified twice because it is possible to give a range of allowed cycle differences. For more details on --io_constraints , check out the docs . For a complete example, see //xls/examples:constraint_sv and associated build targets; the target you'd build to get the mangled channel names is :constraint_ir . The proc network itself will form a tree, but channels may make point-to-point connections between any two procs. \u21a9","title":"Scheduling constraints"},{"location":"tutorials/prefix_scan/","text":"DSLX Tutorial: enumerate and match expressions In this document we explain in detail the implementation of a 8 byte prefix scan computation. In order to understand the implementation, it is useful to understand the intended functionality first. For a given input of 8 bytes, the scan iterates from left to right over the input and produces an output of the same size. Each element in the output contains the count of duplicate values seen so far in the input. The counter resets to 0 if a new value is found. For example, for this input: let input = u32 [ 8 ] : [ 0 , .. .] the code should produce this output: u3 [ 8 ] : [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ]) At index 0 it has not yet found any value, so it assigns a counter value of 0 . At index 1 it finds the second occurrence of the value '0' (which is the 1st duplicate) and therefore adds a 1 to the counter from index 0. At index 2 it finds the third occurrence of the value '0' (which is the 2nd duplicate) and therefore adds a 1 to the counter from index 1. And so on. Correspondingly, for this input: let input = u32 [ 8 ] : [ 0 , 0 , 1 , 1 , 2 , 2 , 3 , 3 ] it should produce: assert_eq ( result , u3 [ 8 ] : [ 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 ]) The full listing is in examples/dslx_intro/prefix_scan_equality.x . Function prefix_scan_eq The implementation displays a few interesting language features. The function prototype is straight-forward. Input is an array of 8 values of type u32 . Output is an array of size 8 holding 3-bit values (the maximum resulting count can only be 7, which fits in 3 bits). fn prefix_scan_eq ( x : u32 [ 8 ]) -> u3 [ 8 ] { The first let expression produces a tuple of 3 values. It only cares about the last value result , so it stubs out the other two elements via the 'ignore' placeholder _ . let ( _ , _ , result ) = Why a 3-Tuple? Because he following loop has tuple of three values as the accumulator. The return type of the loop is the type of the accumulator, so above let needs to be of the same type. Enumerated Loop Using tuples as the accumulator is a convenient way to model multiple loop-carried values: for (( i , elem ), ( prior , count , result )) : (( u32 , u32 ), ( u32 , u3 , u3 [ 8 ])) in enumerate ( x ) { The iterable of this loop is enumerate(x) . On each iteration, this construct delivers a tuple consisting of current index and current element. This is represented as the tuple (i, elem) in the for construct. The loop next specifies the accumulator, which is a 3-tuple consisting of the values named prior , count , and result . The types of the iterable and accumulator are specified next. The iterable is a tuple consisting of two u32 values. The accumulator is more interesting, it is a tuple consisting of a u32 value ( prior ), a u3 value ( count ), and an array type u3[8] , which is an array holding 8 elements of bit-width 3. This is the type of result in the accumulator. Looping back to the prior let statement, it ignores the prior and count members of the tuple and will only return the result part. A Match Expression The next expression is an interesting match expression. The let expression binds the tuple (to_place, new_count): (u3, u3) to the result of the following match expression: let ( to_place , new_count ) : ( u3 , u3 ) = match ( i == u32 : 0 , prior == elem ) { to_place will hold the value that is to be written at a given index. new_count will contain the updated counter value. The match expression evaluates two conditions in parallel: is i == 0? is the prior element the same as the current elem Two tests mean there are four possible cases, which are all handled in the following four cases: // i == 0 (no matter whether prior == elem or not): // we set position 0 to 0 and update the new_counter to 1 ( true , true ) => ( u3 : 0 , u3 : 1 ), ( true , false ) => ( u3 : 0 , u3 : 1 ), // if i != 0 - if the current element is the same as pior, // set to_place to the value of the current count // update new_counter with the increased counter value ( false , true ) => ( count , count + u3 : 1 ), // if i != 0 - if current element is different from prior, // set to_place back to 0 // set new_counter back to 1 ( false , false ) => ( u3 : 0 , u3 : 1 ), }; To update the result, we set index i in the result array to the value to_place via the built-in update function, which produces a new value new_result ): let new_result : u3 [ 8 ] = update ( result , i , to_place ); Finally the updated accumulator value is constructed, it is the last expression in the loop: ( elem , new_count , new_result ) Following the loop body, as an argument to the loop, we initialize the accumulator in the following way. set element prior to -1, in order to not match any other value. set element count to 0. set element result to 8 0's of size u3 . (Note that the ... syntax is short for \"fill in the rest of the elements with the last value specified\".) }(( u32 : - 1 , u3 : 0 , u3 [ 8 ] : [ 0 , .. .])); And, finally, the function simply returns result : result } Testing To test the two cases we've described above, we add the following two test cases right to this implementation file: #[test] fn test_prefix_scan_eq_all_zero () { let input = u32 [ 8 ] : [ 0 , .. .]; let result = prefix_scan_eq ( input ); assert_eq ( result , u3 [ 8 ] : [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ]) } #[test] fn test_prefix_scan_eq_doubles () { let input = u32 [ 8 ] : [ 0 , 0 , 1 , 1 , 2 , 2 , 3 , 3 ]; let result = prefix_scan_eq ( input ); assert_eq ( result , u3 [ 8 ] : [ 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 ]) } To run tests against the file present in the repository: xls$ bazel run -c opt //xls/dslx:interpreter_main -- \\ $ PWD/xls/examples/dslx_intro/prefix_scan_equality.x \\ --compare = none [ RUN UNITTEST ] prefix_scan_eq_all_zero_test [ OK ] [ RUN UNITTEST ] prefix_scan_eq_doubles_test [ OK ] [===============] 2 test(s) ran; 0 failed; 0 skipped. (Note that --compare=none is currently required because enumerate ranges are not currently convertable to IR, otherwise running the DSLX interpreter would do implicit comparison to IR interpreter -- see google/xls#164 .)","title":"Enumerate and match"},{"location":"tutorials/prefix_scan/#dslx-tutorial-enumerate-and-match-expressions","text":"In this document we explain in detail the implementation of a 8 byte prefix scan computation. In order to understand the implementation, it is useful to understand the intended functionality first. For a given input of 8 bytes, the scan iterates from left to right over the input and produces an output of the same size. Each element in the output contains the count of duplicate values seen so far in the input. The counter resets to 0 if a new value is found. For example, for this input: let input = u32 [ 8 ] : [ 0 , .. .] the code should produce this output: u3 [ 8 ] : [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ]) At index 0 it has not yet found any value, so it assigns a counter value of 0 . At index 1 it finds the second occurrence of the value '0' (which is the 1st duplicate) and therefore adds a 1 to the counter from index 0. At index 2 it finds the third occurrence of the value '0' (which is the 2nd duplicate) and therefore adds a 1 to the counter from index 1. And so on. Correspondingly, for this input: let input = u32 [ 8 ] : [ 0 , 0 , 1 , 1 , 2 , 2 , 3 , 3 ] it should produce: assert_eq ( result , u3 [ 8 ] : [ 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 ]) The full listing is in examples/dslx_intro/prefix_scan_equality.x .","title":"DSLX Tutorial: enumerate and match expressions"},{"location":"tutorials/prefix_scan/#function-prefix_scan_eq","text":"The implementation displays a few interesting language features. The function prototype is straight-forward. Input is an array of 8 values of type u32 . Output is an array of size 8 holding 3-bit values (the maximum resulting count can only be 7, which fits in 3 bits). fn prefix_scan_eq ( x : u32 [ 8 ]) -> u3 [ 8 ] { The first let expression produces a tuple of 3 values. It only cares about the last value result , so it stubs out the other two elements via the 'ignore' placeholder _ . let ( _ , _ , result ) = Why a 3-Tuple? Because he following loop has tuple of three values as the accumulator. The return type of the loop is the type of the accumulator, so above let needs to be of the same type.","title":"Function prefix_scan_eq"},{"location":"tutorials/prefix_scan/#enumerated-loop","text":"Using tuples as the accumulator is a convenient way to model multiple loop-carried values: for (( i , elem ), ( prior , count , result )) : (( u32 , u32 ), ( u32 , u3 , u3 [ 8 ])) in enumerate ( x ) { The iterable of this loop is enumerate(x) . On each iteration, this construct delivers a tuple consisting of current index and current element. This is represented as the tuple (i, elem) in the for construct. The loop next specifies the accumulator, which is a 3-tuple consisting of the values named prior , count , and result . The types of the iterable and accumulator are specified next. The iterable is a tuple consisting of two u32 values. The accumulator is more interesting, it is a tuple consisting of a u32 value ( prior ), a u3 value ( count ), and an array type u3[8] , which is an array holding 8 elements of bit-width 3. This is the type of result in the accumulator. Looping back to the prior let statement, it ignores the prior and count members of the tuple and will only return the result part.","title":"Enumerated Loop"},{"location":"tutorials/prefix_scan/#a-match-expression","text":"The next expression is an interesting match expression. The let expression binds the tuple (to_place, new_count): (u3, u3) to the result of the following match expression: let ( to_place , new_count ) : ( u3 , u3 ) = match ( i == u32 : 0 , prior == elem ) { to_place will hold the value that is to be written at a given index. new_count will contain the updated counter value. The match expression evaluates two conditions in parallel: is i == 0? is the prior element the same as the current elem Two tests mean there are four possible cases, which are all handled in the following four cases: // i == 0 (no matter whether prior == elem or not): // we set position 0 to 0 and update the new_counter to 1 ( true , true ) => ( u3 : 0 , u3 : 1 ), ( true , false ) => ( u3 : 0 , u3 : 1 ), // if i != 0 - if the current element is the same as pior, // set to_place to the value of the current count // update new_counter with the increased counter value ( false , true ) => ( count , count + u3 : 1 ), // if i != 0 - if current element is different from prior, // set to_place back to 0 // set new_counter back to 1 ( false , false ) => ( u3 : 0 , u3 : 1 ), }; To update the result, we set index i in the result array to the value to_place via the built-in update function, which produces a new value new_result ): let new_result : u3 [ 8 ] = update ( result , i , to_place ); Finally the updated accumulator value is constructed, it is the last expression in the loop: ( elem , new_count , new_result ) Following the loop body, as an argument to the loop, we initialize the accumulator in the following way. set element prior to -1, in order to not match any other value. set element count to 0. set element result to 8 0's of size u3 . (Note that the ... syntax is short for \"fill in the rest of the elements with the last value specified\".) }(( u32 : - 1 , u3 : 0 , u3 [ 8 ] : [ 0 , .. .])); And, finally, the function simply returns result : result }","title":"A Match Expression"},{"location":"tutorials/prefix_scan/#testing","text":"To test the two cases we've described above, we add the following two test cases right to this implementation file: #[test] fn test_prefix_scan_eq_all_zero () { let input = u32 [ 8 ] : [ 0 , .. .]; let result = prefix_scan_eq ( input ); assert_eq ( result , u3 [ 8 ] : [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ]) } #[test] fn test_prefix_scan_eq_doubles () { let input = u32 [ 8 ] : [ 0 , 0 , 1 , 1 , 2 , 2 , 3 , 3 ]; let result = prefix_scan_eq ( input ); assert_eq ( result , u3 [ 8 ] : [ 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 ]) } To run tests against the file present in the repository: xls$ bazel run -c opt //xls/dslx:interpreter_main -- \\ $ PWD/xls/examples/dslx_intro/prefix_scan_equality.x \\ --compare = none [ RUN UNITTEST ] prefix_scan_eq_all_zero_test [ OK ] [ RUN UNITTEST ] prefix_scan_eq_doubles_test [ OK ] [===============] 2 test(s) ran; 0 failed; 0 skipped. (Note that --compare=none is currently required because enumerate ranges are not currently convertable to IR, otherwise running the DSLX interpreter would do implicit comparison to IR interpreter -- see google/xls#164 .)","title":"Testing"},{"location":"tutorials/xlscc_channels/","text":"Tutorial: XLS[cc] channels, and fixed-width integers. Tutorial: XLS[cc] channels, and fixed-width integers. Introduction to channels. Introduction to fixed-width integers. Configuring XLS[cc] for channel interfaces. Configuring XLS[cc] for fixed-width integers. Translate into optimized XLS IR. Perform code-generation into a pipelined Verilog block. This tutorial is aimed at walking you through the implementation and synthesis into Verilog of a C++ function containing channels and fixed-width integers. Introduction to channels. XLS implements channels via a FIFO-based (ready/valid/data) interface. In C++, these channels are provided by a built-in template class called __xls_channel supporting two methods: read() and write(val) . To utilize channels include the special xls_builtin header file: #include \"/xls_builtin.h\" . An example of a usage is below, which reads an integer on the input channel, multiplies it by 3, and writes it to the output channel. #include \"/xls_builtin.h\" #pragma hls_top void test_channels ( __xls_channel < int >& in , __xls_channel < int >& out ) { out . write ( 3 * in . read ()); } Note that an ac_datatypes compatibility layer is also provided so that the same C++ code can be used for both simulation and XLS[cc] synthesis with just a change in the include paths: ac_compat . Introduction to fixed-width integers. XLS also provides a template class for fixed-width integers. These are declared using the template class XlsInt<int Width, bool Signed = true> . To utilize fixed with integer types, include xls_int.h . Create a test_channels.cc with the following contents for the rest of this tutorial. #include \"/xls_builtin.h\" #include \"xls_int.h\" #pragma hls_top void test_channels ( __xls_channel < XlsInt < 17 , false >>& in , __xls_channel < XlsInt < 22 , false >>& out ) { out . write ( 3 * in . read ()); } NOTE \"/xls_builtin.h\" has a / in order to reduce the chance of collisions with other include files that may exist on the system. Configuring XLS[cc] for channel interfaces. To support different of channel interfaces, a protofile is provided to XLS[cc] to configure the direction and interface of the top level function. The below textproto specifies that in is an input channel with FIFO (ready/valid) signaling out is an output channel with FIFO (ready/valid) signaling. The IR should be created with a top-level proc named xls_test_proc . Create a file test_channels.textproto with the following contents. channels { name: \"in\" is_input: true type: FIFO } channels { name: \"out\" is_input: false type: FIFO } name: \"xls_test\" Then convert it to a protobin. This protobin will be later provided to xlscc with --block_pb test_channels.pb . ./bazel-bin/xls/tools/proto2bin test_channels.textproto --message xlscc.HLSBlock --output test_channels.pb Configuring XLS[cc] for fixed-width integers. XLS[cc] fixed-width integers have a dependency on the ac_datatypes library. Clone the repository (https://github.com/hlslibs) into a directory named ac_datatypes . git clone https://github.com/hlslibs/ac_types.git ac_datatypes Then create the a clang.args file with the following contents to configure the include paths and pre-define the __SYNTHESIS__ name as a macro. -D__SYNTHESIS__ -I/path/to/your/xls/contrib/xlscc/synth_only -I/path/containing/ac_datatypes/.. Translate into optimized XLS IR. With the above setup complete, XLS IR can now be generated using a sequence of xlscc and opt_main . $ ./bazel-bin/xls/contrib/xlscc/xlscc test_channels.cc \\ --clang_args_file clang.args \\ --block_pb test_channels.pb > test_channels.ir $ ./bazel-bin/xls/tools/opt_main test_channels.ir > test_channels.opt.ir Note that unlike in the prior tutorial, XLS[cc] is used to generate XLS procs rather than functions. This is to support the additional interface requirements of channels. Perform code-generation into a pipelined Verilog block. With the same IR, you can either generate a combinational block or a clocked pipelined block with the codegen_main tool. In this section, we'll demonstrate how to generate a pipelined block using the above C++ code. $ ./bazel-bin/xls/tools/codegen_main test_channels.opt.ir \\ --generator = pipeline \\ --delay_model = \"sky130\" \\ --output_verilog_path = test_channels.v \\ --module_name = xls_test \\ --top = xls_test_proc \\ --reset = rst \\ --reset_active_low = false \\ --reset_asynchronous = false \\ --reset_data_path = true \\ --pipeline_stages = 5 \\ --flop_inputs = true \\ --flop_outputs = true \\ --flop_inputs_kind = skid \\ --flop_outputs_kind = skid Below is a quick summary of the new options. --delay_model=\"sky130\" - use the sky130 delay model. --top=xls_test_proc - the proc that is the top-level is named xls_test_proc . This should be the name specified in the textproto given to XLS[cc] with a _proc suffix appended. --flop_inputs_kind=skid and --flop_outputs_kind=skid - control what type of I/O buffering is used. In this case, we configure a skid buffer at both the input and output.","title":"Channels"},{"location":"tutorials/xlscc_channels/#tutorial-xlscc-channels-and-fixed-width-integers","text":"Tutorial: XLS[cc] channels, and fixed-width integers. Introduction to channels. Introduction to fixed-width integers. Configuring XLS[cc] for channel interfaces. Configuring XLS[cc] for fixed-width integers. Translate into optimized XLS IR. Perform code-generation into a pipelined Verilog block. This tutorial is aimed at walking you through the implementation and synthesis into Verilog of a C++ function containing channels and fixed-width integers.","title":"Tutorial: XLS[cc] channels, and fixed-width integers."},{"location":"tutorials/xlscc_channels/#introduction-to-channels","text":"XLS implements channels via a FIFO-based (ready/valid/data) interface. In C++, these channels are provided by a built-in template class called __xls_channel supporting two methods: read() and write(val) . To utilize channels include the special xls_builtin header file: #include \"/xls_builtin.h\" . An example of a usage is below, which reads an integer on the input channel, multiplies it by 3, and writes it to the output channel. #include \"/xls_builtin.h\" #pragma hls_top void test_channels ( __xls_channel < int >& in , __xls_channel < int >& out ) { out . write ( 3 * in . read ()); } Note that an ac_datatypes compatibility layer is also provided so that the same C++ code can be used for both simulation and XLS[cc] synthesis with just a change in the include paths: ac_compat .","title":"Introduction to channels."},{"location":"tutorials/xlscc_channels/#introduction-to-fixed-width-integers","text":"XLS also provides a template class for fixed-width integers. These are declared using the template class XlsInt<int Width, bool Signed = true> . To utilize fixed with integer types, include xls_int.h . Create a test_channels.cc with the following contents for the rest of this tutorial. #include \"/xls_builtin.h\" #include \"xls_int.h\" #pragma hls_top void test_channels ( __xls_channel < XlsInt < 17 , false >>& in , __xls_channel < XlsInt < 22 , false >>& out ) { out . write ( 3 * in . read ()); } NOTE \"/xls_builtin.h\" has a / in order to reduce the chance of collisions with other include files that may exist on the system.","title":"Introduction to fixed-width integers."},{"location":"tutorials/xlscc_channels/#configuring-xlscc-for-channel-interfaces","text":"To support different of channel interfaces, a protofile is provided to XLS[cc] to configure the direction and interface of the top level function. The below textproto specifies that in is an input channel with FIFO (ready/valid) signaling out is an output channel with FIFO (ready/valid) signaling. The IR should be created with a top-level proc named xls_test_proc . Create a file test_channels.textproto with the following contents. channels { name: \"in\" is_input: true type: FIFO } channels { name: \"out\" is_input: false type: FIFO } name: \"xls_test\" Then convert it to a protobin. This protobin will be later provided to xlscc with --block_pb test_channels.pb . ./bazel-bin/xls/tools/proto2bin test_channels.textproto --message xlscc.HLSBlock --output test_channels.pb","title":"Configuring XLS[cc] for channel interfaces."},{"location":"tutorials/xlscc_channels/#configuring-xlscc-for-fixed-width-integers","text":"XLS[cc] fixed-width integers have a dependency on the ac_datatypes library. Clone the repository (https://github.com/hlslibs) into a directory named ac_datatypes . git clone https://github.com/hlslibs/ac_types.git ac_datatypes Then create the a clang.args file with the following contents to configure the include paths and pre-define the __SYNTHESIS__ name as a macro. -D__SYNTHESIS__ -I/path/to/your/xls/contrib/xlscc/synth_only -I/path/containing/ac_datatypes/..","title":"Configuring XLS[cc] for fixed-width integers."},{"location":"tutorials/xlscc_channels/#translate-into-optimized-xls-ir","text":"With the above setup complete, XLS IR can now be generated using a sequence of xlscc and opt_main . $ ./bazel-bin/xls/contrib/xlscc/xlscc test_channels.cc \\ --clang_args_file clang.args \\ --block_pb test_channels.pb > test_channels.ir $ ./bazel-bin/xls/tools/opt_main test_channels.ir > test_channels.opt.ir Note that unlike in the prior tutorial, XLS[cc] is used to generate XLS procs rather than functions. This is to support the additional interface requirements of channels.","title":"Translate into optimized XLS IR."},{"location":"tutorials/xlscc_channels/#perform-code-generation-into-a-pipelined-verilog-block","text":"With the same IR, you can either generate a combinational block or a clocked pipelined block with the codegen_main tool. In this section, we'll demonstrate how to generate a pipelined block using the above C++ code. $ ./bazel-bin/xls/tools/codegen_main test_channels.opt.ir \\ --generator = pipeline \\ --delay_model = \"sky130\" \\ --output_verilog_path = test_channels.v \\ --module_name = xls_test \\ --top = xls_test_proc \\ --reset = rst \\ --reset_active_low = false \\ --reset_asynchronous = false \\ --reset_data_path = true \\ --pipeline_stages = 5 \\ --flop_inputs = true \\ --flop_outputs = true \\ --flop_inputs_kind = skid \\ --flop_outputs_kind = skid Below is a quick summary of the new options. --delay_model=\"sky130\" - use the sky130 delay model. --top=xls_test_proc - the proc that is the top-level is named xls_test_proc . This should be the name specified in the textproto given to XLS[cc] with a _proc suffix appended. --flop_inputs_kind=skid and --flop_outputs_kind=skid - control what type of I/O buffering is used. In this case, we configure a skid buffer at both the input and output.","title":"Perform code-generation into a pipelined Verilog block."},{"location":"tutorials/xlscc_overview/","text":"Tutorial: XLS[cc] Overview Tutorial: XLS[cc] Overview Create your first C++ module. Translate into optimized XLS IR. Perform code-generation into a combinational Verilog block. Create your second C++ module and generate an optimized IR file. Perform code-generation into a pipelined Verilog block. This tutorial is aimed at walking you through getting a function written in C++ and then compiling into a working Verilog module. This assumes that you've already been successful in building XLS. See Installing and building if not. Create your first C++ module. XLS[cc] takes as input a single translation unit -- one .cc file. Other files may be included in that one file, but only the top-level file should be provided. Create a file called test.cc with the following contents. #pragma hls_top int add3 ( int input ) { return input + 3 ; } Note that #pragma hls_top denotes the top-level function for the module. The xls func or proc created will follow that function's interface. Translate into optimized XLS IR. Now that the C++ function has been created, xlscc can be used to translate the C++ into XLS IR. opt_main is used afterwards to optimize and transform the IR into a form more easily synthesized into verilog. $ ./bazel-bin/xls/contrib/xlscc/xlscc test.cc > test.ir $ ./bazel-bin/xls/tools/opt_main test.ir > test.opt.ir The resulting test.opt.ir file should look something like the following package my_package file_number 1 \"./test.cc\" top fn add3(input: bits[32]) -> bits[32] { literal.2: bits[32] = literal(value=3, id=2, pos=[(1,2,23)]) ret add.3: bits[32] = add(input, literal.2, id=3, pos=[(1,2,23)]) } Perform code-generation into a combinational Verilog block. With the same IR, you can either generate a combinational block or a clocked pipelined block with the codegen_main tool. In this section, we'll demonstrate how to generate a combinational block. $ ./bazel-bin/xls/tools/codegen_main test.opt.ir \\ --generator = combinational \\ --delay_model = \"unit\" \\ --output_verilog_path = test.v \\ --module_name = xls_test \\ --top = add3 Below is a quick summary of each option: --generator=combinational states that codegen_main should generate a combinational module. --delay_model=\"unit\" states to use the unit delay model. Additional delay models include asap7 and sky130. --output_verilog_path=test.v is where the output verilog should be written to. --module_name=xls_test states that the generated verilog module should have the name of xls_test . --top=add3 states that the function that should be used for codegen is the function ( fn ) named add3 . The resulting test.v should have contents similar to the following module xls_test( input wire [31:0] input, output wire [31:0] out ); wire [31:0] add_6; assign add_6 = input + 32'h0000_0003; assign out = add_6; endmodule Create your second C++ module and generate an optimized IR file. XLS[cc] supports two ways of handling looping C++ constructs -- it can unroll the loop, or convert the loop into sequential logic. In this section, we'll demonstrate loop unrolling. Unrolled loops are annotated with #pragma hls_unroll yes . For example, create a file called test_unroll.cc with the following contents. #pragma hls_top int test_unroll ( int x ) { int ret = 0 ; #pragma hls_unroll yes for ( int i = 0 ; i < 32 ; ++ i ) { ret += x * i ; } return ret ; } Then compile, and optimize the resulting IR $ ./bazel-bin/xls/contrib/xlscc/xlscc test_unroll.cc > test_unroll.ir $ ./bazel-bin/xls/tools/opt_main test_unroll.ir > test_unroll.opt.ir Perform code-generation into a pipelined Verilog block. The previous section should have left you with an IR file called test_unroll.opt.ir with a function with the signature fn test_unroll(x: bits[32]) -> bits[32] . The function is likely too large to fit into a single clock cycle so we'll create a pipelined module. $ ./bazel-bin/xls/tools/codegen_main test_unroll.opt.ir \\ --generator = pipeline \\ --delay_model = \"asap7\" \\ --output_verilog_path = test_unroll.v \\ --module_name = xls_test_unroll \\ --entry = test_unroll \\ --reset = rst \\ --reset_active_low = false \\ --reset_asynchronous = false \\ --pipeline_stages = 5 \\ --flop_inputs = true \\ --flop_outputs = true Below is a quick summary of each option: --generator=pipeline - codegen_main should generate a pipelined module. --delay_model=\"asap7\" - use the asap7 delay model. --output_verilog_path=test_unroll.v - where the output verilog should be written to. --module_name=xls_test_unroll - the generated verilog module should have the name of xls_unroll_test . --entry=test_unroll - the function that should be used for codegen is the function ( fn ) named test_unroll . --reset=rst - there should be a reset signal named rst . --reset_active_low=false - a high reset signal means reset the module. --reset_asynchronous=false - rst is a synchronous reset signal. --pipeline_stages=5 - create a 5 stage pipeline. --flop_inputs=true and --flop_outputs=true - input and outputs for the block are registered.","title":"Overview"},{"location":"tutorials/xlscc_overview/#tutorial-xlscc-overview","text":"Tutorial: XLS[cc] Overview Create your first C++ module. Translate into optimized XLS IR. Perform code-generation into a combinational Verilog block. Create your second C++ module and generate an optimized IR file. Perform code-generation into a pipelined Verilog block. This tutorial is aimed at walking you through getting a function written in C++ and then compiling into a working Verilog module. This assumes that you've already been successful in building XLS. See Installing and building if not.","title":"Tutorial: XLS[cc] Overview"},{"location":"tutorials/xlscc_overview/#create-your-first-c-module","text":"XLS[cc] takes as input a single translation unit -- one .cc file. Other files may be included in that one file, but only the top-level file should be provided. Create a file called test.cc with the following contents. #pragma hls_top int add3 ( int input ) { return input + 3 ; } Note that #pragma hls_top denotes the top-level function for the module. The xls func or proc created will follow that function's interface.","title":"Create your first C++ module."},{"location":"tutorials/xlscc_overview/#translate-into-optimized-xls-ir","text":"Now that the C++ function has been created, xlscc can be used to translate the C++ into XLS IR. opt_main is used afterwards to optimize and transform the IR into a form more easily synthesized into verilog. $ ./bazel-bin/xls/contrib/xlscc/xlscc test.cc > test.ir $ ./bazel-bin/xls/tools/opt_main test.ir > test.opt.ir The resulting test.opt.ir file should look something like the following package my_package file_number 1 \"./test.cc\" top fn add3(input: bits[32]) -> bits[32] { literal.2: bits[32] = literal(value=3, id=2, pos=[(1,2,23)]) ret add.3: bits[32] = add(input, literal.2, id=3, pos=[(1,2,23)]) }","title":"Translate into optimized XLS IR."},{"location":"tutorials/xlscc_overview/#perform-code-generation-into-a-combinational-verilog-block","text":"With the same IR, you can either generate a combinational block or a clocked pipelined block with the codegen_main tool. In this section, we'll demonstrate how to generate a combinational block. $ ./bazel-bin/xls/tools/codegen_main test.opt.ir \\ --generator = combinational \\ --delay_model = \"unit\" \\ --output_verilog_path = test.v \\ --module_name = xls_test \\ --top = add3 Below is a quick summary of each option: --generator=combinational states that codegen_main should generate a combinational module. --delay_model=\"unit\" states to use the unit delay model. Additional delay models include asap7 and sky130. --output_verilog_path=test.v is where the output verilog should be written to. --module_name=xls_test states that the generated verilog module should have the name of xls_test . --top=add3 states that the function that should be used for codegen is the function ( fn ) named add3 . The resulting test.v should have contents similar to the following module xls_test( input wire [31:0] input, output wire [31:0] out ); wire [31:0] add_6; assign add_6 = input + 32'h0000_0003; assign out = add_6; endmodule","title":"Perform code-generation into a combinational Verilog block."},{"location":"tutorials/xlscc_overview/#create-your-second-c-module-and-generate-an-optimized-ir-file","text":"XLS[cc] supports two ways of handling looping C++ constructs -- it can unroll the loop, or convert the loop into sequential logic. In this section, we'll demonstrate loop unrolling. Unrolled loops are annotated with #pragma hls_unroll yes . For example, create a file called test_unroll.cc with the following contents. #pragma hls_top int test_unroll ( int x ) { int ret = 0 ; #pragma hls_unroll yes for ( int i = 0 ; i < 32 ; ++ i ) { ret += x * i ; } return ret ; } Then compile, and optimize the resulting IR $ ./bazel-bin/xls/contrib/xlscc/xlscc test_unroll.cc > test_unroll.ir $ ./bazel-bin/xls/tools/opt_main test_unroll.ir > test_unroll.opt.ir","title":"Create your second C++ module and generate an optimized IR file."},{"location":"tutorials/xlscc_overview/#perform-code-generation-into-a-pipelined-verilog-block","text":"The previous section should have left you with an IR file called test_unroll.opt.ir with a function with the signature fn test_unroll(x: bits[32]) -> bits[32] . The function is likely too large to fit into a single clock cycle so we'll create a pipelined module. $ ./bazel-bin/xls/tools/codegen_main test_unroll.opt.ir \\ --generator = pipeline \\ --delay_model = \"asap7\" \\ --output_verilog_path = test_unroll.v \\ --module_name = xls_test_unroll \\ --entry = test_unroll \\ --reset = rst \\ --reset_active_low = false \\ --reset_asynchronous = false \\ --pipeline_stages = 5 \\ --flop_inputs = true \\ --flop_outputs = true Below is a quick summary of each option: --generator=pipeline - codegen_main should generate a pipelined module. --delay_model=\"asap7\" - use the asap7 delay model. --output_verilog_path=test_unroll.v - where the output verilog should be written to. --module_name=xls_test_unroll - the generated verilog module should have the name of xls_unroll_test . --entry=test_unroll - the function that should be used for codegen is the function ( fn ) named test_unroll . --reset=rst - there should be a reset signal named rst . --reset_active_low=false - a high reset signal means reset the module. --reset_asynchronous=false - rst is a synchronous reset signal. --pipeline_stages=5 - create a 5 stage pipeline. --flop_inputs=true and --flop_outputs=true - input and outputs for the block are registered.","title":"Perform code-generation into a pipelined Verilog block."},{"location":"tutorials/xlscc_state/","text":"Tutorial: XLS[cc] state. Tutorial: XLS[cc] state. C++ Source Generate optimized XLS IR. Perform code-generation into a pipelined Verilog block. Additional XLS[cc] examples. This tutorial is aimed at walking you through the implementation and synthesis into Verilog of a C++ function containing state. XLS[cc] may infer that in order to achieve a particular implementation of a C++ function, operations may occur over multiple cycles and require additional proc state to be kept. Common constructs that may require this are static variables, or loops that aren't unrolled. In this tutorial we give an example using static variables. C++ Source Create a file named test_state.cc with the following contents. #include \"/xls_builtin.h\" #pragma hls_top void test_state ( __xls_channel < int >& out ) { static int count = 0 ; out . write ( count ); ++ count ; } As the above function uses channels, a proto detailing the type of interface is required. Create a file named test_state.textproto with the following contents to configure out as an output FIFO (ready/valid) interface. channels { name: \"out\" is_input: false type: FIFO } name: \"xls_test_state\" Generate optimized XLS IR. Use a combination of proto2bin , xlscc and opt_main to generate optimized XLS IR. $ ./bazel-bin/xls/tools/proto2bin test_state.textproto --message xlscc.HLSBlock --output test_state.pb $ ./bazel-bin/xls/contrib/xlscc/xlscc test_state.cc \\ --block_pb test_state.pb \\ > test_state.ir $ ./bazel-bin/xls/tools/opt_main test_state.ir > test_state.opt.ir Perform code-generation into a pipelined Verilog block. In this case, we will generate a single-stage pipeline without input and output flops. This will result in a module with a 32-bit increment adder along with 32-bit of state. $ ./bazel-bin/xls/tools/codegen_main test_state.opt.ir \\ --generator = pipeline \\ --delay_model = \"sky130\" \\ --output_verilog_path = xls_counter.v \\ --module_name = xls_counter \\ --top = xls_test_state_proc \\ --reset = rst \\ --reset_active_low = false \\ --reset_asynchronous = false \\ --reset_data_path = true \\ --pipeline_stages = 1 \\ --flop_inputs = false \\ --flop_outputs = false After running codegen, you should see a file named xls_counter.v with contents similar to the following. module xls_counter( input wire clk, input wire rst, input wire out_rdy, output wire [31:0] out, output wire out_vld ); reg [31:0] __st__1; wire literal_43; wire literal_40; wire [31:0] add_37; wire pipeline_enable; assign literal_43 = 1'h1; assign literal_40 = 1'h1; assign add_37 = __st__1 + 32'h0000_0001; assign pipeline_enable = literal_43 & literal_40 & out_rdy & (literal_43 & literal_40 & out_rdy); always_ff @ (posedge clk) begin if (rst) begin __st__1 <= 32'h0000_0000; end else begin __st__1 <= pipeline_enable ? add_37 : __st__1; end end assign out = __st__1; assign out_vld = literal_40 & literal_43 & 1'h1; endmodule Additional XLS[cc] examples. The above tutorials only touches upon the capabilities of XLS[cc]. XLS[cc] is based on libclang and supports many C++17 features. Notable unsupported features include pointers, function pointers, and virtual methods. For developers, it is possible to check if a specific feature is supported by checking translator_logic_test.cc for unit tests.","title":"State"},{"location":"tutorials/xlscc_state/#tutorial-xlscc-state","text":"Tutorial: XLS[cc] state. C++ Source Generate optimized XLS IR. Perform code-generation into a pipelined Verilog block. Additional XLS[cc] examples. This tutorial is aimed at walking you through the implementation and synthesis into Verilog of a C++ function containing state. XLS[cc] may infer that in order to achieve a particular implementation of a C++ function, operations may occur over multiple cycles and require additional proc state to be kept. Common constructs that may require this are static variables, or loops that aren't unrolled. In this tutorial we give an example using static variables.","title":"Tutorial: XLS[cc] state."},{"location":"tutorials/xlscc_state/#c-source","text":"Create a file named test_state.cc with the following contents. #include \"/xls_builtin.h\" #pragma hls_top void test_state ( __xls_channel < int >& out ) { static int count = 0 ; out . write ( count ); ++ count ; } As the above function uses channels, a proto detailing the type of interface is required. Create a file named test_state.textproto with the following contents to configure out as an output FIFO (ready/valid) interface. channels { name: \"out\" is_input: false type: FIFO } name: \"xls_test_state\"","title":"C++ Source"},{"location":"tutorials/xlscc_state/#generate-optimized-xls-ir","text":"Use a combination of proto2bin , xlscc and opt_main to generate optimized XLS IR. $ ./bazel-bin/xls/tools/proto2bin test_state.textproto --message xlscc.HLSBlock --output test_state.pb $ ./bazel-bin/xls/contrib/xlscc/xlscc test_state.cc \\ --block_pb test_state.pb \\ > test_state.ir $ ./bazel-bin/xls/tools/opt_main test_state.ir > test_state.opt.ir","title":"Generate optimized XLS IR."},{"location":"tutorials/xlscc_state/#perform-code-generation-into-a-pipelined-verilog-block","text":"In this case, we will generate a single-stage pipeline without input and output flops. This will result in a module with a 32-bit increment adder along with 32-bit of state. $ ./bazel-bin/xls/tools/codegen_main test_state.opt.ir \\ --generator = pipeline \\ --delay_model = \"sky130\" \\ --output_verilog_path = xls_counter.v \\ --module_name = xls_counter \\ --top = xls_test_state_proc \\ --reset = rst \\ --reset_active_low = false \\ --reset_asynchronous = false \\ --reset_data_path = true \\ --pipeline_stages = 1 \\ --flop_inputs = false \\ --flop_outputs = false After running codegen, you should see a file named xls_counter.v with contents similar to the following. module xls_counter( input wire clk, input wire rst, input wire out_rdy, output wire [31:0] out, output wire out_vld ); reg [31:0] __st__1; wire literal_43; wire literal_40; wire [31:0] add_37; wire pipeline_enable; assign literal_43 = 1'h1; assign literal_40 = 1'h1; assign add_37 = __st__1 + 32'h0000_0001; assign pipeline_enable = literal_43 & literal_40 & out_rdy & (literal_43 & literal_40 & out_rdy); always_ff @ (posedge clk) begin if (rst) begin __st__1 <= 32'h0000_0000; end else begin __st__1 <= pipeline_enable ? add_37 : __st__1; end end assign out = __st__1; assign out_vld = literal_40 & literal_43 & 1'h1; endmodule","title":"Perform code-generation into a pipelined Verilog block."},{"location":"tutorials/xlscc_state/#additional-xlscc-examples","text":"The above tutorials only touches upon the capabilities of XLS[cc]. XLS[cc] is based on libclang and supports many C++17 features. Notable unsupported features include pointers, function pointers, and virtual methods. For developers, it is possible to check if a specific feature is supported by checking translator_logic_test.cc for unit tests.","title":"Additional XLS[cc] examples."}]}